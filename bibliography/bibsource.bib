@article{Benetos2013,
    abstract = {Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.},
    added-at = {2019-10-02T22:08:36.000+0200},
    author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi},
    biburl = {https://www.bibsonomy.org/bibtex/2d49179491ec1c718beb2ceeab4b51c25/cheriell},
    day = {01},
    description = {Automatic music transcription: challenges and future directions | SpringerLink},
    doi = {10.1007/s10844-013-0258-3},
    interhash = {c5888a79c015eeaf227ef2dff86e7b48},
    intrahash = {d49179491ec1c718beb2ceeab4b51c25},
    issn = {1573-7675},
    journal = {Journal of Intelligent Information Systems},
    keywords = {AMT},
    month = {December},
    number = {3},
    pages = {407--434},
    timestamp = {2019-10-02T22:08:36.000+0200},
    title = {Automatic music transcription: challenges and future directions},
    url = {https://doi.org/10.1007/s10844-013-0258-3},
    volume = {41},
    year = {2013}
}

@article{Benetos2019,
    author = {Emmanouil Benetos and
Simon Dixon and
Zhiyao Duan and
Sebastian Ewert},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/spm/BenetosDDE19.bib},
    doi = {10.1109/MSP.2018.2869928},
    journal = {{IEEE} Signal Process. Mag.},
    number = {1},
    pages = {20--30},
    timestamp = {Fri, 18 Jan 2019 23:22:47 +0100},
    title = {Automatic Music Transcription: An Overview},
    url = {https://doi.org/10.1109/MSP.2018.2869928},
    volume = {36},
    year = {2019}
}

@inproceedings{Cogliati2016,
    author = {Andrea Cogliati and
David Temperley and
Zhiyao Duan},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ismir/CogliatiTD16.bib},
    booktitle = {Proceedings of the 17th International Society for Music Information
Retrieval Conference, {ISMIR} 2016, New York City, United States,
August 7-11, 2016},
    editor = {Michael I. Mandel and
Johanna Devaney and
Douglas Turnbull and
George Tzanetakis},
    pages = {758--764},
    timestamp = {Thu, 12 Mar 2020 11:33:02 +0100},
    title = {Transcribing Human Piano Performances into Music Notation},
    url = {https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/088\_Paper.pdf},
    year = {2016}
}

@inproceedings{Cogliati2017,
    author = {Andrea Cogliati and
Zhiyao Duan},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ismir/CogliatiD17.bib},
    booktitle = {Proceedings of the 18th International Society for Music Information
Retrieval Conference, {ISMIR} 2017, Suzhou, China, October 23-27,
2017},
    editor = {Sally Jo Cunningham and
Zhiyao Duan and
Xiao Hu and
Douglas Turnbull},
    pages = {407--413},
    timestamp = {Tue, 04 Jan 2022 10:38:06 +0100},
    title = {A Metric for Music Notation Transcription Accuracy},
    url = {https://archives.ismir.net/ismir2017/paper/000131.pdf},
    year = {2017}
}

@inproceedings{Desain1989,
    author = {Peter Desain and
Henkjan Honing and
Klaus de Rijk},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icmc/DesainHR89.bib},
    booktitle = {Proceedings of the 1989 International Computer Music Conference, {ICMC}
1989, Columbus, Ohio, USA, November 2-5, 1989},
    publisher = {Michigan Publishing},
    timestamp = {Wed, 04 May 2022 13:01:23 +0200},
    title = {A Connectionist Quantizer},
    url = {https://hdl.handle.net/2027/spo.bbp2372.1989.020},
    year = {1989}
}

@inproceedings{Hiramatsu2021,
    author = {Yuki Hiramatsu and
Eita Nakamura and
Kazuyoshi Yoshii},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ismir/HiramatsuNY21.bib},
    booktitle = {Proceedings of the 22nd International Society for Music Information
Retrieval Conference, {ISMIR} 2021, Online, November 7-12, 2021},
    editor = {Jin Ha Lee and
Alexander Lerch and
Zhiyao Duan and
Juhan Nam and
Preeti Rao and
Peter van Kranenburg and
Ajay Srinivasamurthy},
    pages = {278--284},
    timestamp = {Mon, 06 Dec 2021 13:56:05 +0100},
    title = {Joint Estimation of Note Values and Voices for Audio-to-Score Piano
Transcription},
    url = {https://archives.ismir.net/ismir2021/paper/000034.pdf},
    year = {2021}
}

@book{ISO1975,
    author = {ISO/IEC},
    institution = {International Organization for Standardization},
    key = {ISO 16:1975},
    lccn = {????},
    month = {January},
    pages = {1},
    title = {Acoustics. Standard tuning frequency (Standard musical pitch)},
    url = {https://www.iso.org/standard/3601.html},
    year = {1975}
}

@book{ISO2008,
    author = {ISO/IEC},
    institution = {International Organization for Standardization},
    key = {ISO\slash IEC 14496-23:2008},
    lccn = {????},
    month = {February},
    pages = {173},
    title = {Coding of audio-visual objects},
    url = {https://www.iso.org/standard/45531.html},
    year = {2008}
}

@misc{Krueger1996,
    author = {Bernd Krueger},
    title = {Classical Piano MIDI},
    url = {http://www.piano-midi.de},
    year = {1996}
}

@inproceedings{Liu2022,
    author = {Lele Liu and
Qiuqiang Kong and
Veronica Morfi and
Emmanouil Benetos},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ismir/LiuKMB22.bib},
    booktitle = {Proceedings of the 23rd International Society for Music Information
Retrieval Conference, {ISMIR} 2022, Bengaluru, India, December 4-8,
2022},
    editor = {Preeti Rao and
Hema A. Murthy and
Ajay Srinivasamurthy and
Rachel M. Bittner and
Rafael Caro Repetto and
Masataka Goto and
Xavier Serra and
Marius Miron},
    pages = {395--402},
    timestamp = {Mon, 08 May 2023 14:44:00 +0200},
    title = {Performance MIDI-to-score conversion by neural beat tracking},
    url = {https://archives.ismir.net/ismir2022/paper/000047.pdf},
    year = {2022}
}

@inproceedings{McLeod2018,
    abstract = {Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a timefrequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent.We therefore introduce MV 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation— for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H.},
    author = {Andrew McLeod and Mark Steedman},
    booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
    day = {20},
    editor = {Emilia G{\'o}mez and Xiao Hu and Eric Humphrey},
    isbn = {9782954035123},
    language = {English},
    month = {November},
    note = {19th International Society for Music Information Retrieval Conference, ISMIR 2019 ; Conference date: 23-09-2018 Through 27-09-2018},
    pages = {42--49},
    title = {Evaluating Automatic Polyphonic Music Transcription},
    url = {http://sap.ist.i.kyoto-u.ac.jp/members/mcleod/pdf/ISMIR_Eval.pdf},
    year = {2018}
}

@article{McLeod2019,
    author = {Andrew McLeod},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/corr/abs-1906-00566.bib},
    eprint = {1906.00566},
    eprinttype = {arXiv},
    journal = {CoRR},
    timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
    title = {Evaluating Non-aligned Musical Score Transcriptions with {MV2H}},
    url = {http://arxiv.org/abs/1906.00566},
    volume = {abs/1906.00566},
    year = {2019}
}

@article{Schedl2014,
    author = {Markus Schedl and Emilia Gómez and Julián Urbano},
    doi = {10.1561/1500000042},
    issn = {1554-0669},
    journal = {Foundations and Trends® in Information Retrieval},
    number = {2-3},
    pages = {127-261},
    title = {Music Information Retrieval: Recent Developments and Applications},
    url = {http://dx.doi.org/10.1561/1500000042},
    volume = {8},
    year = {2014}
}

@inproceedings{Suzuki2021,
    author = {Masahiro Suzuki},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/mmasia/Suzuki21.bib},
    booktitle = {MMAsia '21: {ACM} Multimedia Asia, Gold Coast, Australia, December
1 - 3, 2021},
    doi = {10.1145/3469877.3490612},
    editor = {Changwen Chen and
Helen Huang and
Jun Zhou and
Tatsuya Harada and
Jianfei Cai and
Wu Liu and
Dong Xu},
    pages = {31:1--31:7},
    publisher = {{ACM}},
    timestamp = {Wed, 12 Jan 2022 10:04:43 +0100},
    title = {Score Transformer: Generating Musical Score from Note-level Representation},
    url = {https://doi.org/10.1145/3469877.3490612},
    year = {2021}
}

@article{Temperley2009,
    author = {David Temperley},
    doi = {10.1080/09298210902928495},
    eprint = {https://doi.org/10.1080/09298210902928495},
    journal = {Journal of New Music Research},
    number = {1},
    pages = {3-18},
    publisher = {Routledge},
    title = {A Unified Probabilistic Model for Polyphonic Music Analysis},
    url = {https://doi.org/10.1080/09298210902928495},
    volume = {38},
    year = {2009}
}

@inproceedings{Ycart2018,
    author = {Adrien Ycart and Emmanouil Benetos},
    booktitle = {19th International Society for Music Information Retrieval Conference, ISMIR, Late Breaking and Demos Papers.},
    title = {A-MAPS: Augmented MAPS dataset with rhythm and key annotations},
    url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/45985},
    year = {2018}
}

@article{Oliveira2017,
    author = {Hélio de Oliveira and Raimundo Oliveira},
    year = {2017},
    month = {May},
    pages = {},
    title = {Understanding MIDI: A Painless Tutorial on Midi Format}
}

@article{Downie2003,
    author = {J. Stephen Downie},
    title = {Music information retrieval},
    journal = {Annual Review of Information Science and Technology},
    volume = {37},
    number = {1},
    pages = {295-340},
    doi = {https://doi.org/10.1002/aris.1440370108},
    url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/aris.1440370108},
    eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/aris.1440370108},
    year = {2003}
}

@article{Kassler1966,
    issn = {00316016},
    url = {http://www.jstor.org/stable/832213},
    author = {Michael Kassler},
    journal = {Perspectives of New Music},
    number = {2},
    pages = {59--67},
    publisher = {Perspectives of New Music},
    title = {Toward Musical Information Retrieval},
    urldate = {2024-02-19},
    volume = {4},
    year = {1966}
}


@Article{Bhattarai2023,
    AUTHOR = {Bhuwan Bhattarai and Joonwhoan Lee},
    TITLE = {A Comprehensive Review on Music Transcription},
    JOURNAL = {Applied Sciences},
    VOLUME = {13},
    YEAR = {2023},
    NUMBER = {21},
    ARTICLE-NUMBER = {11882},
    URL = {https://www.mdpi.com/2076-3417/13/21/11882},
    ISSN = {2076-3417},
    ABSTRACT = {Music transcription is the process of transforming recorded sound of musical performances into symbolic representations such as sheet music or MIDI files. Extensive research and development have been carried out in the field of music transcription and technology. This comprehensive review paper surveys the diverse methodologies, techniques, and advancements that have shaped the landscape of music transcription. The paper outlines the significance of music transcription in preserving, analyzing, and disseminating musical compositions across various genres and cultures. It also provides a historical perspective by tracing the evolution of music transcription from traditional manual methods to modern automated approaches. It also highlights the challenges in transcription posed by complex singing techniques, variations in instrumentation, ambiguity in pitch, tempo changes, rhythm, and dynamics. The review also categorizes four different types of transcription techniques, frame-level, note-level, stream-level, and notation-level, discussing their strengths and limitations. It also encompasses the various research domains of music transcription from general melody extraction to vocal melody, note-level monophonic to polyphonic vocal transcription, single-instrument to multi-instrument transcription, and multi-pitch estimation. The survey further covers a broad spectrum of music transcription applications in music production and creation. It also reviews state-of-the-art open-source as well as commercial music transcription tools for pitch estimation, onset and offset detection, general melody detection, and vocal melody detection. In addition, it also encompasses the currently available python libraries that can be used for music transcription. Furthermore, the review highlights the various open-source benchmark datasets for different areas of music transcription. It also provides a wide range of references supporting the historical context, theoretical frameworks, and foundational concepts to help readers understand the background of music transcription and the context of our paper.},
    DOI = {10.3390/app132111882}
}

@article{Kim2018,
    author = {Jong Kim and Justing Salamon and Peter Li and Juan Bello},
    year = {2018},
    month = {April},
    pages = {},
    title = {CREPE: A Convolutional Representation for Pitch Estimation},
    journal = {Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on}
}

@article{Riley2023,
    title = {CREPE Notes: A new method for segmenting pitch contours into discrete notes},
    author = {Xavier Riley and Simon Dixon},
    journal = {arXiv preprint arXiv:2311.08884},
    year = {2023}
}

@article{Duan2014,
    author = {Zhiyao Duan and Jinyu Han and Bryan Pardo},
    year = {2014},
    month = {January},
    pages = {138-150},
    title = {Multi-pitch Streaming of Harmonic Sound Mixtures},
    volume = {22},
    journal = {Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
    doi = {10.1109/TASLP.2013.2285484}
}

@article{Orio2006,
    author = {Nicola Orio},
    year = {2006},
    month = {November},
    pages = {1-},
    title = {Music Retrieval: A Tutorial and Review},
    volume = {1},
    journal = {Foundations and Trends in Information Retrieval},
    doi = {10.1561/1500000002}
}

@book{Read1969,
    title = {Music Notation: A Manual of Modern Practice},
    author = {Gardner Read},
    isbn = {9780800854539},
    lccn = {68054213},
    series = {Crescendo book},
    url = {https://books.google.com.cy/books?id=pGQJAQAAMAAJ},
    year = {1969},
    publisher = {Allyn and Bacon}
}

@book{Huber2007,
    title = {The MIDI Manual: A Practical Guide to MIDI in the Project Studio},
    author = {Huber, D.M.},
    isbn = {9780240807980},
    lccn = {98045592},
    series = {Audio Engineering Society Presents Series},
    url = {https://books.google.com.cy/books?id=GfHZwBwZuKIC},
    year = {2007},
    publisher = {Focal Press}
}

@manual{LilyPond2002,
    title = {LilyPond --- Essay on automated music engraving},
    author = {The LilyPond Development Team},
    organization = {LilyPond Software},
    year = {2002},
    url = {https://lilypond.org/}
}

@inproceedings{Good2001,
    author = {Michael Good},
    year = {2001},
    month = {January},
    pages = {},
    title = {MusicXML: An Internet-Friendly Format for Sheet Music}
}

@manual{MIDI1996,
    added-at = {2009-11-03T14:17:45.000+0100},
    author = {MIDI Manufacturers Association},
    biburl = {https://www.bibsonomy.org/bibtex/2b1dab50d129176c9da8fb98634717a47/algebradresden},
    booktitle = {Complete MIDI 1.0 Detailed Specification},
    interhash = {7beb5ad4f0306af12f02e02620d3c824},
    intrahash = {b1dab50d129176c9da8fb98634717a47},
    keywords = {handbib informatik musiktheorie regal68 schmidt slub zurueck:2012-07},
    kst = {S},
    msc = {68},
    key = {MIDI1996},
    shorthand = {MIDI1996},
    publisher = {MIDI Manufacturers Association Incorporated},
    rueckgabe = {2011-07-23},
    timestamp = {2011-12-19T15:49:23.000+0100},
    title = {{C}omplete {MIDI} 1.0 {D}etailed {S}pecification},
    url = {http://www.midi.org/techspecs/gm.php},
    woher = {SLUB},
    year = {1996}
}

@book{Smith1999,
    title = {The Scientist and Engineer's Guide to Digital Signal Processing},
    author = {Steven W. Smith},
    isbn = {9780966017649},
    lccn = {97080293},
    url = {https://books.google.com.cy/books?id=ZtfaNwAACAAJ},
    year = {1999},
    publisher = {California Technical Publishing}
}

@book{Rudin1976,
    title = {Principles of Mathematical Analysis},
    author = {Walter Rudin},
    isbn = {9780070856134},
    lccn = {75179033},
    series = {International series in pure and applied mathematics},
    url = {https://books.google.com.cy/books?id=kwqzPAAACAAJ},
    year = {1976},
    publisher = {McGraw-Hill}
}

@manual{ABC2013,
    title = {Abc musical notation --- standard version 2.2},
    author = {Chris Walshaw},
    organization = {ABC Community},
    year = {2013},
    url = {http://abcnotation.com/wiki/abc:standard}
}

@book{Schaeffer2012,
    title = {In Search of a Concrete Music},
    author = {Pierre Schaeffer and Christine North and John Dack},
    isbn = {9780520265738},
    lccn = {2012029627},
    series = {California Studies in 20th-Century Music},
    url = {https://books.google.com.cy/books?id=EqwlDQAAQBAJ},
    year = {2012},
    publisher = {University of California Press}
}

@book{Sethares2005,
    author = {William Sethares},
    year = {2005},
    month = {January},
    pages = {},
    title = {Tuning, Timbre, Spectrum, Scale},
    isbn = {1852337974}
}

@book{Kostka1994,
    author = {Stefan Kostka and Dorothy Payne},
    year = {1994},
    title = {Tonal Harmony with an Introduction to Twentieth-Century Music},
    address = {New York},
    edition = {8},
    isbn = {9788578110796},
    keywords = {music theory},
    mendeley-tags = {music theory},
    publisher = {McGraw-Hill Education}
}

@article{Berenzweig2003,
    author = {Adam Berenzweig and Beth Logan and Daniel P.W. Ellis and Brian Whitman},
    year = {2003},
    month = {November},
    pages = {},
    title = {A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures},
    volume = {28},
    journal = {Computer Music Journal},
    doi = {10.1162/014892604323112257}
}

@misc{Wiki2024B,
    author = "Wikipedia",
    title = "{Sheet music} --- {W}ikipedia{,} The Free Encyclopedia",
    year = "2024",
    howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Sheet\%20music&oldid=1210471824}},
    note = "[Online; accessed 01-March-2024]"
}

@misc{Wiki2024A,
    author = "Wikipedia",
    title = "{Music information retrieval} --- {W}ikipedia{,} The Free Encyclopedia",
    year = "2024",
    howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Music\%20information\%20retrieval&oldid=1189049395}},
    note = "[Online; accessed 01-March-2024]"
}

@misc{Chopin1839,
    title = {Prélude {O}pus 28 {N}o. 4 in {E} Minor},
    author = {Frédéric Chopin},
    year = {1839},
    howpublished = {\url{https://musescore.com/classicman/chopin-opus-28-no-4}},
    note = "[Online; accessed 01-March-2024]"
}

@inproceedings{Takeda2002,
    author = {Haruto Takeda and Naoki Saito and Tomoshi Otsuki and Mitsuro Nakai and Hiroshi Shimodaira and Shigeki Sagayama},
    booktitle = {2002 IEEE Workshop on Multimedia Signal Processing.},
    title = {Hidden Markov model for automatic transcription of MIDI signals},
    year = {2002},
    volume = {},
    number = {},
    pages = {428-431},
    keywords = {Hidden Markov models;Speech recognition;Stochastic processes;Instruments;Multiple signal classification;Automatic speech recognition;Maximum likelihood estimation;Viterbi algorithm;Time measurement;Rhythm},
    doi = {10.1109/MMSP.2002.1203337}
}

@inproceedings{Cambouropoulos2000,
    title = {From MIDI to Traditional Musical Notation},
    author = {Emilios Cambouropoulos},
    year = {2000},
    url = {https://api.semanticscholar.org/CorpusID:7615192}
}

@inproceedings{Grohganz2014,
    title = {Estimating Musical Time Information from Performed MIDI Files},
    author = {Harald Grohganz and Michael Clausen and Meinard M{\"u}ller},
    booktitle = {International Society for Music Information Retrieval Conference},
    year = {2014},
    url = {https://api.semanticscholar.org/CorpusID:15059128}
}

@book{Jurafsky2009,
    abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology - at all levels and with all modern technologies - this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing. -- Book Description from Website.},
    added-at = {2013-04-24T13:46:19.000+0200},
    address = {Upper Saddle River, N.J.},
    author = {Dan Jurafsky and James H. Martin},
    biburl = {https://www.bibsonomy.org/bibtex/2fb7fa20679ebb9d69d27d7c9682fd774/lopusz_kdd},
    description = {Speech and Language Processing (2nd Edition): Daniel Jurafsky, James H. Martin: 9780131873216: Amazon.com: Books},
    interhash = {5f4a309a36c3da5e3becbf0ac5d88413},
    intrahash = {fb7fa20679ebb9d69d27d7c9682fd774},
    isbn = {9780131873216 0131873210},
    keywords = {language},
    publisher = {Pearson Prentice Hall},
    refid = {213375806},
    timestamp = {2013-04-24T13:46:19.000+0200},
    title = {Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition},
    url = {http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/ref=pd_bxgy_b_img_y},
    year = 2009
}

@inproceedings{Yang2005,
    author = {Aaron Yang and Elaine Chew and Anja Volk},
    title = {A Dynamic Programming Approach to Adaptive Tatum Assignment for Rhythm
 Transcription},
    booktitle = {Seventh {IEEE} International Symposium on Multimedia {(ISM} 2005),
 12-14 December 2005, Irvine, CA, {USA}},
    pages = {577--584},
    publisher = {{IEEE} Computer Society},
    year = {2005},
    url = {https://doi.org/10.1109/ISM.2005.5},
    doi = {10.1109/ISM.2005.5},
    timestamp = {Fri, 24 Mar 2023 00:04:45 +0100},
    biburl = {https://dblp.org/rec/conf/ism/YangCV05.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Iyer1997,
    title = {A novel representation for rhythmic structure},
    author = {Vijay Iyer and Jeff Bilmes and Matt Wright and David Wessel},
    booktitle = {Proceedings of the 23rd International Computer Music Conference},
    pages = {97--100},
    year = {1997},
    organization = {Citeseer}
}

@article{Nakamura2017a,
    author = {Eita Nakamura and Kazuyoshi Yoshii and Simon Dixon},
    title = {Note Value Recognition for Piano Transcription Using Markov Random Fields},
    year = {2017},
    issue_date = {September 2017},
    publisher = {IEEE Press},
    volume = {25},
    number = {9},
    issn = {2329-9290},
    url = {https://doi.org/10.1109/TASLP.2017.2722103},
    doi = {10.1109/TASLP.2017.2722103},
    abstract = {This paper presents a statistical method for use in music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times or note values and, thus, could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results show that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.},
    journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
    month = sep,
    pages = {1846–1858},
    numpages = {13}
}

@inproceedings{Karydis2007,
    title = {Horizontal and vertical integration/segregation in auditory streaming: a voice separation algorithm for symbolic musical data},
    author = {Karydis, Ioannis and Nanopoulos, Alexandros and Papadopoulos, Apostolos and Cambouropoulos, Emilios and Manolopoulos, Yannis},
    booktitle = {Proceedings 4th Sound and Music Computing Conference (SMC’2007)},
    year = {2007}
}

@article{Cambouropoulos2008,
    author = {Emilios Cambouropoulos},
    year = {2008},
    month = {09},
    pages = {75-94},
    title = {Voice And Stream: Perceptual And Computational Modeling Of Voice Separation},
    volume = {26},
    journal = {Music Perception - MUSIC PERCEPT},
    doi = {10.1525/mp.2008.26.1.75}
}

@article{Holzapfel2021,
    author = {Andre Holzapfel and Emmanouil Benetos and Andrew Killick and Richard Widdess},
    title = "{Humanities and engineering perspectives on music transcription}",
    journal = {Digital Scholarship in the Humanities},
    volume = {37},
    number = {3},
    pages = {747-764},
    year = {2021},
    month = {10},
    abstract = "{Music transcription is a process of creating a notation of musical sounds. It has been used as a basis for the analysis of music from a wide variety of cultures. Recent decades have seen an increasing amount of engineering research within the field of Music Information Retrieval that aims at automatically obtaining music transcriptions in Western staff notation. However, such approaches are not widely applied in research in ethnomusicology. This article aims to bridge interdisciplinary gaps by identifying aspects of proximity and divergence between the two fields. As part of our study, we collected manual transcriptions of traditional dance tune recordings by eighteen transcribers. Our method employs a combination of expert and computational evaluation of these transcriptions. This enables us to investigate the limitations of automatic music transcription (AMT) methods and computational transcription metrics that have been proposed for their evaluation. Based on these findings, we discuss promising avenues to make AMT more useful for studies in the Humanities. These are, first, assessing the quality of a transcription based on an analytic purpose; secondly, developing AMT approaches that are able to learn conventions concerning the transcription of a specific style; thirdly, a focus on novice transcribers as users of AMT systems; and, finally, considering target notation systems different from Western staff notation.}",
    issn = {2055-7671},
    doi = {10.1093/llc/fqab074},
    url = {https://doi.org/10.1093/llc/fqab074},
    eprint = {https://academic.oup.com/dsh/article-pdf/37/3/747/45501951/fqab074.pdf},
}

@inproceedings{Nakamura2018,
    author = {Eita Nakamura and Emmanouil Benetos and Kazuyoshi Yoshii and Simon Dixon},
    year = {2018},
    month = {04},
    pages = {101-105},
    title = {Towards Complete Polyphonic Music Transcription: Integrating Multi-Pitch Detection and Rhythm Quantization},
    doi = {10.1109/ICASSP.2018.8461914}
}

@inproceedings{Nakamura2017b,
    title = "Performance error detection and post-processing for fast and accurate symbolic music alignment",
    abstract = "This paper presents a fast and accurate alignment method for polyphonic symbolic music signals. It is known that to accurately align piano performances, methods using the voice structure are needed. However, such methods typically have high computational cost and they are applicable only when prior voice information is given. It is pointed out that alignment errors are typically accompanied by performance errors in the aligned signal. This suggests the possibility of correcting (or realigning) preliminary results by a fast (but not-so-accurate) alignment method with a refined method applied to limited segments of aligned signals, to save the computational cost. To realise this, we develop a method for detecting performance errors and a realignment method that works fast and accurately in local regions around performance errors. To remove the dependence on prior voice information, voice separation is performed to the reference signal in the local regions. By applying our method to results obtained by previously proposed hidden Markov models, the highest accuracies are achieved with short computation time. Our source code is published in the accompanying web page, together with a user interface to examine and correct alignment results.",
    author = "Eita Nakamura and Kazuyoshi Yoshii and Haruhiro Katayose",
    note = "Publisher Copyright: {\textcopyright} 2019 Eita Nakamura, Kazuyoshi Yoshii, Haruhiro Katayose.; 18th International Society for Music Information Retrieval Conference, ISMIR 2017 ; Conference date: 23-10-2017 Through 27-10-2017",
    year = "2017",
    language = "English",
    series = "Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017",
    publisher = "International Society for Music Information Retrieval",
    pages = "347--353",
    editor = "Cunningham, {Sally Jo} and Zhiyao Duan and Xiao Hu and Douglas Turnbull",
    booktitle = "Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017",
}

@article{Nakamura2017c,
    author = {Eita Nakamura and Kazuyoshi Yoshii and Shigeki Sagayama},
    title = {Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices},
    year = {2017},
    issue_date = {April 2017},
    publisher = {IEEE Press},
    volume = {25},
    number = {4},
    issn = {2329-9290},
    url = {https://doi.org/10.1109/TASLP.2017.2662479},
    doi = {10.1109/TASLP.2017.2662479},
    abstract = {In a recent conference paper, we have reported a rhythm transcription method based on a merged-output hidden Markov model HMM that explicitly describes the multiple-voice structure of polyphonic music. This model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices. In this paper, we present a complete description of the proposed model and develop an inference technique, which is valid for any merged-output HMMs, for which output probabilities depend on past events. We also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms. Using MIDI recordings of classical piano pieces, we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances. This reveals the state-of-the-art methods of rhythm transcription for the first time in the literature. Publicly available source codes are also provided for future comparisons.},
    journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
    month = apr,
    pages = {794–806},
    numpages = {13}
}

@inproceedings{Raffel2014,
    added-at = {2022-01-04T00:00:00.000+0100},
    author = {Colin Raffel and Brian McFee and Eric J. Humphrey and Justin Salamon and Oriol Nieto and Dawen Liang and Daniel P.W. Ellis},
    biburl = {https://www.bibsonomy.org/bibtex/2d267807f08d81ea2c5f5e67b5bbb9647/dblp},
    booktitle = {ISMIR},
    ee = {http://www.terasoft.com.tw/conf/ismir2014/proceedings/T066_320_Paper.pdf},
    interhash = {a6729652192c037ee4ff21d00f6d9666},
    intrahash = {d267807f08d81ea2c5f5e67b5bbb9647},
    keywords = {dblp},
    pages = {367-372},
    timestamp = {2024-04-09T16:05:24.000+0200},
    title = {MIR{\_}EVAL: A Transparent Implementation of Common MIR Metrics.},
    url = {http://dblp.uni-trier.de/db/conf/ismir/ismir2014.html#RaffelMHSNLE14},
    year = 2014
}

@phdthesis{Harte2010,
    title = {Towards automatic extraction of harmony information from music signals},
    author = {Christopher Harte},
    year = {2010}
}

@inproceedings{Foscarin2020,
    title = {{ASAP}: a dataset of aligned scores and performances for piano transcription},
    author = {Francesco Foscarin and Andrew McLeod and Philippe Rigaux and Florent Jacquemard and Masahiko Sakai},
    booktitle = {International Society for Music Information Retrieval Conference {(ISMIR)}},
    year = {2020},
    pages = {534--541}
}

@techreport{Emiya2010,
    TITLE = {{MAPS - A piano database for multipitch estimation and automatic transcription of music}},
    AUTHOR = {Valentin and Nancy Bertin and Bertrand David and Roland Badeau},
    URL = {https://inria.hal.science/inria-00544155},
    TYPE = {Research Report},
    PAGES = {11},
    YEAR = {2010},
    MONTH = Jul,
    KEYWORDS = {Audio ; database ; piano ; fundamental frequency ; transcription ; music ; MAPS},
    PDF = {https://inria.hal.science/inria-00544155v1/file/publication-205.pdf},
    HAL_ID = {inria-00544155},
    HAL_VERSION = {v1},
}
