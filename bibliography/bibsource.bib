@article{Benetos2013,
    abstract = {Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.},
    added-at = {2019-10-02T22:08:36.000+0200},
    author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi},
    biburl = {https://www.bibsonomy.org/bibtex/2d49179491ec1c718beb2ceeab4b51c25/cheriell},
    day = {01},
    description = {Automatic music transcription: challenges and future directions | SpringerLink},
    doi = {10.1007/s10844-013-0258-3},
    interhash = {c5888a79c015eeaf227ef2dff86e7b48},
    intrahash = {d49179491ec1c718beb2ceeab4b51c25},
    issn = {1573-7675},
    journal = {Journal of Intelligent Information Systems},
    keywords = {AMT},
    month = {December},
    number = {3},
    pages = {407--434},
    timestamp = {2019-10-02T22:08:36.000+0200},
    title = {Automatic music transcription: challenges and future directions},
    url = {https://doi.org/10.1007/s10844-013-0258-3},
    volume = {41},
    year = {2013}
}

@article{Benetos2019,
    author = {Emmanouil Benetos and
Simon Dixon and
Zhiyao Duan and
Sebastian Ewert},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/spm/BenetosDDE19.bib},
    doi = {10.1109/MSP.2018.2869928},
    journal = {{IEEE} Signal Process. Mag.},
    number = {1},
    pages = {20--30},
    timestamp = {Fri, 18 Jan 2019 23:22:47 +0100},
    title = {Automatic Music Transcription: An Overview},
    url = {https://doi.org/10.1109/MSP.2018.2869928},
    volume = {36},
    year = {2019}
}

@inproceedings{Cogliati2016,
    author = {Andrea Cogliati and
David Temperley and
Zhiyao Duan},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ismir/CogliatiTD16.bib},
    booktitle = {Proceedings of the 17th International Society for Music Information
Retrieval Conference, {ISMIR} 2016, New York City, United States,
August 7-11, 2016},
    editor = {Michael I. Mandel and
Johanna Devaney and
Douglas Turnbull and
George Tzanetakis},
    pages = {758--764},
    timestamp = {Thu, 12 Mar 2020 11:33:02 +0100},
    title = {Transcribing Human Piano Performances into Music Notation},
    url = {https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/088\_Paper.pdf},
    year = {2016}
}

@inproceedings{Cogliati2017,
    author = {Andrea Cogliati and
Zhiyao Duan},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ismir/CogliatiD17.bib},
    booktitle = {Proceedings of the 18th International Society for Music Information
Retrieval Conference, {ISMIR} 2017, Suzhou, China, October 23-27,
2017},
    editor = {Sally Jo Cunningham and
Zhiyao Duan and
Xiao Hu and
Douglas Turnbull},
    pages = {407--413},
    timestamp = {Tue, 04 Jan 2022 10:38:06 +0100},
    title = {A Metric for Music Notation Transcription Accuracy},
    url = {https://archives.ismir.net/ismir2017/paper/000131.pdf},
    year = {2017}
}

@inproceedings{Desain1989,
    author = {Peter Desain and
Henkjan Honing and
Klaus de Rijk},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icmc/DesainHR89.bib},
    booktitle = {Proceedings of the 1989 International Computer Music Conference, {ICMC}
1989, Columbus, Ohio, USA, November 2-5, 1989},
    publisher = {Michigan Publishing},
    timestamp = {Wed, 04 May 2022 13:01:23 +0200},
    title = {A Connectionist Quantizer},
    url = {https://hdl.handle.net/2027/spo.bbp2372.1989.020},
    year = {1989}
}

@inproceedings{Hiramatsu2021,
    author = {Yuki Hiramatsu and
Eita Nakamura and
Kazuyoshi Yoshii},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ismir/HiramatsuNY21.bib},
    booktitle = {Proceedings of the 22nd International Society for Music Information
Retrieval Conference, {ISMIR} 2021, Online, November 7-12, 2021},
    editor = {Jin Ha Lee and
Alexander Lerch and
Zhiyao Duan and
Juhan Nam and
Preeti Rao and
Peter van Kranenburg and
Ajay Srinivasamurthy},
    pages = {278--284},
    timestamp = {Mon, 06 Dec 2021 13:56:05 +0100},
    title = {Joint Estimation of Note Values and Voices for Audio-to-Score Piano
Transcription},
    url = {https://archives.ismir.net/ismir2021/paper/000034.pdf},
    year = {2021}
}

@book{ISO1975,
    author = {ISO/IEC},
    institution = {International Organization for Standardization},
    key = {ISO 16:1975},
    lccn = {????},
    month = {January},
    pages = {1},
    title = {Acoustics. Standard tuning frequency (Standard musical pitch)},
    url = {https://www.iso.org/standard/3601.html},
    year = {1975}
}

@book{ISO2008,
    author = {ISO/IEC},
    institution = {International Organization for Standardization},
    key = {ISO\slash IEC 14496-23:2008},
    lccn = {????},
    month = {February},
    pages = {173},
    title = {Coding of audio-visual objects},
    url = {https://www.iso.org/standard/45531.html},
    year = {2008}
}

@misc{Krueger1996,
    author = {Bernd Krueger},
    title = {Classical Piano MIDI},
    url = {http://www.piano-midi.de},
    year = {1996}
}

@inproceedings{Liu2022,
    author = {Lele Liu and
Qiuqiang Kong and
Veronica Morfi and
Emmanouil Benetos},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ismir/LiuKMB22.bib},
    booktitle = {Proceedings of the 23rd International Society for Music Information
Retrieval Conference, {ISMIR} 2022, Bengaluru, India, December 4-8,
2022},
    editor = {Preeti Rao and
Hema A. Murthy and
Ajay Srinivasamurthy and
Rachel M. Bittner and
Rafael Caro Repetto and
Masataka Goto and
Xavier Serra and
Marius Miron},
    pages = {395--402},
    timestamp = {Mon, 08 May 2023 14:44:00 +0200},
    title = {Performance MIDI-to-score conversion by neural beat tracking},
    url = {https://archives.ismir.net/ismir2022/paper/000047.pdf},
    year = {2022}
}

@inproceedings{McLeod2018,
    abstract = {Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a timefrequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent.We therefore introduce MV 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation— for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H.},
    author = {Andrew McLeod and Mark Steedman},
    booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
    day = {20},
    editor = {Emilia G{\'o}mez and Xiao Hu and Eric Humphrey},
    isbn = {9782954035123},
    language = {English},
    month = {November},
    note = {19th International Society for Music Information Retrieval Conference, ISMIR 2019 ; Conference date: 23-09-2018 Through 27-09-2018},
    pages = {42--49},
    title = {Evaluating Automatic Polyphonic Music Transcription},
    url = {http://sap.ist.i.kyoto-u.ac.jp/members/mcleod/pdf/ISMIR_Eval.pdf},
    year = {2018}
}

@article{McLeod2019,
    author = {Andrew McLeod},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/corr/abs-1906-00566.bib},
    eprint = {1906.00566},
    eprinttype = {arXiv},
    journal = {CoRR},
    timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
    title = {Evaluating Non-aligned Musical Score Transcriptions with {MV2H}},
    url = {http://arxiv.org/abs/1906.00566},
    volume = {abs/1906.00566},
    year = {2019}
}

@article{Schedl2014,
    author = {Markus Schedl and Emilia Gómez and Julián Urbano},
    doi = {10.1561/1500000042},
    issn = {1554-0669},
    journal = {Foundations and Trends® in Information Retrieval},
    number = {2-3},
    pages = {127-261},
    title = {Music Information Retrieval: Recent Developments and Applications},
    url = {http://dx.doi.org/10.1561/1500000042},
    volume = {8},
    year = {2014}
}

@inproceedings{Suzuki2021,
    author = {Masahiro Suzuki},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/mmasia/Suzuki21.bib},
    booktitle = {MMAsia '21: {ACM} Multimedia Asia, Gold Coast, Australia, December
1 - 3, 2021},
    doi = {10.1145/3469877.3490612},
    editor = {Changwen Chen and
Helen Huang and
Jun Zhou and
Tatsuya Harada and
Jianfei Cai and
Wu Liu and
Dong Xu},
    pages = {31:1--31:7},
    publisher = {{ACM}},
    timestamp = {Wed, 12 Jan 2022 10:04:43 +0100},
    title = {Score Transformer: Generating Musical Score from Note-level Representation},
    url = {https://doi.org/10.1145/3469877.3490612},
    year = {2021}
}

@article{Temperley2009,
    author = {David Temperley},
    doi = {10.1080/09298210902928495},
    eprint = {https://doi.org/10.1080/09298210902928495},
    journal = {Journal of New Music Research},
    number = {1},
    pages = {3-18},
    publisher = {Routledge},
    title = {A Unified Probabilistic Model for Polyphonic Music Analysis},
    url = {https://doi.org/10.1080/09298210902928495},
    volume = {38},
    year = {2009}
}

@inproceedings{Ycart2018,
    author = {Adrien Ycart and Emmanouil Benetos},
    booktitle = {19th International Society for Music Information Retrieval Conference, ISMIR, Late Breaking and Demos Papers.},
    title = {A-MAPS: Augmented MAPS dataset with rhythm and key annotations},
    url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/45985},
    year = {2018}
}

@article{Oliveira2017,
    author = {Hélio de Oliveira and Raimundo Oliveira},
    year = {2017},
    month = {May},
    pages = {},
    title = {Understanding MIDI: A Painless Tutorial on Midi Format}
}

@article{Downie2003,
    author = {J. Stephen Downie},
    title = {Music information retrieval},
    journal = {Annual Review of Information Science and Technology},
    volume = {37},
    number = {1},
    pages = {295-340},
    doi = {https://doi.org/10.1002/aris.1440370108},
    url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/aris.1440370108},
    eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/aris.1440370108},
    year = {2003}
}

@article{Kassler1966,
    issn = {00316016},
    url = {http://www.jstor.org/stable/832213},
    author = {Michael Kassler},
    journal = {Perspectives of New Music},
    number = {2},
    pages = {59--67},
    publisher = {Perspectives of New Music},
    title = {Toward Musical Information Retrieval},
    urldate = {2024-02-19},
    volume = {4},
    year = {1966}
}


@Article{Bhattarai2023,
    AUTHOR = {Bhuwan Bhattarai and Joonwhoan Lee},
    TITLE = {A Comprehensive Review on Music Transcription},
    JOURNAL = {Applied Sciences},
    VOLUME = {13},
    YEAR = {2023},
    NUMBER = {21},
    ARTICLE-NUMBER = {11882},
    URL = {https://www.mdpi.com/2076-3417/13/21/11882},
    ISSN = {2076-3417},
    ABSTRACT = {Music transcription is the process of transforming recorded sound of musical performances into symbolic representations such as sheet music or MIDI files. Extensive research and development have been carried out in the field of music transcription and technology. This comprehensive review paper surveys the diverse methodologies, techniques, and advancements that have shaped the landscape of music transcription. The paper outlines the significance of music transcription in preserving, analyzing, and disseminating musical compositions across various genres and cultures. It also provides a historical perspective by tracing the evolution of music transcription from traditional manual methods to modern automated approaches. It also highlights the challenges in transcription posed by complex singing techniques, variations in instrumentation, ambiguity in pitch, tempo changes, rhythm, and dynamics. The review also categorizes four different types of transcription techniques, frame-level, note-level, stream-level, and notation-level, discussing their strengths and limitations. It also encompasses the various research domains of music transcription from general melody extraction to vocal melody, note-level monophonic to polyphonic vocal transcription, single-instrument to multi-instrument transcription, and multi-pitch estimation. The survey further covers a broad spectrum of music transcription applications in music production and creation. It also reviews state-of-the-art open-source as well as commercial music transcription tools for pitch estimation, onset and offset detection, general melody detection, and vocal melody detection. In addition, it also encompasses the currently available python libraries that can be used for music transcription. Furthermore, the review highlights the various open-source benchmark datasets for different areas of music transcription. It also provides a wide range of references supporting the historical context, theoretical frameworks, and foundational concepts to help readers understand the background of music transcription and the context of our paper.},
    DOI = {10.3390/app132111882}
}

@article{Kim2018,
    author = {Jong Kim and Justing Salamon and Peter Li and Juan Bello},
    year = {2018},
    month = {April},
    pages = {},
    title = {CREPE: A Convolutional Representation for Pitch Estimation},
    journal = {Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on}
}

@article{Riley2023,
    title = {CREPE Notes: A new method for segmenting pitch contours into discrete notes},
    author = {Xavier Riley and Simon Dixon},
    journal = {arXiv preprint arXiv:2311.08884},
    year = {2023}
}

@article{Duan2014,
    author = {Zhiyao Duan and Jinyu Han and Bryan Pardo},
    year = {2014},
    month = {January},
    pages = {138-150},
    title = {Multi-pitch Streaming of Harmonic Sound Mixtures},
    volume = {22},
    journal = {Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
    doi = {10.1109/TASLP.2013.2285484}
}

@article{Orio2006,
    author = {Nicola Orio},
    year = {2006},
    month = {November},
    pages = {1-},
    title = {Music Retrieval: A Tutorial and Review},
    volume = {1},
    journal = {Foundations and Trends in Information Retrieval},
    doi = {10.1561/1500000002}
}

@book{Read1969,
    title = {Music Notation: A Manual of Modern Practice},
    author = {Gardner Read},
    isbn = {9780800854539},
    lccn = {68054213},
    series = {Crescendo book},
    url = {https://books.google.com.cy/books?id=pGQJAQAAMAAJ},
    year = {1969},
    publisher = {Allyn and Bacon}
}

@book{Huber2007,
    title = {The MIDI Manual: A Practical Guide to MIDI in the Project Studio},
    author = {Huber, D.M.},
    isbn = {9780240807980},
    lccn = {98045592},
    series = {Audio Engineering Society Presents Series},
    url = {https://books.google.com.cy/books?id=GfHZwBwZuKIC},
    year = {2007},
    publisher = {Focal Press}
}

@manual{LilyPond2002,
    title = {LilyPond --- Essay on automated music engraving},
    author = {The LilyPond Development Team},
    organization = {LilyPond Software},
    year = {2002},
    url = {https://lilypond.org/}
}

@inproceedings{Good2001,
    author = {Michael Good},
    year = {2001},
    month = {January},
    pages = {},
    title = {MusicXML: An Internet-Friendly Format for Sheet Music}
}

@manual{MIDI1996,
    added-at = {2009-11-03T14:17:45.000+0100},
    author = {MIDI Manufacturers Association},
    biburl = {https://www.bibsonomy.org/bibtex/2b1dab50d129176c9da8fb98634717a47/algebradresden},
    booktitle = {Complete MIDI 1.0 Detailed Specification},
    interhash = {7beb5ad4f0306af12f02e02620d3c824},
    intrahash = {b1dab50d129176c9da8fb98634717a47},
    keywords = {handbib informatik musiktheorie regal68 schmidt slub zurueck:2012-07},
    kst = {S},
    msc = {68},
    key = {MIDI1996},
    shorthand = {MIDI1996},
    publisher = {MIDI Manufacturers Association Incorporated},
    rueckgabe = {2011-07-23},
    timestamp = {2011-12-19T15:49:23.000+0100},
    title = {{C}omplete {MIDI} 1.0 {D}etailed {S}pecification},
    url = {http://www.midi.org/techspecs/gm.php},
    woher = {SLUB},
    year = {1996}
}

@book{Smith1999,
    title = {The Scientist and Engineer's Guide to Digital Signal Processing},
    author = {Steven W. Smith},
    isbn = {9780966017649},
    lccn = {97080293},
    url = {https://books.google.com.cy/books?id=ZtfaNwAACAAJ},
    year = {1999},
    publisher = {California Technical Publishing}
}

@book{Rudin1976,
    title = {Principles of Mathematical Analysis},
    author = {Walter Rudin},
    isbn = {9780070856134},
    lccn = {75179033},
    series = {International series in pure and applied mathematics},
    url = {https://books.google.com.cy/books?id=kwqzPAAACAAJ},
    year = {1976},
    publisher = {McGraw-Hill}
}

@manual{ABC2013,
    title = {Abc musical notation --- standard version 2.2},
    author = {Chris Walshaw},
    organization = {ABC Community},
    year = {2013},
    url = {http://abcnotation.com/wiki/abc:standard}
}

@book{Schaeffer2012,
    title = {In Search of a Concrete Music},
    author = {Pierre Schaeffer and Christine North and John Dack},
    isbn = {9780520265738},
    lccn = {2012029627},
    series = {California Studies in 20th-Century Music},
    url = {https://books.google.com.cy/books?id=EqwlDQAAQBAJ},
    year = {2012},
    publisher = {University of California Press}
}

@book{Sethares2005,
    author = {William Sethares},
    year = {2005},
    month = {January},
    pages = {},
    title = {Tuning, Timbre, Spectrum, Scale},
    isbn = {1852337974}
}

@book{Kostka1994,
    author = {Stefan Kostka and Dorothy Payne},
    year = {1994},
    title = {Tonal Harmony with an Introduction to Twentieth-Century Music},
    address = {New York},
    edition = {8},
    isbn = {9788578110796},
    keywords = {music theory},
    mendeley-tags = {music theory},
    publisher = {McGraw-Hill Education}
}

@article{Berenzweig2003,
    author = {Adam Berenzweig and Beth Logan and Daniel P.W. Ellis and Brian Whitman},
    year = {2003},
    month = {November},
    pages = {},
    title = {A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures},
    volume = {28},
    journal = {Computer Music Journal},
    doi = {10.1162/014892604323112257}
}

@misc{Wiki2024B,
    author = "Wikipedia",
    title = "{Sheet music} --- {W}ikipedia{,} The Free Encyclopedia",
    year = "2024",
    howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Sheet\%20music&oldid=1210471824}},
    note = "[Online; accessed 01-March-2024]"
}

@misc{Wiki2024A,
    author = "Wikipedia",
    title = "{Music information retrieval} --- {W}ikipedia{,} The Free Encyclopedia",
    year = "2024",
    howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Music\%20information\%20retrieval&oldid=1189049395}},
    note = "[Online; accessed 01-March-2024]"
}

@misc{Chopin1839,
    title = {Prélude {O}pus 28 {N}o. 4 in {E} Minor},
    author = {Frédéric Chopin},
    year = {1839},
    howpublished = {\url{https://musescore.com/classicman/chopin-opus-28-no-4}},
    note = "[Online; accessed 01-March-2024]"
}

@inproceedings{Takeda2002,
    author = {Haruto Takeda and Naoki Saito and Tomoshi Otsuki and Mitsuro Nakai and Hiroshi Shimodaira and Shigeki Sagayama},
    booktitle = {2002 IEEE Workshop on Multimedia Signal Processing.},
    title = {Hidden Markov model for automatic transcription of MIDI signals},
    year = {2002},
    volume = {},
    number = {},
    pages = {428-431},
    keywords = {Hidden Markov models;Speech recognition;Stochastic processes;Instruments;Multiple signal classification;Automatic speech recognition;Maximum likelihood estimation;Viterbi algorithm;Time measurement;Rhythm},
    doi = {10.1109/MMSP.2002.1203337}
}

@inproceedings{Cambouropoulos2000,
    title = {From MIDI to Traditional Musical Notation},
    author = {Emilios Cambouropoulos},
    year = {2000},
    url = {https://api.semanticscholar.org/CorpusID:7615192}
}

@inproceedings{Grohganz2014,
    title = {Estimating Musical Time Information from Performed MIDI Files},
    author = {Harald Grohganz and Michael Clausen and Meinard M{\"u}ller},
    booktitle = {International Society for Music Information Retrieval Conference},
    year = {2014},
    url = {https://api.semanticscholar.org/CorpusID:15059128}
}

@book{Jurafsky2009,
    abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology - at all levels and with all modern technologies - this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing. -- Book Description from Website.},
    added-at = {2013-04-24T13:46:19.000+0200},
    address = {Upper Saddle River, N.J.},
    author = {Dan Jurafsky and James H. Martin},
    biburl = {https://www.bibsonomy.org/bibtex/2fb7fa20679ebb9d69d27d7c9682fd774/lopusz_kdd},
    description = {Speech and Language Processing (2nd Edition): Daniel Jurafsky, James H. Martin: 9780131873216: Amazon.com: Books},
    interhash = {5f4a309a36c3da5e3becbf0ac5d88413},
    intrahash = {fb7fa20679ebb9d69d27d7c9682fd774},
    isbn = {9780131873216 0131873210},
    keywords = {language},
    publisher = {Pearson Prentice Hall},
    refid = {213375806},
    timestamp = {2013-04-24T13:46:19.000+0200},
    title = {Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition},
    url = {http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/ref=pd_bxgy_b_img_y},
    year = 2009
}

@inproceedings{Yang2005,
    author = {Aaron Yang and Elaine Chew and Anja Volk},
    title = {A Dynamic Programming Approach to Adaptive Tatum Assignment for Rhythm
 Transcription},
    booktitle = {Seventh {IEEE} International Symposium on Multimedia {(ISM} 2005),
 12-14 December 2005, Irvine, CA, {USA}},
    pages = {577--584},
    publisher = {{IEEE} Computer Society},
    year = {2005},
    url = {https://doi.org/10.1109/ISM.2005.5},
    doi = {10.1109/ISM.2005.5},
    timestamp = {Fri, 24 Mar 2023 00:04:45 +0100},
    biburl = {https://dblp.org/rec/conf/ism/YangCV05.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Iyer1997,
    title = {A novel representation for rhythmic structure},
    author = {Vijay Iyer and Jeff Bilmes and Matt Wright and David Wessel},
    booktitle = {Proceedings of the 23rd International Computer Music Conference},
    pages = {97--100},
    year = {1997},
    organization = {Citeseer}
}

@article{Nakamura2017a,
    author = {Eita Nakamura and Kazuyoshi Yoshii and Simon Dixon},
    title = {Note Value Recognition for Piano Transcription Using Markov Random Fields},
    year = {2017},
    issue_date = {September 2017},
    publisher = {IEEE Press},
    volume = {25},
    number = {9},
    issn = {2329-9290},
    url = {https://doi.org/10.1109/TASLP.2017.2722103},
    doi = {10.1109/TASLP.2017.2722103},
    abstract = {This paper presents a statistical method for use in music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times or note values and, thus, could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results show that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.},
    journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
    month = sep,
    pages = {1846–1858},
    numpages = {13}
}

@inproceedings{Karydis2007,
    title = {Horizontal and vertical integration/segregation in auditory streaming: a voice separation algorithm for symbolic musical data},
    author = {Karydis, Ioannis and Nanopoulos, Alexandros and Papadopoulos, Apostolos and Cambouropoulos, Emilios and Manolopoulos, Yannis},
    booktitle = {Proceedings 4th Sound and Music Computing Conference (SMC’2007)},
    year = {2007}
}

@article{Cambouropoulos2008,
    author = {Emilios Cambouropoulos},
    year = {2008},
    month = {09},
    pages = {75-94},
    title = {Voice And Stream: Perceptual And Computational Modeling Of Voice Separation},
    volume = {26},
    journal = {Music Perception - MUSIC PERCEPT},
    doi = {10.1525/mp.2008.26.1.75}
}

@article{Holzapfel2021,
    author = {Andre Holzapfel and Emmanouil Benetos and Andrew Killick and Richard Widdess},
    title = "{Humanities and engineering perspectives on music transcription}",
    journal = {Digital Scholarship in the Humanities},
    volume = {37},
    number = {3},
    pages = {747-764},
    year = {2021},
    month = {10},
    abstract = "{Music transcription is a process of creating a notation of musical sounds. It has been used as a basis for the analysis of music from a wide variety of cultures. Recent decades have seen an increasing amount of engineering research within the field of Music Information Retrieval that aims at automatically obtaining music transcriptions in Western staff notation. However, such approaches are not widely applied in research in ethnomusicology. This article aims to bridge interdisciplinary gaps by identifying aspects of proximity and divergence between the two fields. As part of our study, we collected manual transcriptions of traditional dance tune recordings by eighteen transcribers. Our method employs a combination of expert and computational evaluation of these transcriptions. This enables us to investigate the limitations of automatic music transcription (AMT) methods and computational transcription metrics that have been proposed for their evaluation. Based on these findings, we discuss promising avenues to make AMT more useful for studies in the Humanities. These are, first, assessing the quality of a transcription based on an analytic purpose; secondly, developing AMT approaches that are able to learn conventions concerning the transcription of a specific style; thirdly, a focus on novice transcribers as users of AMT systems; and, finally, considering target notation systems different from Western staff notation.}",
    issn = {2055-7671},
    doi = {10.1093/llc/fqab074},
    url = {https://doi.org/10.1093/llc/fqab074},
    eprint = {https://academic.oup.com/dsh/article-pdf/37/3/747/45501951/fqab074.pdf},
}

@inproceedings{Nakamura2018,
    author = {Eita Nakamura and Emmanouil Benetos and Kazuyoshi Yoshii and Simon Dixon},
    year = {2018},
    month = {04},
    pages = {101-105},
    title = {Towards Complete Polyphonic Music Transcription: Integrating Multi-Pitch Detection and Rhythm Quantization},
    doi = {10.1109/ICASSP.2018.8461914}
}

@inproceedings{Nakamura2017b,
    title = "Performance error detection and post-processing for fast and accurate symbolic music alignment",
    abstract = "This paper presents a fast and accurate alignment method for polyphonic symbolic music signals. It is known that to accurately align piano performances, methods using the voice structure are needed. However, such methods typically have high computational cost and they are applicable only when prior voice information is given. It is pointed out that alignment errors are typically accompanied by performance errors in the aligned signal. This suggests the possibility of correcting (or realigning) preliminary results by a fast (but not-so-accurate) alignment method with a refined method applied to limited segments of aligned signals, to save the computational cost. To realise this, we develop a method for detecting performance errors and a realignment method that works fast and accurately in local regions around performance errors. To remove the dependence on prior voice information, voice separation is performed to the reference signal in the local regions. By applying our method to results obtained by previously proposed hidden Markov models, the highest accuracies are achieved with short computation time. Our source code is published in the accompanying web page, together with a user interface to examine and correct alignment results.",
    author = "Eita Nakamura and Kazuyoshi Yoshii and Haruhiro Katayose",
    note = "Publisher Copyright: {\textcopyright} 2019 Eita Nakamura, Kazuyoshi Yoshii, Haruhiro Katayose.; 18th International Society for Music Information Retrieval Conference, ISMIR 2017 ; Conference date: 23-10-2017 Through 27-10-2017",
    year = "2017",
    language = "English",
    series = "Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017",
    publisher = "International Society for Music Information Retrieval",
    pages = "347--353",
    editor = "Cunningham, {Sally Jo} and Zhiyao Duan and Xiao Hu and Douglas Turnbull",
    booktitle = "Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017",
}

@article{Nakamura2017c,
    author = {Eita Nakamura and Kazuyoshi Yoshii and Shigeki Sagayama},
    title = {Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices},
    year = {2017},
    issue_date = {April 2017},
    publisher = {IEEE Press},
    volume = {25},
    number = {4},
    issn = {2329-9290},
    url = {https://doi.org/10.1109/TASLP.2017.2662479},
    doi = {10.1109/TASLP.2017.2662479},
    abstract = {In a recent conference paper, we have reported a rhythm transcription method based on a merged-output hidden Markov model HMM that explicitly describes the multiple-voice structure of polyphonic music. This model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices. In this paper, we present a complete description of the proposed model and develop an inference technique, which is valid for any merged-output HMMs, for which output probabilities depend on past events. We also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms. Using MIDI recordings of classical piano pieces, we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances. This reveals the state-of-the-art methods of rhythm transcription for the first time in the literature. Publicly available source codes are also provided for future comparisons.},
    journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
    month = apr,
    pages = {794–806},
    numpages = {13}
}

@inproceedings{Raffel2014,
    added-at = {2022-01-04T00:00:00.000+0100},
    author = {Colin Raffel and Brian McFee and Eric J. Humphrey and Justin Salamon and Oriol Nieto and Dawen Liang and Daniel P.W. Ellis},
    biburl = {https://www.bibsonomy.org/bibtex/2d267807f08d81ea2c5f5e67b5bbb9647/dblp},
    booktitle = {ISMIR},
    ee = {http://www.terasoft.com.tw/conf/ismir2014/proceedings/T066_320_Paper.pdf},
    interhash = {a6729652192c037ee4ff21d00f6d9666},
    intrahash = {d267807f08d81ea2c5f5e67b5bbb9647},
    keywords = {dblp},
    pages = {367-372},
    timestamp = {2024-04-09T16:05:24.000+0200},
    title = {MIR{\_}EVAL: A Transparent Implementation of Common MIR Metrics.},
    url = {http://dblp.uni-trier.de/db/conf/ismir/ismir2014.html#RaffelMHSNLE14},
    year = 2014
}

@phdthesis{Harte2010,
    title = {Towards automatic extraction of harmony information from music signals},
    author = {Christopher Harte},
    year = {2010}
}

@inproceedings{Foscarin2020,
    title = {{ASAP}: a dataset of aligned scores and performances for piano transcription},
    author = {Francesco Foscarin and Andrew McLeod and Philippe Rigaux and Florent Jacquemard and Masahiko Sakai},
    booktitle = {International Society for Music Information Retrieval Conference {(ISMIR)}},
    year = {2020},
    pages = {534--541}
}

@techreport{Emiya2010,
    TITLE = {{MAPS - A piano database for multipitch estimation and automatic transcription of music}},
    AUTHOR = {Valentin and Nancy Bertin and Bertrand David and Roland Badeau},
    URL = {https://inria.hal.science/inria-00544155},
    TYPE = {Research Report},
    PAGES = {11},
    YEAR = {2010},
    MONTH = Jul,
    KEYWORDS = {Audio ; database ; piano ; fundamental frequency ; transcription ; music ; MAPS},
    PDF = {https://inria.hal.science/inria-00544155v1/file/publication-205.pdf},
    HAL_ID = {inria-00544155},
    HAL_VERSION = {v1},
}

@inproceedings{Ribeiro2016,
    author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
    title = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
    booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on
 Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
 13-17, 2016},
    pages = {1135--1144},
    year = {2016},
}

@article{Sakoe1978,
    title = {Dynamic programming algorithm optimization for spoken word recognition},
    author = {Hiroaki Sakoe and Seibi Chiba},
    journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
    year = {1978},
    volume = {26},
    pages = {159-165},
    url = {https://api.semanticscholar.org/CorpusID:17900407}
}

@inproceedings{Cuthbert2010,
    added-at = {2020-03-12T00:00:00.000+0100},
    author = {Michael Scott Cuthbert and Christopher Ariza},
    biburl = {https://www.bibsonomy.org/bibtex/2655eb165e9efd7371a52cc1ce2d4efab/dblp},
    booktitle = {ISMIR},
    editor = {Downie, J. Stephen and Veltkamp, Remco C.},
    ee = {http://ismir2010.ismir.net/proceedings/ismir2010-108.pdf},
    interhash = {75ae23495cb8183efb69481e6ad51a5f},
    intrahash = {655eb165e9efd7371a52cc1ce2d4efab},
    isbn = {978-90-393-53813},
    keywords = {dblp},
    pages = {637-642},
    publisher = {International Society for Music Information Retrieval},
    timestamp = {2020-03-13T13:00:05.000+0100},
    title = {Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data.},
    url = {http://dblp.uni-trier.de/db/conf/ismir/ismir2010.html#CuthbertA10},
    year = 2010
}

@article{Liu2021,
    title = {ACPAS: a dataset of aligned classical piano audio and scores for audio-to-score transcription},
    author = {Lele Liu and Veronica Morfi and Emmanouil Benetos},
    year = {2021}
}

@book{Manning2008,
    added-at = {2009-03-23T10:41:57.000+0100},
    author = {Christopher D. Manning and Prabhakar Raghavan and Hinrich Schütze},
    biburl = {https://www.bibsonomy.org/bibtex/22588419fae77ef64bd735f4265f7daa5/hotho},
    interhash = {2e574e46b7668a7268e7f02b46f4d9bb},
    intrahash = {2588419fae77ef64bd735f4265f7daa5},
    keywords = {books buch introduction ir lecture standard toread},
    publisher = {Cambridge University Press},
    timestamp = {2011-05-01T15:55:33.000+0200},
    title = {Introduction to Information Retrieval},
    url = {http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html},
    year = 2008
}

@article{Sokolova2009,
    author = {Marina Sokolova and Guy Lapalme},
    year = {2009},
    month = {07},
    pages = {427-437},
    title = {A systematic analysis of performance measures for classification tasks},
    volume = {45},
    journal = {Information Processing and Management},
    doi = {10.1016/j.ipm.2009.03.002}
}

@book{Roelleke2022,
    title = {Information Retrieval Models: Foundations and Relationships},
    author = {Thomas Roelleke},
    isbn = {9783031023286},
    series = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
    url = {https://books.google.pl/books?id=YX9yEAAAQBAJ},
    year = {2022},
    publisher = {Springer International Publishing}
}

@Book{Goodfellow2016,
    Title = {Deep Learning},
    Author = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
    Publisher = {MIT Press},
    Year = {2016},
    Address = {Cambridge, MA, USA},
    Note = {\url{http://www.deeplearningbook.org}}
}

@inproceedings{Vaswani2017,
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Attention is All you Need},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
    volume = {30},
    year = {2017}
}

@article{Dosovitskiy2020,
    title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit
 and Neil Houlsby},
    journal = {ICLR},
    year = {2021}
}

@inproceedings{Zhu2021,
    author = {Hongyuan Zhu and Ye Niu and Di Fu and Hao Wang},
    title = {MusicBERT: A Self-supervised Learning of Music Representation},
    year = {2021},
    isbn = {9781450386517},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3474085.3475576},
    doi = {10.1145/3474085.3475576},
    abstract = {Music recommendation has been one of the most used information retrieval services on internet. Finding suitable music for users' demands from tens of millions of music relies on the understanding of music content. Traditional studies usually focus on music representation based on massive user behavioral data and music meta-data, which ignore the audio characteristic of music. However, it is found that the melodic characteristics of music themselves can be further used to understand music. Moreover, how to utilize large-scale audio data to learn music representation is not well explored. To this end, we propose a self-supervised learning model for music representation. We firstly utilize a beat-level music pre-training model to learn the structure of music. Then, we use a multi-task learning framework to model music self-representation and co-relations between music, concurrently. Besides, we propose several downstream tasks to evaluate music representation, including music genre classification, music highlight, and music similarity retrieval. Extensive experiments on multiple music datasets demonstrate our model's superiority over baselines on learning music representation.},
    booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
    pages = {3955–3963},
    numpages = {9},
    keywords = {music information retrieval, music representation, pre-training},
    location = {Virtual Event, China},
    series = {MM '21}
}

@article{Borsos2023,
    author = {Zal\'{a}n Borsos and Rapha\"{e}l Marinier and Damien Vincent and Eugene Kharitonov and Olivier Pietquin and Matt Sharifi and Dominik Roblek and Olivier Teboul and David Grangier and Marco Tagliasacchi and Neil Zeghidour},
    title = {AudioLM: A Language Modeling Approach to Audio Generation},
    year = {2023},
    issue_date = {2023},
    publisher = {IEEE Press},
    volume = {31},
    issn = {2329-9290},
    url = {https://doi.org/10.1109/TASLP.2023.3288409},
    doi = {10.1109/TASLP.2023.3288409},
    abstract = {We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
    journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
    month = jun,
    pages = {2523–2533},
    numpages = {11}
}

@unknown{Beyer2024,
    author = {Tim Beyer and Angela Dai},
    year = {2024},
    month = {09},
    pages = {},
    title = {End-to-end Piano Performance-MIDI to Score Conversion with Transformers},
    doi = {10.48550/arXiv.2410.00210}
}

@inproceedings{Colin2016,
    abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: first, compute low-level features for each frame using Dense Trajectories or a Convolutional Neural Network to encode local spatiotemporal information, and second, input these features into a classifier such as a Recurrent Neural Network (RNN) that captures high-level temporal relationships. While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
    added-at = {2020-10-15T14:36:56.000+0200},
    address = {Cham},
    author = {Colin Lea and Ren{\'e} Vidal and Austin Reiter and Gregory D. Hager},
    biburl = {https://www.bibsonomy.org/bibtex/201a24c2eb2102dab0a87f7afebf753bd/annakrause},
    booktitle = {Computer Vision -- ECCV 2016 Workshops},
    editor = {Hua, Gang and J{\'e}gou, Herv{\'e}},
    interhash = {302360f8fd68659399f804ce699d3d8e},
    intrahash = {01a24c2eb2102dab0a87f7afebf753bd},
    isbn = {978-3-319-49409-8},
    keywords = {Convolution TCN TimeSeries},
    pages = {47--54},
    publisher = {Springer International Publishing},
    timestamp = {2020-10-15T15:02:36.000+0200},
    title = {Temporal Convolutional Networks: A Unified Approach to Action Segmentation},
    year = 2016
}

@article{Araujo2019,
    year = 2019,
    month = 11,
    title = "Computing Receptive Fields of Convolutional Neural Networks",
    author = "André Araujo and Wade Norris and Jack Sim",
    publisher = "Distill Working Group",
    url = "http://dx.doi.org/10.23915/distill.00021",
    series = "Distill",
    number = "11",
    volume = "4",
    doi = "10.23915/distill.00021"
}

@inproceedings{Chen2015,
    added-at = {2020-06-08T16:54:55.000+0200},
    author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
    biburl = {https://www.bibsonomy.org/bibtex/2e9c7cd52fd55a3c8b7216ba5a57e73cb/flodal},
    booktitle = {ICLR (Poster)},
    editor = {Bengio, Yoshua and LeCun, Yann},
    ee = {http://arxiv.org/abs/1412.7062},
    interhash = {8024e830c121ffe1e0351d1c826a123b},
    intrahash = {e9c7cd52fd55a3c8b7216ba5a57e73cb},
    keywords = {thema:SpatialTransformerNetworks},
    timestamp = {2020-06-08T16:54:55.000+0200},
    title = {Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs.},
    url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2015.html#ChenPKMY14},
    year = 2015
}

@article{Holschneider1989,
    author = {Matthias Holschneider and Richard Kronland-Martinet and Jean Morlet and Philippe Tchamitchian},
    year = {1989},
    month = {01},
    pages = {286},
    title = {A Real-Time Algorithm for Signal Analysis with the Help of the Wavelet Transform},
    volume = {-1},
    isbn = {978-3-642-97179-2},
    journal = {Wavelets, Time-Frequency Methods and Phase Space},
    doi = {10.1007/978-3-642-75988-8_28}
}

@inproceedings{Haviv2022,
    title = "Transformer Language Models without Positional Encodings Still Learn Positional Information",
    author = "Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy",
    editor = "Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.99",
    doi = "10.18653/v1/2022.findings-emnlp.99",
    pages = "1382--1390",
    abstract = "Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models and that this phenomenon is robust across different datasets, model sizes, and sequence lengths.Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information.We conjecture that causal attention enables the model to infer the number of predecessors that each token can attend to, thereby approximating its absolute position.Our findings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism but also from the effects of the causal mask.",
}

@article{Su2024,
    title = {RoFormer: Enhanced transformer with Rotary Position Embedding},
    journal = {Neurocomputing},
    volume = {568},
    pages = {127063},
    year = {2024},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2023.127063},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
    author = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
    keywords = {Pre-trained language models, Position information encoding, Pre-training, Natural language processing},
    abstract = {Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer.}
}
