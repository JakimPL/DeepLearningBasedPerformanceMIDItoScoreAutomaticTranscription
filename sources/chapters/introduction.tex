\chapter*{Introduction}

\addcontentsline{toc}{chapter}{Introduction}

The aim of this thesis is to explore the topic of automatic transcription of performance MIDI into musical scores. Despite the rapid advancements in machine learning methods across various music domains, particularly music generation, we believe that the problem of automatic score generation remains underexplored.

The discussion begins with an introduction to the necessary concepts and theoretical frameworks in Chapter \ref{representation_of_music_information}. The problem of automatic transcription is placed within the broader context of \emph{Automatic Music Transcription}, which is itself a subset of the broader domain of \emph{Music Information Retrieval}. Chapter \ref{automatic_music_transcription} explores a variety of transcription tasks, along with an overview of the challenges and difficulties associated with automating music transcription. 

We delve into how the quality of transcriptions can be measured and the challenges inherent in their evaluation. Three sets of metrics are presented, each representing a distinct approach to assessing transcription quality: MUSTER, MV2H, and \emph{Score Similarity}. This topic is covered in Chapter \ref{midi_to_score_transcription_evaluation}.

In Chapter \ref{overview_of_existing_methods_for_midi_to_score_conversion}, we present brief history of the developments in the field, provide the modern formulation of the problem, and analyze two earlier methods of performance MIDI to score conversion: one based on Hidden Markov Models, and another using dynamic programming. These methods were the main academically documented approaches to the problem. We detail the concepts and algorithms behind them and discuss their limitations.

The core of the thesis focuses on the state-of-the-art model for MIDI transcription, \emph{Performance MIDI-to-Score Conversion by Neural Beat Tracking} \cite{Liu2022}, which combines convolutional recurrent networks with dynamic programming for beat prediction. This model was the first to successfully utilize machine learning methods for (almost) complete MIDI to score transcription. We provide a detailed discussion of the modelâ€™s architecture, performance results, and observed behaviors in Chapter \ref{performance_midi_to_score_conversion_by_neural_beat_tracking}.
As a novel approach, using custom explainable machine learning tools, we analyze the model's undesired behaviors in depth and introduce methods to better understand its individual decisions. We also highlight several common transcription problems generated by the model. Insights gained from this analysis were used to improve the model's robustness to specific transformations, e.g. velocity perturbation, which originally distorted heavily the model's outputs. The results of this investigation are presented in the experimental section.

In Chapter \ref{experiments_and_improvements}, we extend the study by conducting ablation studies that reveal successful strategies to improve the model's robustness. Beyond that, we conducted experiments comparing the base model with two other promising architectures: \emph{Transformers} and \emph{Temporal Convolutional Networks} (TCN). We show that the basic Transformer architecture is not on par with the base model, but small TCNs achieve comparable results, while being more computationally efficient than the original convolutional recurrent networks.
 
Moreover, we extended the base model by incorporating an additional component for handling dynamics. Additionally, we proposed a method to evaluate the quality of dynamics transcription as an extension of the MV2H metric while maintaining its foundational principles. 

At the end of the thesis, in Chapter \ref{conclusions}, we summarize the entire work. We describe potential improvements paths.

This thesis aims to be accessible without requiring prior knowledge of advanced music theory. While we have strived to clarify musical concepts as much as possible, it is beyond the scope of this work to explain all of used musical terms. Readers interested in a deeper understanding of musical terminology are encouraged to refer to \cite{Read1969}, which offers a detailed explanation of the musical elements discussed.

All models were trained on the \emph{Entropy} computing cluster, provided by the \emph{Faculty of Mathematics, Informatics, and Mechanics} at the \emph{University of Warsaw}.

I would like to thank the authors of the paper \cite{Liu2022}, who kindly facilitated reproducing the results by providing tensor-to-MIDI converters. I also acknowledge the assistance of \emph{ChatGPT} in helping me translate some of my clumsy sentences into a more human-readable format.
