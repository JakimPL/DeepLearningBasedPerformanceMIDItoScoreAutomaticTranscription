\chapter{Experiments and Improvements}\label{experiments_and_improvements}

In order to enhance the proposed model, we employed different strategies for finding improvements: \begin{itemize}
	\item We tried smaller and faster networks that are data efficient to optimize the current architecture.
	\item We tested more powerful architectures that proved to be successful (Transformers) in the case of natural language processing.
	\item We extended a model with an additional module for handling other musical elements: dynamics.\end{itemize}
	
In subsequent experiments we stepped out from the full Transformer for attention mechanism only. We wanted to check whether access to the entire tensor sequence is beneficial for the transcription task. However, in the preliminary experiments combinations of self-attention mechanism with other components didn't perform as well as the base model nor Transformer-based ones.
	
Let us present results of these experiments.

\section{Transformers}

\emph{Transformers} are a class of neural network architectures introduced by Vaswani et al. in the landmark paper \emph{Attention Is All You Need} \cite{Vaswani2017}. Originally developed for natural language processing (NLP), have since become versatile and powerful framework for modeling sequential data across numerous other fields, including image processing \cite{Dosovitskiy2020}, audio generation \cite{Borsos2023} and even symbolic music analysis \cite{Zhu2021}.

\begin{figure}[ht!]
\centering
\input{sources/graphs/transformer}
\caption[The Transformer architecture.]{The Transformer architecture \cite{Vaswani2017}.}
\end{figure}

The key innovation in Transformers is the \emph{self-attention} mechanism, which enables the model to weigh the relevance of each part of an input sequence relative to other parts, regardless of their distance from each other in the sequence. Unlike recurrent neural networks, which process data sequentially and can struggle with long-range dependencies, Transformers use that mechanism to directly model these dependencies in parallel.

As the authors state at the end of the paper \cite{Liu2022}: \begin{quote}Possible next steps include investigating more powerful model architectures such as the Transformer.\end{quote} We analyzed various setups involving Transformer and attention mechanisms, preserving the training and evaluation setup.

\subsection{Vanilla Transformer}

We tested the vanilla architecture of the Transformer model, limited to the encoder only\footnote{There is no decoding in the transcription task.}. As a rule of thumb, we decided to keep the model size comparable to the original network. Different number of embedding sizes have been employed: $128$, $256$ and $512$ dimensions. 

As Transformers are primarily used for sequential discrete data, we used an additional encoding scheme with discretized note durations and note velocities. We also added feature embeddings, concatenated before positional encoding. Preliminary experiments showed the superiority of that scheme over durations and velocities encoded by floating point numbers. See Figure \ref{vanilla_transformer_encoder_with_embedding} for architecture comparison.

The note durations have been encoded using the same number of features as onsets, and the velocity, as a minor parameter, has been reduced to 8 discrete categories. In total, we have: \[\underbrace{128}_{\textrm{pitch}}+\underbrace{401}_{\textrm{onset}}+\underbrace{401}_{\textrm{duration}}+\underbrace{8}_{\textrm{velocity}} = 938\] features.

\begin{figure}[ht!]
\centering
\begin{tabular}{cc}a)
\input{sources/graphs/vanilla_transformer_encoder.tex} & b)
\input{sources/graphs/vanilla_transformer_encoder_with_embedding.tex}
\end{tabular}
\caption[The Transformer Encoder block.]{The Transformer Encoder block: a) default encoding, b) feature embedding. The second architecture has been chosen for the further experiments. The positional encoding is optional.}
\label{vanilla_transformer_encoder_with_embedding}
\end{figure}

For the training, we decreased the learning rate from the default value \num{1e-3} to \num{1e-4} and we set a warmup of 2500 steps. Variants without positional encoding have been tested as well\footnote{Some language models have been reported to learn comparably good without positional encodings \cite{Haviv2022}.}.

We stick to the default \texttt{pytorch} implementation of the Transformer encoder layer.

\subsection{Results}

In general, Transformers didn't perform as good as the base model. Positional encoding worsened the results in most of the cases. Even with comparable validation results on certain subtask, Transformer-based models didn't generalize well enough to be on par with the original model. Only the Transformer-based time signature model outperformed the original, rather simple, convolutional neural classifier.

We present various setups for all submodels: the beat model, the hand part model, the key and time signature models.

\subsection{Beat Model}

The vanilla Transformer architecture couldn't learn properly the correct beat prediction. It fails especially for downbeat classification, for all variants of the model.

\begin{table}[ht!]
\centering
\input{sources/tables/beat_transformer}
\caption[Transformer results for the beat model.]{Transformer results for the beat model.}
\label{beat_transformer}
\end{table}

A classical position encoding turned out to be not helpful, and even worsened the results. This suggests that for Transformers in order to properly predict the metric structure of a piece, a more adequate positional encoding is needed. The \emph{Rotary Positional Embedding} (RoPE) \cite{Su2024} is a fairly recent technique that could be worth examining in that scenario.

\subsection{Hand Part Model}

While the base hand part model outperformed all Transformer-based architectures, the latter in many cases were very close to the original one. Positional encoding slightly improved the results, although the overall results is comparable.

\begin{table}[ht!]
\centering
\input{sources/tables/hand_part_transformer}
\caption[Transformer results for the hand part model.]{Transformer results for the hand part model.}
\label{hand_part_transformer}
\end{table}

However, the MV2H metric results were noticeably worse, despite comparable results on the validation set. 

\subsection{Key Signature Model}

For the key signature model, different Transformers achieved superior performance over the base model but usually only for one evaluation category. Yet, for all models, the MV2H results didn't get close to the the original, high result of the harmony component.

\begin{table}[ht!]
\centering
\input{sources/tables/key_signature_transformer}
\caption[Transformer results for the key signature.]{Transformer results for the key signature.}
\label{key_signature_transformer}
\end{table}

\subsection{Time Signature Model}

Transformed-based architectures outperformed the base convolutional layer network in the binary classification task.  

\begin{table}[ht!]
\centering
\input{sources/tables/time_signature_transformer}
\caption[Transformer results for the time signature.]{Transformer results for the time signature.}
\label{time_signature_transformer}
\end{table}

The time signature model does not affect the MV2H metric.

\section{Temporal Convolutional Network}

Classical convolutional networks usually have a rather small receptive field\footnote{\emph{Receptive field} can be understood as the size of the region in the input data that produces a feature \cite{Araujo2019}. For a point on a grid, these and only these parts in that region are accessible by the network.}, and the canonical way to enlarge the size is to stack several layers of the network. As the input sequence can be very large, covering greater portions of data requires more model parameters, which at some point may yield too complex architecture for a given data scenario.

\subsection{Dilated Convolution}

To keep the size of the model small, one could use a \emph{dilated convolution}, which expands the receptive field by spacing out the kernel element with gaps, called \emph{dilation rates}.

\begin{figure}[ht!]
\centering
\input{sources/graphs/dilated_convolution}
\caption[Dilated convolution.]{Dilated convolution with kernel size $k = 3$ and dilation rate $d = 2$, illustrated on the $(-1, 1)$ point on a $7 \times 7$ grid.}
\end{figure}

Dilated convolutional layers increase the receptive field of the network exponentially with linear parameter accretion. 

While the idea can be traced back to wavelet decompositions from 1989 \cite{Holschneider1989}, the rediscovery of the method in the machine learning field can be attributed to Chen et al. \cite{Chen2015}.

\subsection{Temporal Convolutional Network}

Temporal Convolutional Network (TCN), introduced by \cite{Colin2016}, leverage the idea of dilated convolutions, to cover varied regions of data in a hierarchical way. The network consists of layers of dilated convolutions, where the dilation rate increases exponentially, starting from $d = 1, 2, 4$ and so on.

For sequential or time-series data, this structure strikes a balance between capturing both short-term and long-term dependencies. While TCNs can look ahead in time, they exhibit behavior similar to recurrent networks.

They are several advantages of using TCNs over recurrent networks: \begin{itemize}
	\item Recurrent networks suffer from inability to parallelize computation, as the result of next state is dependent on the previous one. This is no longer a problem of convolutional networks. The TCNs can be trained much faster.
	\item Recurrent networks need careful architecture adjustments to capture long-term relationships. Dilated convolutions allows to directly control the range of the field.
	\item The training of convolutional networks is usually stable. Recurrent networks are reported to be unstable during training. One of the reasons behind that phenomenon is the problem of vanishing/exploding gradients. However, gated recurrent units mitigate some of these issues. 
\end{itemize}

In some cases, these advantages come without sacrificing the performance. In the next sections we are going to show that indeed is the case for certain transcription subtasks. Before that, let us provide more details on the exact model structure.

\subsection{Model Architecture}

Since the musical features are not spatial, the convolutions are one-dimensional. We used the TCN variant of the kernel size $9$, and dilation rates $1$, $2$ and $4$. The receptive field $r$ can be calculated as the following sum: \[r = 1 + \sum_{i=1}\left(k_i - 1\right)\cdot d_i\] where $k_i$ is the kernel size, and $d_i$ is the dilation rate at $i$-th layer. Here, the receptive field after $3$ layers is $r_3 = 57$. See Figure \ref{temporal_convolutional_network} for the architecture layout.

\begin{figure}[ht!]
\centering
\input{sources/graphs/temporal_convolutional_network}
\caption[The Temporal Convolutional Block.]{The Temporal Convolutional Block.}
\label{temporal_convolutional_network}
\end{figure}

We considered two variants of the model: with $128$ and $256$ number of filters per convolutional layer.

The training/evaluation setup remained exactly the same.

\subsection{Results}

The experiments conducted using TCNs shows that, for certain models, the networks achieved comparable of superior results while having smaller number of parameters, and, due to the nature of the architecture, training much faster than recurrent networks. 

\subsubsection{Beat Model}

The neural network of the beat quantization model has been replaced by TCNn. The original model performed better than the both TCN variants, however the bigger TCN model performed only slightly worse than the original model.

\begin{table}[ht!]
\centering
\input{sources/tables/beat_tcn}
\caption[Temporal Convolutional Network results for the beat model.]{Temporal Convolutional Network results for the beat model.}
\label{beat_tcn}
\end{table}

Increasing the number of layers not only didn't improve the results but worsened the quality of the model.

The dynamic programming algorithm for out-of-note beat prediction has been left intact.

\subsubsection{Hand Part Model}

The TCN hand part model comparable results for both two variants, while having a much smaller and faster network.

\begin{table}[ht!]
\centering
\input{sources/tables/hand_part_tcn}
\caption[Temporal Convolutional Network results for the hand part model.]{Temporal Convolutional Network results for the hand part model.}
\label{hand_part_tcn}
\end{table}

This shows no strong benefits of using recurrent networks in the case of hand part assignment.

\subsubsection{Key Signature Model}

The key signature model based on TCNs was not on par with the original model. The convolutional model struggled especially with rarer key signatures, which is visible by the discrepancy between macro and weighted metrics. 

\begin{table}[ht!]
\centering
\input{sources/tables/key_signature_tcn}
\caption[Temporal Convolutional Network results for the key signature model.]{Temporal Convolutional Network results for the key signature model.}
\label{key_signature_tcn}
\end{table}

Variants with more layers didn't improve the quality of the model.

\subsubsection{Time Signature Model}

The second version of the model consisted only on convolutional layers, without GRUs. The TCN architecture outperformed this model greatly.

\begin{table}[ht!]
\centering
\input{sources/tables/time_signature_tcn}
\caption[Temporal Convolutional Network results for the time signature model.]{Temporal Convolutional Network results for the time signature model.}
\label{time_signature_tcn}
\end{table}

As in the case of ablation studies, the time signature model does not affect the MV2H metric.

\section{Dynamics}

In a musical score, dynamics describe how loud (or hard) a note should be played. 

We enhanced the model by adding a new transcription submodel. It is a classification model with ten categories, ranging from $\lilyDynamics{pppp}$ to $\lilyDynamics{ffff}$. We made a few simplifications regarding score markings interpretations: \begin{itemize}
	\item \emph{Crescendi} and \emph{diminuendi} have been not considered.
	\item Similarly, single note accents $>$ are ignored.
	\item We assumed that each dynamics marking affects both hands at the same time. In other words, we disallow separate dynamics in our model.
\end{itemize}

Similarly to the key signature, the actual markings are shown only when the dynamics change. We assume then that the dynamics value is constant, and, if there is none provided, \emph{mezzo-forte} is a default one.

The dynamics model is independent of all other submodels.

\subsection{Architecture}

The architecture of the model follows the same design as other submodules of the model. The dynamics module reflects the structure of the key signature model and consists of convolutional layers followed by a GRU block and the final linear output, with 10 categories.

See Figure \ref{model_architecture_extended} for the model architecture, with later time signature model reduction and removed the quantization submodel.

\begin{figure}[!ht]
\centering
\input{sources/graphs/model_architecture_extended}
\caption[The extended and architecture of the model.]{The extended architecture of the model by the \emph{dynamics} model. In the newer version of the base model, the time signature model has been reduced to a convolutional classifier. The quantization model is not shown as it does not take part in the final MIDI score generation.}
\label{model_architecture_extended}
\end{figure}

The dynamics block may be replaced by any other considered block (Transformer or TCN) since the architecture of the entire model is modular.

\subsection{Challenges}

There were two main serious challenges in the process of development the dynamics model: \begin{itemize}
	\item There are no direct dynamics annotations in any of the dataset considered.
	\item The interpretation of dynamics in the performances are subjective and thus of a large variance.
\end{itemize}

\subsubsection{Dynamics Data}

The ACPAS dataset does not contain information of dynamics at. No MIDI file encodes dynamics marking, nor any annotation describes that musical feature in the data. Out of all 3 sources, only the ASAP dataset provides dynamics annotations in the form, but only for the ground truth score in the MusicXML format.

For the performance MIDI files from the ASAP dataset we recovered dynamics annotations using an imperfect automation process. The Table \ref{annotations_dynamics} shows the augmented annotations structure on an example.

\begin{table}[ht!]
\centering
\input{sources/tables/annotations_dynamics}
\caption[Dynamics TSV annotations for the ASAP dataset.]{An augmented example of Table \ref{annotations}, with a starting \emph{forte} dynamics marking encoded as $f$.}
\label{annotations_dynamics}
\end{table}

Moreover, not all musical pieces use dynamics. Pieces written for harpsichord or organs which lack dynamic control, as certain Bach's fugues. This makes the dataset even more limited in terms of dynamics markings.

\subsubsection{Matching Algorithm}

The matching algorithm parses the original ground truth MusicXML score file in order to extract positions of dynamics annotations aligned to certain block of notes.

Performance MIDI files use absolute time in seconds, while scores measure time in beats, hence the positions cannot be compared directly. For initial experiments, we employed a naive matching algorithm based on the ratio of the number of already played notes to the number of all notes. Since some notes in the performance are either missing or superfluous, some slight deviations are allowed. 

A chord with a dynamics annotation from a score is matched to a chord consisting of the same notes, and on the same position as in the original. Unmatched or misaligned chords are discarded. 

A more sophisticated algorithm could be employed (as DTW), however we aimed for simplicity. We believe that the effort of providing high-quality dynamics data annotations to the dataset is unavoidable to achieve reliable dynamics models, and even more advanced matching algorithms are insufficient. The presented solution is only a preliminary work towards incorportating the richer vocabulary of a music score.

\subsection{Subjectivity of Dynamics Markings}

The second challenge comes from the fact that the dynamics annotations are not objective nor precise. The interpretation of these depends on musical context which is not only encompassed through MIDI data. That includes: the instrument on which the music is supposed to be played, music era and genre, and finally, the playing style of a performer. 

This leads to a high variance of the expected outcome, as the correspondence between a score and a performance is far from strict.

\subsection{Measure}

We used a standard negative log-likelihood loss as in the case of key signature model. We employed the standard set of multiclass metrics, with $F_1$ macro metric as the final score, mimicking the evaluation scheme for the key signature model.

In order to no penalize misclassification of similar classes (e.g. $\lilyDynamics{f}$ from $\lilyDynamics{ff}$), one could use standard distance between labels, where $0$ is the softest dynamics, $\lilyDynamics{pppp}$, and $1$ is the loudest possible one, $\lilyDynamics{ffff}$. Though, we used exactly the same setup as for the key signature model in our approach.

\subsubsection{MV2HD}

Since dynamics of notes can be evaluated independently of other features, assuming they correctly belong to the note stream, we propose an enhancement to the MV2H metric with the dynamics component: \textbf{MV2HD} where D stands for dynamics. The dynamics submetric is simply aforementioned $F_1$ score for correctly aligned notes, and the total metric can be viewed as: \[\textrm{MV2HD} = \tfrac{5}{6}\textrm{MV2HD} + \tfrac{1}{6}F_1\] where $F_1$ is the dynamics submetric score. Since the $F$-score lie in the unit interval $[0, 1]$, the MV2HD metric. The weight $\tfrac{1}{6}$ of dynamics is disputable, as the role of dynamics in the is of not the same importance as for instance meter alignment, however the value has been chosen for the sake of symmetry.

Due to dynamics independency of other features, the metric still obeys the principle of disjoint penalties.

As of now, the MV2HD metric is only a theoretical proposition and requires implementation to the Java code provided by \cite{McLeod2019}.

\subsubsection{Results}

\missing