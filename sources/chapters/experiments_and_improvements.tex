\chapter{Experiments and Improvements}\label{experiments_and_improvements}

In order to enhance the proposed model, we employed different strategies for finding improvements: \begin{itemize}
	\item Removing some input features to improve the robustness of the model.
	\item Testing smaller, more data-efficient networks to optimize the current architecture.
	\item Experimenting with more powerful architectures, such as Transformers, which have demonstrated remarkable success in natural language processing (NLP).
	\item Extending the model by introducing an additional module for handling other musical elements: dynamics. \end{itemize}
	
In subsequent experiments we stepped out from the full Transformer for attention mechanism only. We wanted to check whether access to the entire tensor sequence is beneficial for the transcription task. However, in the preliminary experiments combinations of self-attention mechanism with other components didn't perform as well as the base model nor Transformer-based ones, and the self-attention-based models have been excluded from the study.
	
Let us present results of these experiments.

\section{Ablation Studies} \label{ablation_studies}

To validate the assumptions presented in Section \ref{robustness_analysis}, we conducted ablation studies by training models that utilized only a restricted set of input features. Specifically, the following configurations were employed: \begin{itemize}
	\item The \textbf{hand part} model was trained only on note pitches and onsets.
	\item The \textbf{key signature} model was trained only on note pitches and onsets.
	\item The \textbf{time signature} model was trained using only note onsets.
\end{itemize}

As a result, all models became invulnerable to transformations described in the Section \ref{perturbations}. This section evaluates the impact of such feature reduction on model performance.

Each model was evaluated using two metrics:
\begin{itemize} \item Model-specific $F_1$ scores. \item The MV2H metric, with restricted models replacing their corresponding original components. \end{itemize}

All evaluations were conducted on the test subset of the dataset, with both base and restricted models trained exclusively on the training set. For a fair comparison, the model with the best validation $F_1$ score across 50 epochs was selected for each ablation study.

\subsection{Beat Tracking} \label{beat_tracking}

The authors of the original paper provided ablation studies for the beat-tracking model, analyzing how the omission of features and data augmentation influenced its performance. The results of these analyses are summarized in Tables \ref{beat_tracking_feature_omission} and \ref{beat_tracking_data_augmentation}.

\begin{table}[ht!]
\centering
\input{sources/tables/feature_omission}
\caption[Feature omission study.]{Feature omission study \cite{Liu2022}.}
\label{beat_tracking_feature_omission}
\end{table}

Not surprisingly, note onsets are the most important feature used by the beat-tracking model. While neglecting other features doesn't hardly impact the model, still feature omission results in a slight quality degradation.

Velocity may carry some information about beats as they tend to occur along with heavier notes \cite{Liu2022}. Harmonic structure inferred by pitch may also transfer such information, for example it is quite common to play chords at the beginning of a measure. Note durations may also offer hints about the underlying meter.

\begin{table}[ht!]
\centering
\input{sources/tables/data_augmentation}
\caption[Ablation study for data augmentation.]{Ablation study for data augmentation \cite{Liu2022}.}
\label{beat_tracking_data_augmentation}
\end{table}

\subsection{Hand Part Model}

A variant of the hand part model was trained without using note durations or velocities. As the hand part assignment influences only the voice separation subscale of the MV2H metric, other submetrics were excluded from the evaluation.

\begin{table}[ht!]
\centering
\input{sources/tables/hand_part_ablation}
\caption[Ablation study for the hand part model.]{Ablation study for the hand part model.}
\label{hand_part_ablation}
\end{table}

The results (Table \ref{hand_part_ablation}) show comparable performance between the restricted and original models across all metrics. This indicates that velocity provides marginal, if any, additional information for hand part assignment. These findings align with the observations in Section \ref{results}, where velocity had a limited influence on model predictions.

\subsection{Key Signature Model}

The restricted key signature model, trained using only note pitches and onsets, achieved performance similar to the original model.

\begin{table}[ht!]
\centering
\input{sources/tables/key_signature_ablation}
\caption[Ablation study for the key signature model.]{Ablation study for the key signature model.}
\label{key_signature_ablation}
\end{table}

Table \ref{key_signature_ablation} shows the results with included the harmony MV2H subscale. Let us remind that weighted metric results are calculated independently, and weighted by the number of true labels of each class, while macro metrics are unweighted.

The observed differences fall within the expected range of model stochasticity. The result agrees also with the negligible influence of other factors to the final model decision on key signature, described in Section \ref{results}.

\subsection{Time Signature Model}

A restricted time signature model, trained solely on note onsets, outperformed the original convolutional network classifier in terms of $F_1$ scores.

\begin{table}[ht!]
\centering
\input{sources/tables/time_signature_ablation}
\caption[Ablation study for the time signature model.]{Ablation study for the time signature model.}
\label{time_signature_ablation}
\end{table}

Interestingly, as the MV2H metric does not directly depend on time signature predictions, substituting the original model with the restricted version had no impact on the overall evaluation. This shows that not only introducing full information to the model is not beneficial, but actually hurts the performance of the model. 

\section{Transformers}

\emph{Transformers} are a class of neural network architectures introduced by Vaswani et al. in the landmark paper \emph{Attention Is All You Need} \cite{Vaswani2017}. Originally developed for natural language processing (NLP), have since become versatile and powerful framework for modeling sequential data across numerous other fields, including image processing \cite{Dosovitskiy2020}, audio generation \cite{Borsos2023} and even symbolic music analysis \cite{Zhu2021}.

\begin{figure}[ht!]
\centering
\input{sources/graphs/transformer}
\caption[The Transformer architecture.]{The Transformer architecture \cite{Vaswani2017}.}
\end{figure}

The key innovation in Transformers is the \emph{self-attention} mechanism, which enables the model to weigh the relevance of each part of an input sequence relative to other parts, regardless of their distance from each other in the sequence. Unlike recurrent neural networks, which process data sequentially and can struggle with long-range dependencies, Transformers use that mechanism to directly model these dependencies in parallel.

As the authors state at the end of the paper \cite{Liu2022}: \begin{quote}Possible next steps include investigating more powerful model architectures such as the Transformer.\end{quote} We analyzed various setups involving Transformer and attention mechanisms, preserving the training and evaluation setup.

\subsection{Vanilla Transformer}

We evaluated a standard Transformer encoder architecture, limited to the encoder block since the transcription task does not involve decoding. As a rule of thumb, we decided to keep the model size comparable to the original network. Different number of embedding sizes have been employed: $128$, $256$ and $512$ dimensions. 

Given that Transformers are designed for sequential discrete data, we applied an additional encoding scheme to discretize note durations and velocities. Features were concatenated prior to positional encoding. Preliminary experiments demonstrated that this approach outperformed the use of continuous floating-point encodings for durations and velocities. The note durations were encoded with the same number of discrete features as onsets, while velocities were reduced to eight discrete categories. The total feature space thus consisted of: \[\underbrace{128}_{\textrm{pitch}}+\underbrace{401}_{\textrm{onset}}+\underbrace{401}_{\textrm{duration}}+\underbrace{8}_{\textrm{velocity}} = 938\]

\begin{figure}[ht!]
\centering
\begin{tabular}{cc}a)
\input{sources/graphs/vanilla_transformer_encoder} & b)
\input{sources/graphs/vanilla_transformer_encoder_with_embedding}
\end{tabular}
\caption[The Transformer Encoder block.]{The Transformer Encoder block: a) default encoding, b) feature embedding. The $\frown$ symbols stands for concatenation. The second architecture was selected for subsequent experiments. The positional encoding is optional.}
\label{vanilla_transformer_encoder_with_embedding}
\end{figure}

For training, the learning rate was reduced to \num{1e-4}, with a warmup period of 2500 steps. We also tested configurations without positional encoding\footnote{Certain language models have been shown to perform well without explicit positional encodings \cite{Haviv2022}.}. All experiments utilized the default \texttt{pytorch} Transformer encoder implementation.

\subsection{Results}

Overall, Transformer-based models did not match the performance of the base architecture. Positional encoding, in particular, often degraded performance. Even when achieving comparable validation results on specific subtasks, Transformer-based models struggled to generalize as effectively as the original network. The sole exception was the time signature model, where Transformers outperformed the original convolutional neural network.

We present various setups for all submodels: the beat model, the hand part model, the key and time signature models.

\subsection{Beat Model}

The vanilla Transformer architecture failed to learn accurate beat predictions, particularly for downbeat classification. This was consistent across all model variants.

\begin{table}[ht!]
\centering
\input{sources/tables/beat_transformer}
\caption[Transformer results for the beat model.]{Transformer results for the beat model. The symbol \emph{b} stands for beat prediction, and \emph{db} stands for downbeat classification. Models with the positional encoding are labeled as PE.}
\label{beat_transformer}
\end{table}

Sinusoidal position encoding turned out to be not helpful, and most of the time worsened the results a bit. This suggests that for Transformers in order to properly predict the metric structure of a piece, a more adequate positional encoding is needed. The \emph{Rotary Positional Embedding} (RoPE) \cite{Su2024} is a fairly recent technique that may warrant further exploration in this context.

Additionally, a note’s position within a song does not map cleanly onto tensor indices, as tensor positions lack a direct correspondence to the actual temporal context of note onsets. This indicates a need for an alternative positional encoding feature that can represent a note’s position relative to its onset.

\subsection{Hand Part Model}

While the original hand part model consistently outperformed Transformer-based alternatives, the latter achieved results close to the base model in many cases. Positional encoding slightly improved performance, but the overall difference was rather small.

\begin{table}[ht!]
\centering
\input{sources/tables/hand_part_transformer}
\caption[Transformer results for the hand part model.]{Transformer results for the hand part model.}
\label{hand_part_transformer}
\end{table}

Notably, despite comparable validation performance, Transformer-based hand part models performed worse on the MV2H metric, suggesting issues with generalization.

\subsection{Key Signature Model}

For the key signature model, some Transformer configurations achieved superior performance in individual evaluation categories. However, none of these models matched the high harmony component score achieved by the original model in the MV2H metric.

\begin{table}[ht!]
\centering
\input{sources/tables/key_signature_transformer}
\caption[Transformer results for the key signature.]{Transformer results for the key signature.}
\label{key_signature_transformer}
\end{table}

\subsection{Time Signature Model}

Transformer-based architectures outperformed the original convolutional network for the binary time signature classification task. This is rather unsurprising as the base network was not so expressive.

\begin{table}[ht!]
\centering
\input{sources/tables/time_signature_transformer}
\caption[Transformer results for the time signature.]{Transformer results for the time signature.}
\label{time_signature_transformer}
\end{table}

The time signature model does not affect the MV2H metric.

\subsection{Future Work}

While Transformers hold significant promise for sequence modeling tasks, the results suggest that their application to musical transcription may require more specialized adaptations. In particular: \begin{itemize}
	\item Transformers struggled to outperform the base model for the beat prediction. This suggests that a different quantization scheme may be needed, potentially involving modifications to the target variables.
	\item Classical positional encodings were often detrimental, indicating a need for more sophisticated positional representations. Moreover, note positions in a sequence do not directly map to tensor indices, and the order of notes alone provides insufficient information about temporal relationships within the sequence.
	\item The dataset used in these experiments is not fully representative of performance MIDI, as it includes a significant number of human-crafted MIDI files. Transformers likely require larger and more diverse datasets to achieve superior results. Addressing that issue demands refining the dataset and acquiring more data of high quality.
	\item The time signature model could benefit from two enhancements: expanding its scope to include a wider variety of time signature classes as initially, and incorporating beat and downbeat predictions as auxiliary inputs to refine its classification accuracy.
\end{itemize}

\section{Temporal Convolutional Network}

Classical convolutional networks usually have a rather small receptive field\footnote{\emph{Receptive field} can be understood as the size of the region in the input data that produces a feature \cite{Araujo2019}. For a point on a grid, these and only these parts in that region are accessible by the network.}, and the canonical way to enlarge the size is to stack several layers of the network. As the input sequence can be very large, covering greater portions of data requires more model parameters, which at some point may yield too complex architecture for a given data scenario.

\subsection{Dilated Convolution}

To keep the size of the model small, one could use a \emph{dilated convolution}, which expands the receptive field by spacing out the kernel element with gaps, called \emph{dilation rates}.

\begin{figure}[ht!]
\centering
\input{sources/graphs/dilated_convolution}
\caption[Dilated convolution.]{Dilated convolution with kernel size $k = 3$ and dilation rate $d = 2$, illustrated on the $(-1, 1)$ point on a $7 \times 7$ grid.}
\end{figure}

Dilated convolutional layers increase the receptive field of the network exponentially with linear parameter accretion. 

While the idea can be traced back to wavelet decompositions from 1989 \cite{Holschneider1989}, the rediscovery of the method in the machine learning field can be attributed to Chen et al. \cite{Chen2015}.

\subsection{Temporal Convolutional Network}

Temporal Convolutional Network (TCN), introduced by \cite{Colin2016}, leverage the idea of dilated convolutions, to cover varied regions of data in a hierarchical way. The network consists of layers of dilated convolutions, where the dilation rate increases exponentially (e.g., $d = 1, 2, 4, \dots$) across layers.

For sequential or time-series data, this structure efficiently captures both short- and long-term dependencies. Unlike recurrent networks, TCNs process data in parallel, making them computationally efficient.

Compared to recurrent neural networks, TCNs offer several advantages:
\begin{itemize}
	\item \textbf{Parallel Computation}: TCNs allow parallel processing of sequences, unlike recurrent networks where computation is sequential and each state depends on the previous one. This significantly reduces training time.
	\item \textbf{Long-Term Dependency Modeling}: TCNs achieve flexible receptive fields through dilated convolutions, enabling the direct control of dependency range without architectural adjustments.
	\item \textbf{Stability}: Convolutional networks, including TCNs, generally exhibit stable training dynamics. Recurrent networks, on the other hand, are prone to issues such as vanishing or exploding gradients, although gated mechanisms like GRUs mitigate some of these challenges. \end{itemize}

In many cases, these advantages come without sacrificing performance, as we demonstrate in subsequent sections. We evaluate TCNs on various transcription subtasks and provide details about the model structure.

\subsection{Model Architecture}

Since musical features are not spatial, the convolutions are one-dimensional. For our experiments, we used a TCN variant with a kernel size of $9$ and dilation rates of $1$, $2$, and $4$. The receptive field $r$ for $L$ layers can be calculated as follows: \[r = 1 + \sum_{i=1}\left(k_i - 1\right)\cdot d_i\] where $k_i$ and $d_i$ denote the kernel size and dilation rate, respectively, at the $i$-th layer. For three layers, the receptive field is $r_3 = 57$. See Figure \ref{temporal_convolutional_network} for an illustration of the architecture.

\begin{figure}[ht!]
\centering
\input{sources/graphs/temporal_convolutional_network}
\caption[The Temporal Convolutional Block.]{The Temporal Convolutional Block.}
\label{temporal_convolutional_network}
\end{figure}

We tested two model variants, with $128$ and $256$ filters per convolutional layer. The training and evaluation procedures followed the same setup as the baseline models.

\subsection{Results}

Experiments with TCNs showed that these networks achieved comparable or superior performance for certain transcription subtasks, while requiring fewer parameters and training faster compared to recurrent networks.

We use standard abbreviations for the metrics: \emph{acc}, \emph{prec}, \emph{rec} and $F$ for accuracy, precision, recall and $F_1$ score, respectively.

\subsubsection{Beat Model}

The beat quantization task was less suited to the TCN architecture. While the larger TCN variant performed close to the baseline, the original model consistently outperformed both TCN variants.

\begin{table}[ht!]
\centering
\input{sources/tables/beat_tcn}
\caption[Temporal Convolutional Network results for the beat model.]{Temporal Convolutional Network results for the beat model.}
\label{beat_tcn}
\end{table}

Adding more layers or increasing the receptive field worsened model performance, indicating that the architecture may require further tuning for this task. 

The dynamic programming algorithm for out-of-note beat prediction has been left intact.

\subsubsection{Hand Part Model}

The TCN-based hand part model achieved results comparable to the baseline while benefiting from a significantly smaller and faster architecture. This suggests that recurrent networks are not strictly necessary for the hand part assignment task.

\begin{table}[ht!]
\centering
\input{sources/tables/hand_part_tcn}
\caption[Temporal Convolutional Network results for the hand part model.]{Temporal Convolutional Network results for the hand part model.}
\label{hand_part_tcn}
\end{table}

\subsubsection{Key Signature Model}

The TCN-based key signature model struggled with rarer key signatures, resulting in a noticeable gap between macro and weighted metrics. Variants with more layers didn't improve the quality of the model.

\begin{table}[ht!]
\centering
\input{sources/tables/key_signature_tcn}
\caption[Temporal Convolutional Network results for the key signature model.]{Temporal Convolutional Network results for the key signature model.}
\label{key_signature_tcn}
\end{table}

\subsubsection{Time Signature Model}

The TCN-based time signature model significantly outperformed the baseline convolutional network. As observed in earlier experiments, time signature estimation does not impact the MV2H metric.

\begin{table}[ht!]
\centering
\input{sources/tables/time_signature_tcn}
\caption[Temporal Convolutional Network results for the time signature model.]{Temporal Convolutional Network results for the time signature model.}
\label{time_signature_tcn}
\end{table}

\section{Dynamics}

In a musical score, dynamics indicate the intensity or volume with which notes should be played. To enhance the transcription capabilities of the model, we introduced a new submodule for transcribing dynamics. This submodule classifies notes into one of ten dynamic levels, ranging from $\lilyDynamics{pppp}$ (very soft) to $\lilyDynamics{ffff}$ (very loud). Several simplifications were made to address common challenges in interpreting dynamics:
\begin{itemize}
	\item Gradual dynamic changes such as \emph{crescendo} and \emph{diminuendo} were excluded.
	\item Accents on individual notes (marked as $>$) were not considered.
	\item Dynamics were assumed to apply uniformly to both hands, disallowing independent dynamics for left and right hands.
	\end{itemize}

As in standard musical practice, dynamic markings are displayed only when there is a change. If no marking is explicitly given, \emph{mezzo-forte} is assumed as the default level. The dynamics model operates independently of the other transcription submodules.

\subsection{Architecture}

The architecture of the dynamics model follows the modular design of other submodules. It mirrors the structure of the key signature model, comprising convolutional layers, a GRU block, and a final linear layer with 10 output categories corresponding to the dynamics levels.

The extended architecture of the model, including the dynamics module, is shown in Figure \ref{model_architecture_extended}. The modularity of the model enables the dynamics block to be replaced with alternative architectures, such as Transformer or TCN-based blocks.

\begin{figure}[!ht]
\centering
\input{sources/graphs/model_architecture_extended} \caption[Extended model architecture with dynamics module.]{The extended architecture of the model, incorporating the \emph{dynamics} module. Note that the quantization model is not shown, as it does not directly contribute to the final MIDI score generation.}
\label{model_architecture_extended}
\end{figure}

\subsection{Challenges}

Two major challenges arose during the development of the dynamics model:
\begin{itemize} \item No direct dynamics annotations exist in any of the datasets considered. \item The interpretation of dynamics in musical performances is subjective and thus varies considerably. \end{itemize}

\subsubsection{Dynamics Data}

The ACPAS dataset does not contain any direct information about dynamics. Neither the MIDI files nor the annotations provide this data. Among the three datasets used, only the ASAP dataset includes dynamics annotations, but only for the ground truth score in the MusicXML format.

For the performance MIDI files in the ASAP dataset, dynamics annotations were recovered through an imperfect automation process. Table \ref{annotations_dynamics} presents an example of the augmented annotation structure.

\begin{table}[ht!]
\centering
\input{sources/tables/annotations_dynamics}
\caption[Dynamics TSV annotations for the ASAP dataset.]{An example of augmented dynamics annotations in the ASAP dataset, showing a starting \emph{forte} dynamics marking encoded as $f$.}
\label{annotations_dynamics}
\end{table}

Additionally, certain musical pieces, such as those written for the harpsichord or organ (e.g., some of Bach's fugues), lack dynamic markings, limiting the dataset even more.

\subsubsection{Matching Algorithm}

The matching algorithm parses the original ground truth MusicXML score to extract dynamics annotation positions aligned to corresponding note blocks.

MIDI performance files use absolute time in seconds, while scores are in beats, so direct comparison is not possible. To overcome this, we employed a basic matching algorithm that relies on the ratio of already played notes to the total number of notes. Deviations are allowed to account for missing or extra notes.

Chords with a dynamics annotation from the score are matched to chords with identical notes and corresponding positions in the original performance. Any unmatched or misaligned chords are discarded. We achieved the total coverage of $58.6\%$ of all dynamics markings.

While more sophisticated algorithms could be used, we opted for simplicity. We believe that high-quality dynamics annotations are essential for building a reliable dynamics model, and even more advanced matching algorithms are unlikely to resolve the core issue of data quality. The current solution is a preliminary step toward incorporating a richer vocabulary for musical scores.

\subsection{Subjectivity of Dynamics Markings}

A significant challenge in modeling dynamics is the inherent subjectivity of dynamics annotations. The interpretation of dynamics depends on various factors, including the instrument on which the music is performed, the era and genre of the music, and the performing style of the musician.

This variability leads to considerable differences in expected outcomes, as the relationship between the score and the performance is far from exact.

\subsection{Evaluation}

The model was trained using the standard negative log-likelihood loss, similar to the approach used for the key signature model. We evaluated the model using the standard set of multiclass classification metrics, with the macro $F_1$ score as the final evaluation metric, consistent with the evaluation scheme for the key signature model.

To avoid penalizing misclassifications between similar dynamic classes (e.g., distinguishing $\lilyDynamics{f}$ from $\lilyDynamics{ff}$), one could apply a standard distance metric between labels, where $0$ represents the softest dynamics, $\lilyDynamics{pppp}$, and $1$ represents the loudest, $\lilyDynamics{ffff}$. However, we followed the same setup as the key signature model for simplicity.

Since the intersection of the original test set and the ASAP dataset is empty, the test set is not available. 

\subsubsection{MV2HD}

Since dynamics can be evaluated independently of other features, assuming they are correctly aligned with the note stream, we propose an enhancement to the MV2H metric, called \textbf{MV2HD}, where D stands for dynamics. The dynamics submetric is simply the $F_1$ score for correctly aligned notes, and the total metric is calculated as: \[\textrm{MV2HD} = \tfrac{5}{6}\textrm{MV2HD} + \tfrac{1}{6}F_1^{(D)}\] Here, $F_1^{(D)}$ is the dynamics submetric score. Since the $F_1$ score lies in the interval $[0, 1]$, the MV2HD metric is bounded between $0$ and $1$. The weight of $\tfrac{1}{6}$ for dynamics is debatable, as dynamics do not play the same role as, for example, meter alignment. However, this weight was chosen for symmetry in the overall metric.

Because dynamics are independent of other features, the MV2HD metric still adheres to the principle of disjoint penalties.

Currently, the MV2HD metric is a theoretical proposal and would require implementation in the Java code provided by McLeod et al. \cite{McLeod2019}.

\subsubsection{Results}

The base architecture (convolutional layers followed by a GRU block) achieved the best result among all considered architectures. However, the final results is unsatisfactory.

\begin{table}[ht!]
\centering
\input{sources/tables/dynamics_results}
\caption[Results for the dynamics model.]{Results for the dynamics model.}
\label{dynamics_results}
\end{table}

These results are understandable, given the challenges discussed earlier. We believe that refining the dataset, particularly with respect to dynamics annotations, could significantly enhance the performance of all models. The availability of high-quality data appears to be a more important factor than the specific architectural choices for the model.