\chapter{Conclusions}\label{conclusions}

We have explored the automatic transcription task from performance MIDI to symbolic scores in detail, discussing its components, challenges, and earlier methods. The state-of-the-art deep learning model has been analyzed thoroughly, with an in-depth examination of its performance and behavior.

Our analysis included the following: \begin{itemize}
	\item A comprehensive evaluation of the primary model's performance and behavior.
	\item Ablation studies addressing model anomalies, especially the undesired influence of the velocity feature.
	\item Development of local feature importance methods to interpret the model's decisions.
	\item Comparison with alternative architectures, such as Transformers and Temporal Convolutional Networks (TCNs). \end{itemize}

Additionally, we proposed an enhancement to the model to handle an additional musical feature: dynamics.

Below is a summary of the results.

\section{Robustness Analysis}

Our analysis uncovered artifacts in the transcription models, particularly related to velocity's influence on hand part assignment. The time signature model showed sensitivity to transformations that should have been inconsequential, while the key signature model remained robust across all perturbations.

By employing data augmentation and selective feature removal, we mitigated these undesired behaviors without degrading model performance.

\section{Feature Importance Methods}

Symbolic music data presents unique challenges for explainable artificial intelligence (XAI). Existing XAI methods often do not adapt well to this domain. Developing more targeted and specialized XAI tools could improve model interpretability.

We implemented two methods to analyze submodel behavior: \begin{itemize}
	\item A semi-LIME approach to visualize how velocity features influenced hand part assignment.
	\item A note omission method to quantify how individual notes contributed to key signature assignment decisions. \end{itemize}

These methods provided insights into the models' decision but have certain limitations. The semi-LIME approach, for example, assumes feature independence. Ont he other hand, both methods disregard temporal relationships.

A broader challenge in developing XAI tools for symbolic music lies in the absence of a meaningful pitch space representation that encodes musical relationships comprehensively. The current treatment of pitch as a discrete space hinders the analysis of key signature assignment and limits the interpretability of related tasks.

\section{Experiments}

A series of experiments compared the state-of-the-art performance with alternative architectures. Below are the findings:

\subsection{Transformers}

Vanilla Transfomers didn't perform as good as other networks. We hypothesize that Transformer architecture could outperform the proposed network but in a data-abundant scenario. In the considered scenario the entire dataset is though too small (or not diverse enough) and needs more data-efficient models. For a limited amount of data, Transformers may be prone to overfitting.

Another suspected reason is that some features like hand part assignment are local, and long-term dependencies are not that useful as they could be, or they are not encoded correctly. This indicates that the traditional trigonometric positional encoding does not serve the purpose well. Some other form of positional encodings, that represent an actual onsets more than the position in a tensor sequence, may significantly improve the quality of the model.

Recently, there were a successful use of Transformers in the field \cite{Beyer2024} on a subset of the considered dataset. The authors provide a RoFormer encoder-decoder model with a custom token embedding. They also provided additional data augmentations as duration and onset jitter, introducing noise to the note beginning and ends. The authors modified the initial ACPAS dataset as they claim that a hand-crafted part of the is not representative for the performance. 

The aforementioned work operates on different score format (MusicXML) and uses a different set of metrics than \cite{Liu2022}, making hard to compare both models without further investigation.

\subsection{Temporal Convolutional Networks}

TCNs provided a compelling alternative to recurrent architectures for certain transcription tasks. These networks are computationally efficient and maintain stability during training.

While TCNs achieved comparable results for hand part assignment and time signature models, their performance lagged in key signature prediction.

\subsection{Dynamics}

We enhanced the base model with a dedicated dynamics submodel to transcribe score dynamics, ranging from $\lilyDynamics{pppp}$ to $\lilyDynamics{ffff}$. While this addition captures a significant aspect of musical expression, two major challenges arose: 	\begin{itemize}
	\item \textbf{Data scarcity:} None of the datasets provided direct dynamics annotations, requiring heuristic and automated processes to align performance data with score annotations.
	\item \textbf{Subjectivity:} Dynamics markings are context-dependent and subjective, varying by instrument, genre, and performer interpretation. As a result, the relationship between a musical sheet and its realization is far from strict.	\end{itemize}

The dynamics model demonstrated an average, but reasonable under above circumstances, performance.

We proposed an extension of the MV2H metric, denoted MV2HD, which incorporates an $F_1$ score for dynamics. This enhancement maintains disjoint penalty principles and provides a more comprehensive evaluation of transcription quality. However, the metric is yet to be implemented.

Future work should focus on refining dynamics annotations in the dataset, implementation of the MV2HD metric, and experimenting with alternative architectures, such as Transformers and TCNs, may yield further improvements.