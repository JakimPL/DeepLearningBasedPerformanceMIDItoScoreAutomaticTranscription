\chapter{MIDI-to-Score Transcription Evaluation}\label{midi_to_score_transcription_evaluation}

When transcribing a performance MIDI into a musical score, one needs to evaluate how faithful an output is to the original sheet. A transcription quality metric should take a transcription of a performance-MIDI and a ground-truth score as input, and produce a non-negative real number as an output, ideally within the range $[0, 1]$.

As the musical transcription is a multifaceted process, measuring transcription fidelity is far from a trivial task. Music notation is complex, consisting of a variety of interdependent elements. Solely for that reason, it is difficult to achieve metric that simultaneously: \begin{itemize}
	\item assigns a single value to an entire transcription
	\item encompasses all aspects mentioned in the Section \ref{music_score_encoding}. 
\end{itemize}

On top of that, there are additional layers of complications: \begin{itemize}
	\item Even for a single aspect it may be not clear how to measure quality of a transcription. For example, assessing voice separation is a hard task on its own\footnote{The term \emph{voice} itself is ambiguous and there is no single definition that tells exactly what constitutes a voice, let alone how to separate voices. An extensive discussion on that topic can be found in \cite{Cambouropoulos2008}.}.
	\item Performers tend to deviate from the written score, whether intentional through artistic interpretation or not due to errors. For example, a pianist can introduce own ornaments or make mistakes by omitting certain notes or playing wrong ones. While the basic structure, such as time and key signature, is usually preserved in a performance, we may expect differences in a note stream. The goal of the evaluation is not to judge the performance's fidelity to the notation but rather the accuracy of a transcription.
	\item Musical elements are interconnected. A transcription error in one area can cascade into another. For instance, inaccurate beat recognition may make difficult to assign a proper time signature. It is not clear how to penalize a single transcription error once, and only once. It is also not easy to fairly attribute the faithfulness of a transcription of an interpretation that substantially diverges from the sheet.
	\item In general, there can be multiple valid notations of the same musical idea, and in some case neither of them are more correct than other ones. A well-designed metric should allows some degree of freedom when it does not hurt the quality of the transcription.
\end{itemize}

As a result, more often than not, single aspects of a model are being analyzed and evaluated in isolation to other parts of a transcription. The overall result may be an aggregation of submetrics, though this approach is not always justifiable.

Earlier approaches to assessing transcription quality were often lacking in rigor. Some common pitfalls in the literature include:\begin{itemize}
	\item {\bf Lack of Thorough Analysis}. A rigorous evaluation requires an assessment of all relevant aspects of a proposed solution. Failing to attribute vital parts of a model or algorithm renders the analysis incomplete.
	\item {\bf Limited Case Demonstration}. The results of a algorithm were often demonstrated on a very limited set of musical pieces. It is not obvious how a model performs outside showcased examples, and it endangers the research to cherry-picking. This is the case of both works \cite{Takeda2002} or \cite{Yang2005}, which are described in more detail in Chapter \ref{overview_of_existing_methods_for_midi_to_score_conversion}.
	\item {\bf No Public Dataset or Reproducibility}. For both training and evaluation, the dataset used should be clearly described and, ideally, made publicly available. Without this, results are difficult or impossible to reproduce, as seen in \cite{Takeda2002}.
	\item {\bf Absence of Human Expert Evaluation}. While statistical benchmarks are necessary for quantitative assessment, there may overlook nuances that are essential for performers relying on transcriptions. 
\end{itemize} 

In the following sections we will explore various attempts from the literature to provide transcription quality metric.

\section{Manual Attribution}

Manual attribution refers to any form of quality assessment that involves human intervention. It almost exclusively requires experts, as the task of evaluating music transcription demands understanding of both musical theory and performance practice. Human evaluators are typically trained musicians, musicologists, or composers who can assess various dimensions of a transcription beyond what automated systems can measure. These experts are called upon to determine not only the technical accuracy of a transcription but also its musicality, readability, and adherence to stylistic norms. The output of human evaluation may be either qualitative or quantitative. 

One of the key benefits of manual attribution is its ability to handle complex, nuanced and multi-layered aspects of transcription. Trained musicians are well-equipped to judge how well a transcription serves musicians.

Some research has investigated the relationship between computational metrics and human evaluation. A fairly recent study shows that, surprisingly enough, there is no significant correlation between certain transcription metrics\footnote{Two metrics \emph{missing note rate} and \emph{onset time error} had no significant correlation with human score. These metrics are going to be explained in further sections.} and human quality ratings \cite{Holzapfel2021}. However, combined metrics correlated more strongly (0.682) with human expert evaluations, although this correlation was still lower than the inter-rater correlation between two human experts (0.754).

Despite a large value of human evaluation, it comes with limitations. Manual attribution is time-consuming and labor-intensive. Carefully reviewing a transcription requires domain expertise, time and effort. Access to qualified experts can be costly, and the process is not easily scalable. Another drawback coming from manual attribution that evaluators may be subjective and biased towards certain notation practices. This of course not to say, that quantitative benchmarks are free of biases.

\section{Binary and Multiclass Classifier Metrics} \label{binary_and_multiclass_classifier_metrics}

The transcription problem can be viewed as a set of interdependent classification tasks. In particular: \begin{itemize}
	\item Beat quantization can be thought as the classification of beats and downbeats.
	\item The hand part model is a binary classifier (left or right hand).
	\item Time and key signatures assignment is a multiclass classification problem\footnote{The time signature model has been further reduced to a binary classifier.}. \end{itemize} Hence, all loss functions and metrics that apply to classifiers can be used both for both training and evaluation. Since these are standard measures, they won't be discussed in much detail, and notions of true/false positive and negatives are assumed to be known\footnote{A discussion on these metrics can be found in \cite[p. 142--144]{Manning2008}.}.

\subsection{Cross-entropy Loss}

The \emph{cross-entropy loss} is a commonly used loss function for training classifiers in machine learning, derived from the \emph{cross-entropy} between two probability distributions over a shared set of events: \[H(X, Y) = -\mathbb{E}_{t\sim X}\log Y(t) = -\sum_t P_X(t)\log P_Y(t)\] For a dataset with true labels $y_n$ and model predictions $\hat{y}_n$, where $n=1,\ldots,N$, the \emph{cross-entropy loss function} is defined as: \[\mathcal{L} = -\frac{1}{N}\sum_{n=1}^N\left[y_n\log \hat{y}_n + (1 - y_n)\log(1 - \hat{y}_n)\right]\] Further discussion with justification of cross-entropy can be found in \cite[p. 73--75, 178]{Goodfellow2016}.

\subsection{Accuracy, Precision and Recall}

In binary classification, one class is labeled as \emph{positive} (or \emph{true}) and the other as \emph{negative} (or \emph{false}). Items belonging to a class are \emph{relevant}, while those predicted by the model as belonging to the class are considered \emph{retrieved} \cite[p. 76]{Roelleke2022}.

The \emph{accuracy} of a predictor is defined as the ratio of correctly identified elements to the total number of elements: \[\textrm{accuracy} = \frac{\textrm{true positives} + \textrm{true negatives}}{\textrm{true positives} + \textrm{false positives} +\textrm{true negatives} + \textrm{false negatives}}\] \emph{Precision} represents the proportion of correctly retrieved items to the total number of items retrieved: \[\textrm{precision} = \frac{\textrm{true positives}}{\textrm{true positives} + \textrm{false positives}}\] \emph{Recall} measures the proportion of correctly retrieved items to the total number of relevant items: \[\textrm{recall} = \frac{\textrm{true positives}}{\textrm{true positives} + \textrm{false negatives}}\] These definitions naturally extend to multiclass classification.

\subsection{$F_1$ score}

\emph{$F_1$ score} (\emph{$F$-measure}) is a measure that equally balances precision and recall. It is defined as a harmonic mean of the precision and recall: \[F_1 = 2 \times \frac{\textrm{precision} \times \textrm{recall}}{\textrm{precision} + \textrm{recall}}\] For multiclass classification, two common methods can be used to compute the $F$-measure: \emph{macro} and \emph{micro} averaging \cite[p. 259--261]{Manning2008}. These approaches handle cases of class imbalance differently:\begin{itemize}
	\item \textbf{Macro $F_1$}: The unweighted average of $F_1$ scores across all classes, treating each class equally.
	\item \textbf{Micro $F_1$}: A weighted average of $F_1$ scores, where weights correspond to the number of instances in each class.\end{itemize}

In the context of hand part assignment, applying the $F_1$ score is dubious. It stems from the fact that the metric is sensitive to label switching; the score is affected by which hand is labeled as ``true'' \cite{Sokolova2009}, whether the labeling of hands should not matter at all. Nevertheless, we adopt this measure for consistency with the considered work.

\section{MUSTER: Music Score Transcription Error Rates}

Nakamura et al. (2018) proposed a set of metrics to evaluate their extension of a Hidden Markov Model \cite{Nakamura2018} for automatic transcription. These metrics build upon earlier work \cite{Nakamura2017a} and include: \begin{itemize}
	\item \textbf{Pitch error rate} $E_{\textrm{p}}$. Measures the proportion of notes detected with incorrect pitch.
	\item \textbf{Missing note rate} $E_{\textrm{m}}$. Measures the proportion of notes that are present in the ground-truth score but missing from the transcription.
	\item \textbf{Extra note rate} $E_{\textrm{e}}$. Measures the proportion of notes that appear in the transcription but are not present in the original score.
	\item \textbf{Onset-time error rate} $E_{\textrm{on}}$. Quantifies errors in note onset times relative to the expected values.
	\item \textbf{Offset-time error rate} $E_{\textrm{off}}$. Measures errors in note offset times after normalizing to the correct onset times.
	\item \textbf{Overall Error Rate} $E_{\textrm{all}}$. The arithmetic mean of all other error rates.
\end{itemize} The metrics rely on two components: \emph{note alignment} and \emph{rhythm correction cost}.

The overall set of all metrics is called \emph{MUSTER}: \emph{MUsic Score Transcription Error Rates} \cite{Hiramatsu2021}.

\subsection{Note Alignment}

The transcription metrics used to evaluate the performance of a model depend on determining how many notes are ``correct'' or ``incorrect'' based on several criteria: whether the notes are missing, superfluous, or played with incorrect pitch. Classifying notes in these categories is a non-trivial task. In particular, due to timing variations, missing or extra notes\footnote{Note that intended ornament notes are classified as superfluous too, henceforth incorrect.} a direct note-by-note comparison is not a viable approach.

The note alignment process is designed by Nakamura et al. in \cite{Nakamura2017b} to robustly align the performed MIDI notes with the corresponding score notes. The algorithm's objective is to identify and categorize notes into three main groups: \begin{itemize}
	\item {\bf Matched notes}. These include both notes that are correctly matched in terms of pitch and onset time, and notes that have correct onset timing but pitch errors (wrong pitch).
	\item {\bf Extra notes}. These are notes that appear in the performance but do not exist in the original score (false positives). They may occur due to performance errors or expressive choices like embellishments.
	\item {\bf Missing notes}. Notes that are present in the score but were omitted in the performance or failed to be detected by the transcription system (false negatives) are considered missing.
\end{itemize}

For more information the exact problem statement, the procedure and algorithm evaluation, refer to \cite{Nakamura2017b}.

\subsection{Rhythm Correction Cost}

In music, note values are not absolute and depend on the tempo choice. A quarter note played at a tempo $60$ beats per minute lasts the same duration as a half note played at a tempo 120 BPM. The choice of the unit is to somewhat arbitrary. Yet, doubling or halving the tempo may disrupt transcription metrics, as some naive metrics might inaccurately treat an entire passage as incorrect when the tempo is altered. This remark points out that the metric values should be analyzed in relationship to other notes, not as absolute values \cite[p. 7]{Nakamura2017c}.

To address this issue, Nakamura et al. introduced \emph{Rhythm Correction Cost} (RCC), which is defined as: \emph{the minimum number of scale and shift operations for onset score times} \cite{Nakamura2018}. This metric takes into account tempo variations and adjusts for structural differences in note onsets between the transcription and the ground truth via two operations: scales and shifts.

\begin{figure}[ht!]
\centering
\input{sources/graphs/rhythm_correction}
\caption[Scaling and shift operations that recover the rhythm structure.]{Scaling and shift operations that recover the rhythm structure \cite{Nakamura2017b}.}
\label{rhythm_correction}
\end{figure}

In the Figure \ref{rhythm_correction}, two scaling and two shift operations are needed to recover the ground-truth rhythm structure, and that number is minimal. Hence, the RCC is $4$.

\subsection{Metrics}

Let us review the definitions of metrics from the paper \cite{Nakamura2018}.

The \emph{pitch error rate} metric quantifies how many notes are detected with incorrect pitch. The formula is rather straightforward: \[E_{\textrm{p}}=\frac{\textrm{number of notes with incorrect pitch}}{\textrm{total number of ground-truth notes}}\] The \emph{missing note rate} and \emph{extra note rate} values are derived similarly: \begin{flalign*}E_{\textrm{m}}&=\frac{\textrm{number of missing notes}}{\textrm{total number of ground-truth notes}} \\ E_{\textrm{e}}&=\frac{\textrm{number of extra notes}}{\textrm{total number of ground-truth notes}}\end{flalign*} 

Onset/offset-time error rates rely on RCC. The \emph{onset-time error rate} is defined as: \[E_{\textrm{on}} = \frac{\textrm{RCC}}{\textrm{number of matched notes}}\] while \emph{offset-time error rate} is derived from the number of notes with an incorrect offset score time after normalization using the closest onset score time: \[E_{\textrm{off}} = \frac{\textrm{number of notes with an incorrect offset score time after normalization}}{\textrm{number of matched notes}}\] The exact procedure of how to calculate the numerator is present in \cite{Nakamura2017a}.

Finally, $E_{\textrm{all}}$ is calculated as the arithmetic mean of all metrics above. It should reflect the overall error rate of a score.

\subsection{Limitations}

The metrics proposed by Nakamura et al. primarily evaluate specific aspects of musical transcription accuracy, focusing on note-related parameters such as pitch, onset, and offset times. While effective for assessing the correctness of a note stream, these metrics do not encompass broader transcription dimensions, including voice leading, harmonic context, or overall musical structure. Consequently, they are limited in their ability to fully represent a broader musical context of a transcription.

The overall metric allows penalty accumulation, meaning that a single transcription error may impact several ratings. For example, a misplaced note may be marked as ``missing'' from its expected position while also counted as ``extra'' in its actual position, leading to compounded penalties.

Additionally, empirical studies, such as Holzapfel et al. (2021) \cite{Holzapfel2021}, suggest that several of these metrics may not align closely with human expert evaluations. 

\section{MV2H Metric} \label{MV2H_section}

The MV2H (\emph{\textbf{M}ulti-pitch detection}, \emph{\textbf{V}oice separation}, \emph{\textbf{M}etrical alignment}, note \emph{\textbf{V}alue detection} and \emph{\textbf{H}armonic analysis}) metric, introduced by McLeod and Steedman \cite{McLeod2018}, is a comprehensive evaluation framework designed to assess AMT systems. The MV2H metric integrates several dimensions of transcription quality into a single evaluative measure. This section elaborates on the components of MV2H, the method of calculating the metric, and the advantages it offers over traditional evaluation metrics.

The goal of the MV2H metric is to evaluate AMT systems comprehensively across five key aspects: \emph{multi-pitch detection}, \emph{voice separation}, \emph{metrical alignment}, \emph{note value detection} and \emph{harmonic analysis}. These components represent the essential features of music transcription, including pitch accuracy, rhythm, note durations, and harmony. 

One of key assumptions of the MV2H metric is its \emph{disjoint penalty principle}, which prevents cascading penalties across multiple transcription aspects due to a single mistake.

The metric is the main evaluation tool used for the main model quality assessment.

\subsection{Components of the MV2H Metric}

The MV2H metric is composed of five submetrics, each targeting a distinct transcription task. These metrics are calculated independently and combined to form the overall MV2H score, which, as previously, the arithmetic mean of all submetrics.

Below, we describe each component and how it is calculated.

\subsubsection{Multi-pitch Detection}

The multi-pitch detection submetric evaluates how well the AMT system detects the pitch and onset time of notes. A transcribed note is considered correctly detected if it satisfies two criteria: \begin{enumerate}
	\item The onset time must be within a small threshold ($50$ ms by default) of the ground truth onset time.
	\item The pitch of the transcribed note must match the corresponding pitch in the ground truth.\end{enumerate} Other detected notes are false positives, and unmatched ground-truth notes are false negatives. Note offsets are ignored. Then, the multi-pitch detection metric is defined as the $F$-measure.

Notes that are not correctly detected in the multi-pitch detection stage are excluded from subsequent evaluations, such as voice separation or metrical alignment. This ensures that the multi-pitch detection errors are taken into account only once.

A serious drawback of this approach is inability to compare a performance MIDI with a sheet that contains idealized note onsets. In the subsequent version of the metric \cite{McLeod2019}, the authors enhanced the metric to handle non-aligned musical scores, using a variant of \emph{Dynamic Time Warping} (DTW), a dynamic programming algorithm introduced in 1978 by Sakoe et al. \cite{Sakoe1978} to compare two time-dependent sequences that may vary in speed or length. 

\subsubsection{Voice Separation}

Voice separation evaluates how well an AMT system groups notes into coherent musical voices. This aspect is crucial for polyphonic music, where multiple voices may play simultaneously, such as in fugues or piano pieces.

Voice separation is evaluated by comparing the system’s transcribed voices to the ground-truth voices in the score. A note is considered correctly placed if it is assigned to the correct voice in addition to its onset time and pitch being correct. The same $F$-measure used for multi-pitch detection is employed here, but only for notes that have already been correctly matched in terms of pitch and onset. This prevents over-penalization due to errors in the multi-pitch detection stage, aligning with the disjoint penalty principle.

\subsubsection{Metrical Alignment}

Metrical alignment measures how well the transcribed notes align rhythmically with the ground truth. Here, beat groupings (sub-beat, beat, and bar level) are evaluated to determine whether the system correctly identifies the temporal structure of the piece.

To calculate the metrical alignment score, the system compares the onset and offset times of transcribed notes against those in the ground truth. A note is considered correctly aligned if its onset and offset times fall within a small threshold ($50$ ms by default) of the expected beat location. The metrical alignment is thus based on comparing the actual onset positions between a transcription and the corresponding ground-truth notation.

\subsubsection{Note Value Detection}

Note value detection evaluates the accuracy with which an AMT system transcribes the duration of each note. Note durations, referred to as note values in musical terminology, are normalized to account for tempo variations. This ensures that a note played in a faster tempo is still transcribed correctly relative to the piece's overall structure. For example, a quarter note at $60$ BPM should be equivalent in duration to a half note at $120$ BPM\footnote{Unlike the previous set of metrics and the RCC component, the tempo misalignment is not considered here as an error.}.

Only a subset of correctly identified notes is subject to the metric. A note is included in the analysis, if corresponds with a true positive ground-truth: \begin{enumerate}
	\item multi-pitch detection
	\item voice segment
\end{enumerate}

For each correctly matched note (in terms of pitch and onset), its duration is compared to the ground truth. The error in note duration is computed, with a score of $1$ indicating a perfect match and a score of $0$ for complete mismatch. For a single transcription note and its length $d_{\textrm{det}}$ and the corresponding ground-truth length $d_{\textrm{gt}}$, the individual note score is: \[\textrm{score} = \max\left(0, 1 - \frac{\left|d_{\textrm{gt}} - d_{\textrm{det}}\right|}{d_{\textrm{gt}}}\right)\] The overall note value detection score for a transcription is computed as the arithmetic mean of individual note value scores, evaluated for all correctly matched notes. 

\subsubsection{Harmonic Analysis}

Harmonic analysis assesses the system’s ability to correctly transcribe the harmonic structure of the music, which includes key and chord detection. 

The metric is based on standard key detection evaluation \cite{Raffel2014}, which measures how closely the transcribed key signature matches the ground truth, with partial credit given for related keys (e.g., relative or parallel major/minor of the correct key or key a fifth too high).

The standard chord tracking evaluation is \emph{chord symbol recall} (CSR), described in detail in \cite{Harte2010}. It can be defined as \emph{proportion of the input for which the annotated chord matches the ground truth chord} \cite{McLeod2018}.

The Harmonic Analysis metric is the arithmetic mean of key detection metric and CSR.

\subsection{Disjoint Penalty Principle}

The disjoint penalty principle is a core feature of MV2H, designed to prevent a single mistake from being penalized across multiple components of the metric. For example, if a note is missed during multi-pitch detection, it is excluded from the voice separation, metrical alignment, and note value detection evaluations. This prevents errors from compounding and ensures that the system is evaluated fairly across all aspects.

The principle is applied by first evaluating multi-pitch detection and removing any unmatched notes from further evaluations. The remaining correctly transcribed notes are then used for evaluating voice separation, metrical alignment, note value detection, and harmonic analysis. Subsequently, note value detection is performed only on correctly metrically aligned notes.

\subsection{Limitations}

The metric assumes linear, measure-by-measure alignment with limited ability to handle structural variations such as repeats, cuts, or large-scale rearrangements of musical sections. 

It is also highly dependent of the multi-pitch detection as the subsequent components rely on the result of the alignment. The design of the metric prevents a cascade of a single error, however since the later submetrics operate on filtered data, the result may be biased.

Additionally, the harmony submetric implicitly assumes tonal framework. This is a non-issue in the case of classical music datasets, however this limitation should be acknowledged.

Finally, to the author’s knowledge, the MV2H metric has not yet been rigorously evaluated through human expert assessment, and the relevance and weight of each submetric remain to be validated against expert judgment.

\section{Score Similarity}

Most methods of score comparison operate at the level of MIDI files. As previously discussed, several limitations in the MIDI format complicate its use as a reliable representation of scores. These limitations include the absence of key and time signatures, pitch spelling, clefs, dynamics, and articulation markings. Such issues are sometimes addressed by adding supplementary information as annotations, either embedded as meta messages within the MIDI file or in external files.

In 2017, Cogliati et al. \cite{Cogliati2017} proposed a metric that directly compares scores in the MusicXML format, allowing for the evaluation of musical elements not typically encoded in MIDI. The \emph{Score Similarity} metric is calculated in two stages: note alignment and musical element comparison.

\subsection{Note Alignment}

To prepare for comparison, the transcription is aligned to a ground-truth reference based solely on pitch content, disregarding all other musical elements at this stage.

The alignment process employs \emph{Dynamic Time Warping}, similarly to the revised MV2H metric for non-aligned transcriptions. In this application, DTW minimizes mismatched pitches, irrespective of duration, pitch spelling, or staff position \cite[p. 409]{Cogliati2016}. This ensures that transcription musical elements are being properly compared to the ground-truth counterparts, even if the transcription meter alignment is incorrect.

\subsection{Comparison}

Once the transcription is aligned with the reference, musical objects inside each portion of the two scores are grouped into sets. These sets are then compared according to several categories: \begin{itemize}
	\item barlines
	\item clefs
	\item key signatures
	\item time signatures
	\item rests
	\item notes (with grouping)
\end{itemize}

For barlines, clefs, key signature and time signatures binary matching is used. Rests are evaluated not only for duration, but also staff assignment. Missing or extra rests are considered as errors. Notes are matched for the following qualities: spelling, duration, stem direction, staff assignment, and grouping into chords. Incorrect note spelling (of a correct pitch) counts as an error.

Each category is analyzed for discrepancies, and errors are recorded based on the alignment. Some errors, such as clefs and key signatures, are counted globally, while others, like notes and durations, are normalized by the total number of instances.

Not all musical errors are of equal importance. For that reason, each category is weighted according to its perceptual importance, based on a linear regression model trained to align with human ratings. Additional details on the weighting procedure are available in \cite[p. 410--411]{Cogliati2017}.

\subsection{Limitations}

The metric is designed specifically for MusicXML\footnote{The metric code uses \emph{music21} \cite{Cuthbert2010} as the backend, so it is possible to measure scores faithfully represented within this framework.}. This renders it incompatible with MIDI files, as MIDI lacks many of these musical components required for a complete comparison.

As in the case of the MV2H metric, the score similarity measure use of DTW, while effective for basic alignment, assumes linear relationships between elements in both sequences. This makes it less effective in cases of large-scale reordering or insertion/deletion of structural elements, such as repeats or cuts in the transcription.

Finally, the human-rating alignment is fixed for all evaluations, which may be inadequate for certain musical styles or genres. The weighting parameters were calibrated using musical examples from a single source \cite{Kostka1994}, making the metric particularly suited to classical piano compositions but potentially less applicable to other genres, such as jazz.