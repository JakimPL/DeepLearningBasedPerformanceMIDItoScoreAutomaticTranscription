\chapter{Performance MIDI-to-Score Conversion by Neural Beat Tracking}

The main model of interest \cite{Liu2022}, developed by Liu et al. (2022), is a state-of-the-art machine learning model for complete music transcription. It has been awarded as the best paper of the \emph{International Society for Music Information Retrieval Conference} (ISMIR) in December 2022.

The model is composed of two parts:
\begin{itemize}
	\item Beat Tracking.
	\item Score Element Assignment.
\end{itemize}

The first part is responsible for quantizing the raw material into beats and downbeats. Each measure is consisted of several beats, and the number of beats in a measure is governed by the time signature. The beats mark the tempo, and they are the basis for the musical onsets detection.

The second part assigns every other aspect of the musical score to the MIDI stream. Let us recall these elements: musical onsets, note values, hand parts, key signature and time signature.

Let us review both components.

\section{Beat Tracking}

A single measure consists of several beats, and the rhythmic structure of beats is determined by time signature (or signatures). A performance MIDI does not contain information about beats and one of the objectives of the model is to predict locations of these.

After these predictions, note beginnings (onsets) can be placed in a subdivision of a single beat. The smallest unit of time distance is thus dictated by the size of the beat subdivision. More precisely, a musical onset $mo_i$ of the $i$-th note is defined as \[mo_i = \frac{s_n}{S}\] where $S$ is the number of subdivisions per beat in rhythm quantization, and $s_n$ is the musical onset time expressed in the number of subdivisions.

The authors provide a novel approach to detecting beats, splitting them into two categories, for which there are two separate methods.

Beats can fall into two distinct groups:
\begin{itemize}
	\item \emph{In-note}: beats concurrent with at least one note onset. This means that some notes play within the start of the beat.
	\item \emph{Out-of-note}: a beat is not concurrent with any note onset. No notes are beginning with such beats.
\end{itemize}

\subsection{In-note Beat Prediction}

For predicting in-note beats, one can reduce the problem to a binary classification task on the note sequence $\mathbf{X}$. This sequence is assumed to have a length $N$.

\begin{figure}[!ht]
\centering
\input{sources/graphs/in_note}
\caption[In-note beat prediction model.]{In-note beat prediction model with a CRNN with 3 convolutional layers and 2 bi-directional gated recurrent unit (GRU) layers.}
\end{figure}

The probability $P_n$ that $n$-th note is concurrent with a beat is defined as: \[P_n = \mathbb{P}\left(B_n|\mathbf{X}\right)\] where $B_n\in\{0,1\}$ is the ground-truth beat label for the $n$-th note from the note sequence. The model is trained using the standard cross-entropy loss function: \[\mathcal{L}=-\frac{1}{N}\sum_{n=1}^N B_n\ln P_n + \left(1-B_n\right)\ln\left(1-P_n\right)\] The authors used CRNN with 3 convolutional layers and 2 bidirectional gated recurrent unit (GRU) layers. The probability threshold for positive classification has been set dynamically, depending on the maximum probability in a fixed segment length.

\subsection{Out-of-note Beat Prediction}

The approach to predicting out-of-note beats, which do not align with note onsets, requires distinct approach. Liu et al. (2022) proposed a dynamic programming strategy to solve this problem \cite{Liu2022}.

Let us assume that there are $B^i$ in-note beats $\{b_n^i\}_{n=1}^{B^i}$ in total, and out-of-note beats are at subdivisions of the neighboring in-note beats\footnote{We may select only one note per in-note beat, if there are more.} $b_{n}^i$ and $b_{n+1}^i$.

The goal of the procedure is to find out-of-note beats $b^o$ from a set of candidates: \begin{equation}\label{out_of_note_candidates}
b_{n,K}^o = \left\{b_n^i + \frac{k}{K+1}\left(b_{n+1}^i-b_n^i\right)\right\}_{k=1}^K
\end{equation} where $K\in\{0,1,2,3\}$ is the number of out-of-note beats to insert inside the $\left(b_{n+1}^i, b_n^i\right)$ interval.

The number of candidates is selected in order to minimize the tempo change after adding out-of-note beats. The function may be represented as the sum: \[\mathcal{O}_1 = \sum_{n=1}^{B-2}\left|\ln\left(\frac{b_{n+2} - b_{n+1}}{b_{n+1} - b_n}\right)\right|\] where $\{b_n\}_{n=1}^B$ is the sequence of all $B$ beats (both in-note and out-of-note, sorted chronologically).

To discourage the procedure from adding too many out-of-note beats, which leads to an unnecessarily subdivided output, an additional penalty is associated with the objective function: \begin{equation}\label{out_of_note_objective}
\mathcal{O} = \mathcal{O}_1 + \lambda B^o
\end{equation} where $B^o$ is the number of added out-of-note beats, and $\lambda$ is the penalty coefficient. The coefficient is to be found experimentally, however the default value set by the authors is $1$.

\input{sources/algorithms/out_of_note}

\section{Input Data Encoding}

The entire score needs to be transformed to a suitable data format before passing to any of the model components. Given a note sequence tensor as described earlier, the data is encoded into a variety of features: \begin{itemize}
	\item $128$-dimensional one-hot encoding of MIDI pitches.
	\item One-hot onset time-shift ($o_i - o_{i-1}$) quantised by $10$ ms resolution, with maximum value of $4$ s, $401$ features in total.
	\item Raw duration values in seconds.
	\item Velocities normalized to the unit interval $[0, 1]$.
\end{itemize}

There are $531$ features in total.

The authors tried different encoding schemes, for pitches, onset times and durations, including raw float values in seconds. The full study is available in the paper \cite{Liu2022}.

\section{Score Elements Assignment}

As discussed in the Section \ref{music_score_encoding}, score elements assignment may be viewed as a function from a note sequence $\mathbf{X}$ into a score encoded by a tensor $\mathbf{Y}_n$ representing musical onsets, note values, key signature, time signature, and hand parts, for each note separately.

Besides the quantization model, which relies on the beat tracking module, key signature, time signature and hand parts modules can be treated independently.

\begin{figure}[!ht]
\centering
\input{sources/graphs/model_architecture}
\caption[The initial architecture of the model.]{The initial architecture of the model proposed in \cite{Liu2022}. There are five separate modules of the entire model in total: \emph{beat}, \emph{quantization}, \emph{time signature}, \emph{key signature} and \emph{hand parts}.}
\end{figure}

The initial version of time signature model handled a finite set of time signature numerators and denominators, as described in Section \ref{music_score_encoding}. One year after the paper release, the model has been reduced to recognize only two the most common time signatures: ${4 \atop 4}$ and ${3 \atop 4}$, with removed GRUBlock. With an additional support of beat quantizer, additional two time signatures ${2 \atop 4}$ and ${6 \atop 8}$ could be recognized.  For performance analysis, we use the latter approach.

Instead of direct tempo estimation, for each note an \emph{inter-beat-interval} time is estimated using classification, capturing time intervals between successive beats with 10 ms resolution, up to $4$ seconds (401 features).

\section{Score Generation}

The final score generation process synthesizes the source MIDI stream, incorporating pitch sequence, with the model's output. The resultant annotated MIDI encompasses essential information for visual score generation, including key/time signatures, which are not typically present in standard MIDI files.

\section{Training and Evaluation}

The training and evaluation procedures were implemented to closely replicate those described by the original authors. In the following sections, we provide a detailed overview of the datasets, augmentation techniques, and specific configurations for training and evaluation setups.

\subsection{Datasets}\label{datasets}

The dataset used by the authors of the paper \cite{Liu2022} integrates three sources of MIDI files: \begin{itemize}
	\item The \emph{Classical Piano MIDI} (CPM) database \cite{Krueger1996}.
	\item The \emph{Augmented MIDI Aligned Piano Sounds} (A-MAPS) \cite{Ycart2018}.
	\item The \emph{Aligned Scores and Performances} (ASAP) dataset \cite{Foscarin2020}.
\end{itemize}

The datasets consists of a variety of classical piano pieces by composers including as Bach, Mozart, Beethoven, Schubert, Chopin, Liszt, and others from the Western European classica repertoire.

Certain musical features, such as time and key signatures, are not always encoded in MIDI files. Consequently, the MIDI files have been annotated using different strategies.

\begin{table}[ht!]
\centering
\input{sources/tables/datasets}
\caption[Statistics of the dataset used for training.]{Statistics of the dataset used for training \cite{Liu2022}. Performances of the same piece are counted only once.}
\label{train_valid_test}
\end{table}

The datasets partially overlap; for instance, A-MAPS is derived in part from CPM. Authors avoid using the same pieces in train, validation and test sets by using distinct music piece labels coming from different datasets.

All dataset are jointly combined into one dataset, \emph{A Dataset of Aligned Classical Piano Audio and Scores} (ACPAS) \cite{Liu2021}.

Below is a brief overview of each data source.

\subsubsection{Classical Piano MIDI Database}

The \emph{Classical Piano MIDI} (CPM) database was created by Bernd Krüger, who produced hundreds of MIDI files containing interpretations of classical piano works. Krüger describes his motivation as follows \cite{Krueger1996}:

\begin{quote}The page serves to describe and make available my interpretations of classical piano works. Although I am a layman in terms of music, I have set myself the goal of painstakingly interpreting difficult works. I would like to make these works accessible to as many musically interested people as possible.\end{quote}

The dataset consists 337 pieces with a cumulative duration of approximately $23$ hours. All MIDI files are score-informed, with separate tracks for the left and right hands.  and time signatures are encoded in the MIDI files as meta messages.

However, despite the fact that the MIDI files are tempo-varied, they were manually crafted, not performed. For instance, note onsets which lie on the same beat, occur simultaneously. In musical performance, even chords are played with certain time variation.

\subsubsection{Augmented MIDI Aligned Piano Sounds}

The \emph{Augmented MIDI Aligned Piano Sounds} (A-MAPS) dataset builds on the original \emph{MIDI Aligned Piano Sounds} (MAPS) dataset, introduced by Emiya et al. \cite{Emiya2010}. MAPS contains approximately 65 hours of data, including both MIDI and corresponding audio recordings, and is divided into four main categories:\begin{itemize}
	\item \textbf{ISOL}: Isolated notes and monophonic excerpts.
	\item \textbf{RAND}: Chords with random pitch notes.
	\item \textbf{UCHO}: Usual chords from Western music.
	\item \textbf{MUS}: Piano music pieces, sourced from the \emph{Classical Piano MIDI} database. \end{itemize}

MAPS has been widely used as a benchmark for AMT systems \cite{Ycart2018}. However, the information provided by the MIDI files is limited to basic attributes: pitch, onsets and offsets in seconds, and velocity. The A-MAPS dataset enriches the original with additional annotations, including meter, note values, key signatures and hand separation. Moreover, A-MAPS provides also tempo curves and sustain pedal activations, although this information is not used by the considered model.

The A-MAPS dataset contains 269 files of 159 unique pieces.

\subsubsection{Aligned Scores and Performances}

The \emph{Aligned Scores and Performances} (ASAP) dataset consists of 222 musical score aligned with 1068 performances, of the total duration of 92 hours \cite{Foscarin2020}.

The ground truth is provided twofold: as musical scores in MusicXML format, and quantized MIDI files.

For each performance, including the quantized MIDI file, annotations are provided in tab-separated values (TSV) format. These annotation specify timestamps for: \begin{itemize}
	\item Beats (b) and downbeats (db).
	\item Time signature changes.
	\item Key signature changes.
\end{itemize}

\begin{table}[ht!]
\centering
\input{sources/tables/annotations}
\caption[Example of performance MIDI annotation in TSV format.]{An example of a performance MIDI annotation in TSV format. The first row indicates an initial time signature of ${6 \atop 8}$. The piece begins in the key of D major, encoded as 2.}
\end{table}

The MIDI are actual human performances recorded via digital instruments. The annotations for performance provide ground-truth labels for all considered models. Otherwise, one would need to compare all different interpretations to a single score, risking judging the performance's fidelity to the original instead of transcription quality.

\subsection{Data Augmentation} \label{data_augmentation}

To increase model's versatility, the authors considered four data augmentation techniques, that should not affect the transcription. The note sequence tensors have been transformed using the following methods: \begin{itemize}
	\item \textbf{Pitch Shift}. Transpose all notes by a constant pitch from the range of $\{-12, -11, \ldots, 12\}$.
	\item \textbf{Tempo Change}: Change the tempo by multiplication by a constant from the range $[0.8, 1.2]$.
	\item \textbf{Note Removal}. For each group of $m$ concurrent notes, a random number from $0$ to $m - 1$ of notes is being removed.
	\item \textbf{Note Introduction}. For some notes a concurrent ones are being added, with the same velocity and duration as 
\end{itemize}

Section \ref{beat_tracking} shows ablation studies for data augmentation, for the beat-tracking model.

\subsection{Training and Evaluation}

To reproduce the original results, we followed the authors' training and evaluation setup, adhering to the same training, validation, and test dataset divisions. In the following sections, we provide a detailed examination of the training and evaluation pipeline, including feature preparation and the training and evaluation scheme.

\subsubsection{Feature Preparation}

As the data comes from different sources, they need to be unified before training.

Initially, all MIDI files were gathered from the datasets outlined in Section \ref{datasets}. Following the authors' methodology, all pieces in the \texttt{ENSTDkCl} subset of the MAPS dataset were designated as the test set. The remaining pieces were randomly allocated to the validation set, with $90\%$ reserved for training\footnote{Specifically, pieces with an ID divisible by 10 were assigned to the validation set.}. Refer to Table \ref{train_valid_test} for dataset division details.

The next step involved extracting note sequence tensors and additional features from MIDI files and, where relevant, from external annotations, as in the case of the ASAP dataset. The features encompassed all essential elements for transcription, including beats, downbeats, time signatures, key signatures, onsets, note durations, and hand part assignments.

Once processed, the datasets were ready for model training.

\subsubsection{Training}

Each model was trained with a maximum of 50 epochs. Training examples were augmented according to the procedures discussed in Section \ref{data_augmentation}.

\subsubsection{Evaluation}

During training, validation metrics were calculated after each epoch. The metrics used varied by model: for binary classifiers (such as the time signature and hand part models), standard metrics—accuracy, precision, recall, and $F_1$ score—were employed. For the key signature model, which is a multiclass classifier, both micro and macro metrics were used (see Section \ref{binary_and_multiclass_classifier_metrics}).

Upon completion of training, the model with the highest $F_1$ score on the validation set was selected for further evaluation on the test set. A joint performance score across all models was then calculated using the MV2H metric on the test set.

\section{Model Performance}

We reproduced the results using the training and evaluation setup provided by the authors, described in the previous sections. We achieved a comparable MV2H metric scores on the test set as the paper's authors.

\subsection{Validation Metrics}

Below, we present the validation metrics, including $F_1$ scores, for each submodel.

\begin{table}[ht!]
\centering
\input{sources/tables/f1_hand_part}
\caption[Validation metrics of the hand part model.]{Validation metrics of the hand part model.}
\end{table}

\begin{table}[ht!]
\centering
\input{sources/tables/f1_key_signature}
\caption[Validation metrics of the time signature model.]{Validation metrics of the time signature model.}
\end{table}

\begin{table}[ht!]
\centering
\input{sources/tables/f1_time_signature}
\caption[Validation metrics of the key signature model.]{Validation metrics of the key signature model.}
\end{table}

\begin{table}[ht!]
\centering
\input{sources/tables/f1_beat}
\caption[Validation metrics of the beat quantization model.]{Validation metrics of the beat quantization model.}
\end{table}

\subsection{MV2H Metric}

The MV2H metric results on the test set are presented below, alongside the original authors' evaluation results. It should be noted that minor changes have been introduced since the release of the model, so the comparison is not exact.

We use the following notation for the MV2H components: \begin{itemize}
	\item Multi-pitch detection $F_{\textrm{p}}$
	\item Voice separation $F_{\textrm{vs}}$
	\item Metrical alignment $F_{\textrm{ma}}$
	\item Note value detection $F_{\textrm{nv}}$
	\item Harmonic analysis $F_{\textrm{ha}}$
\end{itemize}

$F$ stands for the final MV2H metric, the average of all submetrics.

\begin{table}[ht!]
\centering
\input{sources/tables/mv2h}
\caption[MV2H metric evaluation on the test set.]{MV2H metric evaluation on the test set, with results compared to the original model evaluation.}
\end{table}

\section{Robustness Analysis} \label{robustness_analysis}

The quality of the model can be considered in many dimensions. Besides the fidelity of the outputs to the ground truth, one can consider how a model is robust to certain transformations that should not affect the result.

For such analysis, we assumed the following: \begin{itemize}
	\item Note velocity should not affect time signature or hand part assignment.
	\item Note pitch does not matter for the time signature.
	\item Note pitch is crucial for key signature/hand part assigning, while other features should not contribute.
	\item Note duration should not impact key signature.
\end{itemize}

These are not meant to be rigid principles, rather rules of thumb. It may be the case that in special situations some deviations from these rules may occur: for instance a chord played be one hand tend to be of uniform intensity. We can use another hand to play a certain note with a completely different articulation. This is also the reason why we cannot ditch all but main features completely in general. See Section \ref{ablation_studies} for a more detailed analysis.

A robust model should obey to these rules. It is natural to expect the model to give the same time signatures for a transposition of a piece, that is shifting all pitches by a constant value.

We have conducted an analysis of how the model is robust to transformation, for each of three components: \begin{itemize}
	\item The \textbf{hand part} $f_H$.
	\item The \textbf{key signature} model $f_K$.
	\item The \textbf{time signature} model $f_T$.
\end{itemize}

In the subsequent sections, we use models pretrained by the authors unless stated otherwise.

\subsection{Ceteris Paribus}

We conducted a \emph{ceteris paribus} (Latin for ,,all other things being equal'') type of analysis by augmenting original note sequence tensors. This approach allowed us to compare model outputs for transformed note sequences, isolating and measuring the influence of specific features.

A natural way of augmenting data in this context is to randomly distort a single feature (e.g., pitch, duration, or velocity), while keeping all others intact.

\subsubsection{Perturbations} \label{perturbations}

For a sample $M = 50$ musical pieces from the dataset, we introduced perturbations to test the assumptions given in the Section \ref{robustness_analysis}: \begin{itemize}
	\item Changing velocity with standard deviations $\sigma_v$ of 8, 16, 32, and 64, but with clipping to the range $[1, 127]$.
	\item Scaling note lengths by a factor from an interval $\alpha\in(\alpha_l,1)$, where $\alpha_l\in\{0.9, 0.75, 0.5, 0.2\}$ (note extension may lead to overlapping).
	\item Changing pitch with standard deviations $\sigma_p$ of 12 and 24 (whole octave, double octave). However, we ensure, that the entire piece stays in the playable range of pitches $[21, 108]$, corresponding to the range of a typical $88$-key piano.
\end{itemize}

Each transformation is applied separately to each feature, note by note. Notice that we don't change onsets at all, as they change the musical structure of a piece.

For each feature model $f$: \emph{hand part} $f_H$, \emph{key signature} $f_K$, \emph{time signature} $f_T$, we calculated the average error between the output sequence and the output of a perturbed sequence.

For each element ${\bf x}_i$ in the dataset sample $i\in\{1,2,\ldots,50\}$ and for each change, we sampled $m=10$ perturbations ${\bf x}^{(j)}$ of the original item and measured the error: \[\operatorname{error}\left(f\right) = \frac{1}{M}\sum_{i=1}^{M}\frac{1}{m}\sum_{j=1}^{m}\operatorname{error}\left(f\left({\bf x}_i\right), f\left({\bf x}_i^{(j)}\right)\right)\] Here, $\operatorname{error}$ between two sequences ${\bf x} = (x_i)_{i=1}^N$ and ${\bf y} = (y_i)_{i=1}^N$ is defined as the mean number of indices for which these sequences differ: \[\operatorname{error}({\bf x},{\bf y}) = \frac{1}{N}\sum_{i=1}^N \left[x_i \neq y_i\right]\] Higher error values indicate greater influence on the output $f$ from a transformation. Notice that we don't measure if a particular change improved or worsened a model.

This analysis directly measures the model's robustness to specific transformations and does not rely on the model overall performance.

\subsubsection{Results} \label{results}

Among all three models, the key signature $f_K$ model proved to be robust to all proposed transformations, with an average error of less than $1.5\%$ in each category.

The time signature model is quite robust to note velocity and pitch manipulations (average errors less than $13\%$), but loses consistency when notes are shortened. \begin{table}[ht!]
\input{sources/tables/perturbations_errors}
\caption[The average errors of certain perturbations (in percent).]{The average errors of certain perturbations (in percent).}
\label{perturbations}
\end{table}

The hand part model $f_H$ error is robust to note shortening (maximum average error of $3\%$), but inconsistent when note velocities are changed. This inconsistency is mitigated by maintaining consistent velocity for notes played simultaneously. We hypothesize that the difference stems from the fact that chords played by one hand typically have similar velocities (see Table \ref{hand_part_perturbations} for detailed results).

\begin{table}[ht!]
\input{sources/tables/hand_part_perturbations}
\caption[The average errors for the hand part model.]{The average errors for the hand part model $f_H$ for 1. standard perturbation, 2. uniform random change for notes played in the same time. The second transformation introduces less inconsistencies.}
\label{hand_part_perturbations}
\end{table} 

\begin{figure}[ht!]
\centering
\includesvg[width=0.9\textwidth]{images/ceteris_paribus_results_h.svg}
\includesvg[width=0.9\textwidth]{images/ceteris_paribus_results_k.svg}
\includesvg[width=0.9\textwidth]{images/ceteris_paribus_results_t.svg}
\caption[Results of the \emph{ceteris paribus} experiment.]{Results of the \emph{ceteris paribus} experiment for the hand part model $f_H$, the key signature model $f_K$ and the time signature model $f_T$. The hand part model is robust to note duration changes while it is susceptible to velocity manipulation. On the other hand, the time signature model is sensitive to note duration changes, which is expected to some extent, and relatively robust to other transformations. The key signature model is robust to all perturbations.}
\label{ceteris_paribus}
\end{figure}

Figure \ref{hand_part_misalignment} shows the result of hand part assignment for velocity perturbation.

\begin{figure}[!ht]
\centering
\includesvg[width=1.0\textwidth]{images/hand_part1.svg}
\includesvg[width=1.0\textwidth]{images/hand_part2.svg}
\caption[The piano roll for the hand part model output.]{The piano roll for the hand part model output. Each note is represented as a block, indicating a pitch (note), duration and velocity (by opacity). The first graph represents an original sequence while the second has perturbed velocity. Left-hand notes with a diagonal pattern. We can see that there are many misalignment in the hand assignment in the second output. As a rule of thumb, the left hand notes should be at the bottom while the right hand ones should be on the top.}\end{figure}

\section{Local Feature Importance}

To understand better the models' behavior, we tried to measure local influences of each feature onto models' decisions. In particularly, we posed several questions:

\begin{itemize}
	\item Is there any way to check which note features influenced models' decisions?
	\item Can we pinpoint notes that contribute the most to a model assignment, for example key signature attribution?
	\item Can we measure the strength and direction of a particular note feature? In particular, can we assign if a note feature voted for or against right hand assignment?
\end{itemize}

This type of questions, along with previous analysis, may help understanding the decision process of the model, facilitate recognizing weak spots of AMT systems and its limitations, and, finally, guide towards developing better AMT system.

We encountered challenges with common explainable machine learning tools: 

\begin{itemize}
	\item The input data is a very high-dimensional space, computationally infeasible for certain methods (e.g. Shapley values).
	\item Pitch space is a discrete space that lacks a meaningful measure of distance.
	\item Input data is a tensor of variable length, not conforming to tabular or image-like formats supported by many tools. Some model components (e.g. GRU blocks, ELU activation function) are not supported by certain XAI libraries. \missing
\end{itemize} 

We developed a custom solution to overcome these obstacles, following \emph{Local Interpetable Model-agnostic Explanations} (LIME) approach \cite{Ribeiro2016}.

We generated a locally modified version of the MIDI stream tensor for each note, randomizing the pitch/velocity for one note. We created $100$ samples for each note, calculating model predictions to compare with the original prediction. This approach aids in explaining the findings from the previous section (see Figure \ref{hand_part_misalignment} for an example).

The exact procedure consists of several steps. Let assume that we have a note sequence tensor. Then:\begin{enumerate}
	\item For each row, and for each feature (either pitch or velocity), prepare $n=50$ perturbations of a given feature, replacing exactly one value from the tensor ${\bf x}$ by a perturbed one.
	\item Measure the distance $d$ between the original value and the replacement. For velocity, let the weight be $w=\exp\left(-\tfrac{|d|}{20}\right)$, for pitch let the weight be $1$ if the difference is non-zero\footnote{Pitch space is discrete. The choice of weight function for velocity perturbation is arbitrary.}.
	\item Train a linear regression model on differences between the model prediction of the perturbed tensor and the original one, with weights $w$ dependent on the proximity $d$ to the original tensor.
	\item Store the model coefficients to a vector.
\end{enumerate}

This yields a tensor of the size $2 \times N \times N$ where $N$ is the size of the note sequence tensor, corresponding to two features: pitch and velocity. A single vector contains influences of a particular feature onto a decision on the level of a single note.

The proposed approach ignores temporal aspect of the sequence, as not taking onsets and durations into account. Moreover, the approach assumes feature independence, which is clearly not justified. However, it may give a basic insight into two influence of two basic MIDI elements: pitch and velocity.

The technique has been used to support conclusions from Section \ref{results}, illustrated by concrete examples. Figure \ref{hand_part_misalignment} shows how changing velocity in the right-hand notes affects hand part assignment of neighborhood notes. 

\begin{figure}[ht!]
\centering
\includesvg[width=0.95\textwidth]{images/lime_hand_part.svg}
\caption[Velocity influence on hand part assignment.]{A graph showing how the hand part assignment of the G2 note (with a thick outline) is influenced by the velocity of other notes. The negative direction of influence is indicated by the diagonal pattern. Increasing the volume of the low note leads to misalignment of G3 and B3 notes to the left hand. On the contrary, reducing the velocity of G2 note switches the note to a right-hand note. This is, of course, an undesired behavior of the model.}
\label{hand_part_misalignment}\end{figure}

As one-note replacement usually won't lead to key signature change, this practice is useful mostly for the hand-part assignment. For the key signature model we propose another technique.

\subsection{Key Signature Assignment by Note Omission}

For the key signature model, as the pitch space is not a metric space, we applied a different strategy. The method is as follows: \begin{enumerate}
	\item Get the key signature prediction values (before the last activation function) for the entire sequence.
	\item Individually remove each note in a sequence and calculate the difference between the original prediction and the prediction made on a sequence without a single note.
	\item Compute a contribution score, represented by the mean of the prediction differences for each note, in the neighborhood of a note. Only predictions on the current key signature are taken into account.
	\end{enumerate} This approach enables the measurement of each note's contribution to the attribution of a specific scale. Refer to Figure \ref{note_removing} for an illustrative example.
	
Using this technique we were able to estimate the influence of certain note feature to a model's decision.

\begin{figure}[ht!]
\centering
\includesvg[width=0.95\textwidth]{images/note_removing.svg}
\caption[A key signature alignment for G scale.]{A key signature alignment for G major scale. Note's key signature assignment intensity is represented using gray scale, where the diagonal pattern indicates that notes voted against the current key signature. Notes D$\sharp$ and G$\sharp$ are out of the scale, and have negative influence on the assignment most of the time. Sporadically, some other notes in the scale, especially B, voted slightly against the current key signature. The strongest negative vote changes the assignment probability by $6$ percentage points.}
\label{note_removing}\end{figure}

\subsection{Ablation Studies} \label{ablation_studies}

In order to enforce Section \ref{robustness_analysis} assumptions, we trained a set of models that take only a limited number of features. More precisely: \begin{itemize}
	\item The \textbf{hand part} model has been trained only on note pitches and onsets.
	\item The \textbf{key signature} model has been trained only on note pitches and onsets.
	\item The \textbf{time signature} model has been trained only on note onsets.
\end{itemize}

As a result, all models became invulnerable to transformations described in the Section \ref{perturbations}. We are going to check check how such change affects the quality of models. 

Each model has been evaluated in two ways, using: \begin{itemize}
	\item model-specific $F_1$ metrics
	\item the MV2H metric
\end{itemize}

The MV2H metric has been evaluated on the set of all original models with one replaced. Only the test subset of the dataset has been used. For a fair comparison, both base and restricted models have been trained on the training set only.

For each ablation study, we picked the model that had the best $F_1$ score among all models trained in the span of 50 epochs.

\subsubsection{Beat Tracking} \label{beat_tracking}

The authors themselves provided ablation studies for the beat-tracking model. They analyzed how feature omission or data augmentation influences the beat-tracking model. The results of the analysis are provided in: Table \ref{beat_tracking_feature_omission} and Table \ref{beat_tracking_data_augmentation}.

\begin{table}[ht!]
\centering
\input{sources/tables/feature_omission}
\caption[Feature omission study.]{Feature omission study \cite{Liu2022}.}
\label{beat_tracking_feature_omission}
\end{table}

Not surprisingly, note onsets are the most important feature used by the beat-tracking model. While neglecting other features doesn't hardly impact the model, still feature omission results in a slight quality degradation.

Velocity may carry some information about beats as they tend to occur along with heavier notes \cite{Liu2022}. Harmonic structure inferred by pitch may also transfer such information, for example it is quite common to play chords at the beginning of a measure. On the other hand note durations may hint the metrical structure of a piece.

\begin{table}[ht!]
\centering
\input{sources/tables/data_augmentation}
\caption[Ablation study for data augmentation.]{Ablation study for data augmentation \cite{Liu2022}.}
\label{beat_tracking_data_augmentation}
\end{table}

\subsubsection{Hand Part Model}

A variant of the hand part model that does not use neither durations nor velocities has been evaluated.

As the hand part affects only voice separation subscale of the MV2H metric, we omit remaining submetrics.

\begin{table}[ht!]
\centering
\input{sources/tables/hand_part_ablation}
\caption[Ablation study for the hand part model.]{Ablation study for the hand part model.}
\label{hand_part_ablation}
\end{table}

The results of both models are comparable in terms of $F_1$-score, although the voice separation MV2H submetric dropped. This suggests that the velocity may hint the hand assignment from time to time, but it is not (and it should not be) a direct indication of the hand assignment. Since durations and velocities make only two float features, including them into the model is of little computational cost.

\subsubsection{Key Signature Model}

Restricted to note pitches and onsets key signature model turned out to be of the same performance as the original one. 

\begin{table}[ht!]
\centering
\input{sources/tables/key_signature_ablation}
\caption[Ablation study for the key signature model.]{Ablation study for the key signature model.}
\label{key_signature_ablation}
\end{table}

Table \ref{key_signature_ablation} shows the results with included harmony MV2H subsccale. Weighted metric results are calculated independently, and weighted by the number of true labels of each class. Macro is unweighted.

The differences of the model lie in the expected range of model stochasticity. The result agrees also with the negligible influence of other factors to the final model decision on key signature, described in Section \ref{results}.

\subsubsection{Time Signature Model}

With no changes to the architecture, the restricted time signature model to only note onsets achieved better scores that the vanilla convolutional network classifier.  

\begin{table}[ht!]
\centering
\input{sources/tables/time_signature_ablation}
\caption[Ablation study for the time signature model.]{Ablation study for the time signature model.}
\label{time_signature_ablation}
\end{table}

As the MV2H metric does not rely on time signature estimation directly, the change has not impacted the evaluation.

This shows that not only introducing full information to the model is not beneficial, but actually hurts the performance of the model. 

The original network has not been tested in this regard.
