\chapter{Performance MIDI-to-Score Conversion by Neural Beat Tracking}

The main model of interest \cite{Liu2022} is composed of two parts:
\begin{itemize}
	\item Beat Tracking.
	\item Score Element Assignment.
\end{itemize}

The first part is responsible for quantizing the raw material into beats and downbeats. Each measure is consisted of several beats, and the number of beats in a measure is governed by the time signature. The beats mark the tempo, and they are the basis for the musical onsets detection.

The second part assigns every other aspect of the musical score to the MIDI stream. Let us recall these elements: musical onsets, note values, hand parts, key signature and time signature.

Let us review both components.

\section{Beat Tracking}

A single measure consists of several beats, and the rhythmic structure of beats is determined by time signature (or signatures). A performance MIDI does not contain information about beats and one of the objectives of the model is to predict locations of these.

After these predictions, note beginnings (onsets) can be placed in a subdivision of a single beat. The smallest unit of time distance is thus dictated by the size of the beat subdivision. More precisely, a musical onset $mo_i$ of the $i$-th note is defined as \[mo_i = \frac{s_n}{S}\] where $S$ is the number of subdivisions per beat in rhythm quantization, and $s_n$ is the musical onset time expressed in the number of subdivisions.

The authors provide a novel approach to detecting beats, splitting them into two categories, for which there are two separate methods.

Beats can fall into two distinct groups:
\begin{itemize}
	\item \emph{In-note}: beats concurrent with at least one note onset. This means that some notes play within the start of the beat.
	\item \emph{Out-of-note}: a beat is not concurrent with any note onset. No notes are beginning with such beats.
\end{itemize}

\subsection{In-note Beat Prediction}

For predicting in-note beats, one can reduce the problem to a binary classification task on the note sequence $\mathbf{X}$. This sequence is assumed to have a length $N$.

\begin{figure}[!ht]
\centering
\input{sources/graphs/in_note}
\caption[In-note beat prediction model]{In-note beat prediction model with a CRNN with 3 convolutional layers and 2 bi-directional gated recurrent unit (GRU) layers.}
\end{figure}

The probability $P_n$ that $n$-th note is concurrent with a beat is defined as: \[P_n = \mathbb{P}\left(B_n|\mathbf{X}\right)\] where $B_n\in\{0,1\}$ is the ground-truth beat label for the $n$-th note from the note sequence. The model is trained using the standard cross-entropy loss function: \[\mathcal{L}=-\frac{1}{N}\sum_{n=1}^N B_n\ln P_n + \left(1-B_n\right)\ln\left(1-P_n\right)\] The authors used CRNN with 3 convolutional layers and 2 bidirectional gated recurrent unit (GRU) layers. The probability threshold for positive classification has been set dynamically, depending on the maximum probability in a fixed segment length.

\subsection{Out-of-note Beat Prediction}

The approach to predicting out-of-note beats, which do not align with note onsets, requires distinct approach. Liu et al. (2022) proposed a dynamic programming strategy to solve this problem \cite{Liu2022}.

Let us assume that there are $B^i$ in-note beats $\{b_n^i\}_{n=1}^{B^i}$ in total, and out-of-note beats are at subdivisions of the neighboring in-note beats\footnote{We may select only one note per in-note beat, if there are more.} $b_{n}^i$ and $b_{n+1}^i$.

The goal of the procedure is to find out-of-note beats $b^o$ from a set of candidates: \begin{equation}\label{out_of_note_candidates}
b_{n,K}^o = \left\{b_n^i + \frac{k}{K+1}\left(b_{n+1}^i-b_n^i\right)\right\}_{k=1}^K
\end{equation} where $K\in\{0,1,2,3\}$ is the number of out-of-note beats to insert inside the $\left(b_{n+1}^i, b_n^i\right)$ interval.

The number of candidates is selected in order to minimize the tempo change after adding out-of-note beats. The function may be represented as the sum: \[\mathcal{O}_1 = \sum_{n=1}^{B-2}\left|\ln\left(\frac{b_{n+2} - b_{n+1}}{b_{n+1} - b_n}\right)\right|\] where $\{b_n\}_{n=1}^B$ is the sequence of all $B$ beats (both in-note and out-of-note, sorted chronologically).

To discourage the procedure from adding too many out-of-note beats, which leads to an unnecessarily subdivided output, an additional penalty is associated with the objective function: \begin{equation}\label{out_of_note_objective}
\mathcal{O} = \mathcal{O}_1 + \lambda B^o
\end{equation} where $B^o$ is the number of added out-of-note beats, and $\lambda$ is the penalty coefficient. The coefficient is to be found experimentally, however the default value set by the authors is $1$.

\input{sources/algorithms/out_of_note}

\section{Input Data Encoding}

The entire score needs to be transformed to a suitable data format before passing to any of the model components. Given a note sequence tensor as described earlier, the data is encoded into a variety of features: \begin{itemize}
	\item $128$-dimensional one-hot encoding of MIDI pitches.
	\item One-hot onset time-shift ($o_i - o_{i-1}$) quantised by $10$ ms resolution, with maximum value of $4$ s, $401$ features in total.
	\item Raw duration values in seconds.
	\item Velocities normalized to the unit interval $[0, 1]$.
\end{itemize}

The authors tried different encoding schemes, for pitches, onset times and durations, including raw float values in seconds. The full study is available in the paper \cite{Liu2022}.

\section{Score Elements Assignment}

As discussed in the Section \ref{music_score_encoding}, score elements assignment may be viewed as a function from a note sequence $\mathbf{X}$ into a score encoded by a tensor $\mathbf{Y}_n$ representing musical onsets, note values, key signature, time signature, and hand parts, for each note separately.

Besides the quantization model, which relies on the beat tracking module, key signature, time signature and hand parts modules can be treated independently.

\begin{figure}[!ht]
\centering
\input{sources/graphs/model_architecture}
\caption[The architecture of the model.]{The architecture of the model. There are five separate modules of the entire model in total: \emph{beat}, \emph{quantization}, \emph{time signature}, \emph{key signature} and \emph{hand parts}.}
\end{figure}

\section{Score Generation}

The final score generation process synthesizes the source MIDI stream, incorporating pitch sequence, with the model's output. The resultant annotated MIDI encompasses essential information for visual score generation, including key/time signatures, which are not typically present in standard MIDI files.

\missing

\section{Training}

\subsection{Datasets}

The dataset used by the authors of the paper \cite{Liu2022} integrates three sources of MIDI files: \begin{itemize}
	\item The \emph{Classical Piano MIDI} (CPM) database \cite{Krueger1996}.
	\item The \emph{Augmented MIDI Aligned Piano Sounds} (A-MAPS) \cite{Ycart2018}.
	\item The \emph{Aligned Scores and Performances} (ASAP) dataset \cite{Foscarin2020}.
\end{itemize}

The datasets consists of a variety of classical piano pieces by composers including as Bach, Mozart, Beethoven, Schubert, Chopin, Liszt, and others from the Western European classica repertoire.

Certain musical features, such as time and key signatures, are not always encoded in MIDI files. Consequently, the MIDI files have been annotated using different strategies.

\begin{table}[ht!]
\centering
\input{sources/tables/datasets}
\caption[Statistics of the dataset used for training]{Statistics of the dataset used for training \cite{Liu2022}. Performances of the same piece are counted only once.}
\end{table}

The datasets partially overlap; for instance, A-MAPS is derived in part from CPM. Below is a brief overview of each data source.

\subsubsection{Classical Piano MIDI Database}

The \emph{Classical Piano MIDI} (CPM) database was created by Bernd Krüger, who produced hundreds of MIDI files containing interpretations of classical piano works. Krüger describes his motivation as follows \cite{Krueger1996}:

\begin{quote}The page serves to describe and make available my interpretations of classical piano works. Although I am a layman in terms of music, I have set myself the goal of painstakingly interpreting difficult works. I would like to make these works accessible to as many musically interested people as possible.\end{quote}

The dataset consists 337 pieces with a cumulative duration of approximately $23$ hours. All MIDI files are score-informed, with separate tracks for the left and right hands.  and time signatures are encoded in the MIDI files as meta messages.

However, despite the fact that the MIDI files are tempo-varied, they were manually crafted, not performed. For instance, note onsets which lie on the same beat, occur simultaneously. In musical performance, even chords are played with certain time variation.

\subsubsection{Augmented MIDI Aligned Piano Sounds}

The \emph{Augmented MIDI Aligned Piano Sounds} (A-MAPS) dataset builds on the original \emph{MIDI Aligned Piano Sounds} (MAPS) dataset, introduced by Emiya et al. \cite{Emiya2010}. MAPS contains approximately 65 hours of data, including both MIDI and corresponding audio recordings, and is divided into four main categories:\begin{itemize}
	\item \textbf{ISOL}: Isolated notes and monophonic excerpts.
	\item \textbf{RAND}: Chords with random pitch notes.
	\item \textbf{UCHO}: Usual chords from Western music.
	\item \textbf{MUS}: Piano music pieces, sourced from the \emph{Classical Piano MIDI} database. \end{itemize}

MAPS has been widely used as a benchmark for AMT systems \cite{Ycart2018}. However, the information provided by the MIDI files is limited to basic attributes: pitch, onsets and offsets in seconds, and velocity. The A-MAPS dataset enriches the original with additional annotations, including meter, note values, key signatures and hand separation. Moreover, A-MAPS provides also tempo curves and sustain pedal activations, although this information is not used by the considered model.

The A-MAPS dataset contains 269 files of 159 unique pieces.

\subsubsection{Aligned Scores and Performances}

The \emph{Aligned Scores and Performances} (ASAP) dataset consists of 222 musical score aligned with 1068 performances, of the total duration of 92 hours \cite{Foscarin2020}.

The ground truth is provided twofold: as musical scores in MusicXML format, and quantized MIDI files.

For each performance, including the quantized MIDI file, annotations are provided in tab-separated values (TSV) format. These annotation specify timestamps for: \begin{itemize}
	\item Beats (b) and downbeats (db).
	\item Time signature changes.
	\item Key signature changes.
\end{itemize}

\begin{table}[ht!]
\centering
\input{sources/tables/annotations}
\caption[Example of performance MIDI annotation in TSV format]{An example of a performance MIDI annotation in TSV format. The first row indicates an initial time signature of ${6 \atop 8}$. The piece begins in the key of D major, encoded as 2.}
\end{table}

The MIDI are actual human performances recorded via digital instruments. 

\section{Model Performance}

\missing

\section{Robustness Analysis}

\subsection{Ceteris Paribus}

We conducted a \emph{ceteris paribus} analysis focusing on musical element models, making several assumptions:

\begin{itemize}[noitemsep, topsep=4pt]
	\item Note velocity should not affect time signature or hand part assignment.
	\item Note duration should not impact key signature.
	\item Note pitch is crucial for key signature/hand part assigning, while other features should not contribute.
	\item Note pitch does not matter for the time signature.
\end{itemize}

These should be regarded as general guidelines rather than strict principles.

\subsubsection{Perturbations}

For a sample $M=50$ musical pieces from the dataset, we introduced perturbations to test the assumptions:

\begin{itemize}[noitemsep, topsep=4pt]
	\item Changing velocity with standard deviations $\sigma_v$ of 8, 16, 32, and 64.
	\item Scaling note lengths by a factor from an interval $\alpha\in(\alpha_l,1)$, where $\alpha_l\in\{0.9, 0.75, 0.5, 0.2\}$ (note extension may lead to overlapping).
	\item Changing pitch with standard deviations $\sigma_p$ of 12 and 24 (whole octave, double octave).
\end{itemize}

Each transformation is applied separately to each feature, note by note. Notice that we don't change onsets as they change the musical structure of a piece.

For each feature model $f$: \emph{hand part} $f_H$, \emph{key signature} $f_K$, \emph{time signature} $f_T$, we calculated the average error between the output sequence and the output of a perturbed sequence.

For each element ${\bf x}_i$ in the dataset sample $i\in\{1,2,\ldots,50\}$ and for each change, we sampled $m=10$ perturbations ${\bf x}^{(j)}$ of the original item and measured the error: \[\operatorname{error}\left(f\right) = \frac{1}{M}\sum_{i=1}^{M}\frac{1}{m}\sum_{j=1}^{m}\operatorname{error}\left(f\left({\bf x}_i\right), f\left({\bf x}_i^{(j)}\right)\right)\] Here, $\operatorname{error}$ between two sequences ${\bf x} = (x_i)_{i=1}^N$ and ${\bf y} = (y_i)_{i=1}^N$ is defined as: \[\operatorname{error}({\bf x},{\bf y}) = \frac{1}{N}\sum_{i=1}^N \left[x_i \neq y_i\right]\] Higher error values indicate greater influence on the output $f$ from a transformation.

This analysis directly measures the model's robustness to specific transformations and does not rely on the model's overall performance.

\subsubsection{Results}

Among all three models, the key signature $f_K$ model proved to be robust to all proposed transformations, with an average error of less than $1.5\%$ in each category.

The hand part model $f_H$ error is robust to note shortening (maximum average error of $3\%$), but inconsistent when note velocities are changed. This inconsistency is mitigated by maintaining consistent velocity for notes played simultaneously. We hypothesize that chords played by one hand typically have similar velocities (see Table \ref{perturbations_hand_part} for detailed results).

The time signature model is quite robust to note velocity and pitch manipulations (average errors less than $13\%$), but loses consistency when notes are shortened. \begin{table}[ht!]
\input{sources/tables/perturbations_errors}
\caption{The average errors of certain perturbations (in percent).}
\label{perturbations}
\end{table}

\begin{table}[ht!]
\input{sources/tables/hand_part_perturbations}
\caption[The average errors for the hand part model]{The average errors for the hand part model $f_H$ for 1. standard perturbation, 2. uniform random change for notes played in the same time. The second transformation introduces less inconsistencies.}
\label{hand_part_perturbations}
\end{table} 

\begin{figure}[!ht]
\centering
\includegraphics[width=0.96\textwidth]{images/ceteris_paribus_results.png}
\caption[\emph{Ceteris paribus} analysis model errors]{\emph{Ceteris paribus} analysis model errors.}
\label{ceteris_paribus}
\end{figure}

\subsection{Local Feature Importance}

We encountered challenges with common explainable machine learning tools: 

\begin{itemize}[noitemsep, topsep=4pt]
	\item Input data is a tensor of variable length, not conforming to tabular or image-like formats supported by many tools.
	\item The input data is a very high-dimensional space, computationally infeasible for certain methods (e.g. Shapley values).
	\item Pitch perturbations introduce non-integer values, and the pitch space lacks a meaningful measure of distance.
	\item Some model components (e.g. GRU blocks, ELU activation function) are not supported by certain XAI libraries.
\end{itemize} 

We developed a custom solution to overcome these obstacles. For velocity, we applied a LIME-like approach due to the absence of metric structure in the pitch space. We generated a locally modified version of the MIDI stream tensor for each note, randomizing the velocity for one note. We created $100$ samples for each note, calculating model predictions to compare with the original prediction. This approach aids in explaining the findings from the previous section (see Figure \ref{hand_part_misalignment} for an example).

We also measured the mean influence through this approach: the square root of the mean of squares of linear regression coefficients. This approach aligns with the \emph{ceteris paribus} analysis.

\begin{table}[ht!]
\input{sources/tables/lime_like_approach}
\caption{The average influence of three models calculated by a LIME-like approach.}
\end{table} 

However, the proposed approach ignores the feature interaction and silently assumes variable independence, both horizontally and vertically. This is a serious limitation of this approach. 

\subsection{Key Signature Assignment by Note Omission}

For the key signature model, as the pitch space is not a metric space, we applied a different strategy. The method is as follows: 
\begin{enumerate}[noitemsep, topsep=4pt]
	\item Get the key signature prediction values (before the last activation function) for the entire sequence.
	\item Individually remove each note in a sequence and calculate the difference between the original prediction and the prediction made on a sequence without a single note.
	\item Compute a contribution score, represented by the mean of the differences for each note.
	\end{enumerate} This approach enables the measurement of each note's contribution to the attribution of a specific scale. Refer to Figure \ref{note_removing} for an illustrative example.

\subsection{Conclusion}

We were able to discover certain artifacts of the score generation models, especially when it comes to velocity contribution to hand part assignment. The time signature model is not fully robust to alterations that should not have affect the output.

Unfortunately, the proposed methods have drawbacks and are not fully justified. Current XAI methods do not work well with symbolic music data in general. Developing more tailored and adaptable XAI methods for musical applications could contribute to improved model interpretability. One of the challenge would be to find a reasonable (and interpretable) embedding of the pitch space that encodes musical features. 

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{images/ceteris_paribus_results_h.png}
\includegraphics[width=0.7\textwidth]{images/ceteris_paribus_results_k.png}
\includegraphics[width=0.7\textwidth]{images/ceteris_paribus_results_t.png}
\caption[Results of the \emph{ceteris paribus} experiment]{Results of the \emph{ceteris paribus} experiment for the hand part model $f_H$, the key signature model $f_K$ and the time signature model $f_T$. The hand part model is robust to note duration changes while it is susceptible to velocity manipulation. On the other hand, the time signature model is sensitive to note duration changes, which is expected to some extent, and relatively robust to other transformations. The key signature model is robust to all perturbations.}
\label{ceteris_paribus}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{images/hand_part1.png}
\includegraphics[width=0.95\textwidth]{images/hand_part2.png}
\caption[The piano roll for the hand part model output]{The piano roll for the hand part model output. Each note is represented as a (blue or red) block, indicating a pitch (note), duration and velocity. The first graph represents an original sequence while the second has perturbed velocity. Left-hand notes are marked as red. We can see that there are much many misalignment in the hand assignment in the second output. As a rule of thumb, the left hand notes (red) should be at the bottom while the right hand ones (blue) should be at the top.}\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{images/lime_hand_part.png}
\caption[A graph showing how the velocity of a note influences hand part assignments of other notes]{A graph showing how the velocity of the second C5 note (with a thick outline) influences hand part assignments of other notes. It can be observed that the current velocity of this note makes the model think of the first F3 note as a left hand note. Reducing the note C5 velocity to a low value results in a misassigning F3 as the right hand note. This is, of course, a undesired behavior of the model.}
\label{hand_part_misalignment}\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{images/note_removing.png}
\caption[A key signature alignment for D$\sharp$ scale]{A key signature alignment for D$\sharp$ scale. The names on the notes represent the model key signature attribution. There are two different scales assigned by the model: D$\sharp$ scale and G$\sharp$ scale. Notes: D, E and A are not in the scale and have a negative influence on the assignment.}
\label{note_removing}\end{figure}
