\chapter{Performance MIDI-to-Score Conversion by Neural Beat Tracking}\label{performance_midi_to_score_conversion_by_neural_beat_tracking}

The main model of interest \cite{Liu2022}, developed by Liu et al. (2022), is a state-of-the-art machine learning model for complete music transcription. It has been awarded as the best paper of the \emph{International Society for Music Information Retrieval Conference} (ISMIR) in December 2022.

The model is composed of two parts:
\begin{itemize}
	\item Beat Tracking.
	\item Score Element Assignment.
\end{itemize}

The first part is responsible for quantizing the raw material into beats and downbeats. Each measure is consisted of several beats, and the number of beats in a measure is governed by the time signature. The beats mark the tempo, and they are the basis for the musical onsets detection.

The second part assigns every other aspect of the musical score to the MIDI stream. Let us recall these elements: musical onsets, note values, hand parts, key signature and time signature.

Let us review both components.

\section{Beat Tracking}

A single measure consists of several beats, and the rhythmic structure of beats is determined by time signature (or signatures). A performance MIDI does not contain information about beats and one of the objectives of the model is to predict locations of these.

After these predictions, note beginnings (onsets) can be placed in a subdivision of a single beat. The smallest unit of time distance is thus dictated by the size of the beat subdivision. More precisely, a musical onset $mo_i$ of the $i$-th note is defined as \[mo_i = \frac{s_n}{S}\] where $S$ is the number of subdivisions per beat in rhythm quantization, and $s_n$ is the musical onset time expressed in the number of subdivisions.

The authors provide a novel approach to detecting beats, splitting them into two categories, for which there are two separate methods.

Beats can fall into two distinct groups:
\begin{itemize}
	\item \emph{In-note}: beats concurrent with at least one note onset. This means that some notes play within the start of the beat.
	\item \emph{Out-of-note}: a beat is not concurrent with any note onset. No notes are beginning with such beats.
\end{itemize}

\subsection{In-note Beat Prediction}

For predicting in-note beats, one can reduce the problem to a binary classification task on the note sequence $\mathbf{X}$. This sequence is assumed to have a length $N$.

\begin{figure}[!ht]
\centering
\input{sources/graphs/in_note}
\caption[In-note beat prediction model.]{In-note beat prediction model with a CRNN with 3 convolutional layers and 2 bi-directional gated recurrent unit (GRU) layers.}
\end{figure}

The probability $P_n$ that $n$-th note is concurrent with a beat is defined as: \[P_n = \mathbb{P}\left(B_n|\mathbf{X}\right)\] where $B_n\in\{0,1\}$ is the ground-truth beat label for the $n$-th note from the note sequence. The model is trained using the standard cross-entropy loss function: \[\mathcal{L}=-\frac{1}{N}\sum_{n=1}^N B_n\ln P_n + \left(1-B_n\right)\ln\left(1-P_n\right)\] The authors used CRNN with 3 convolutional layers and 2 bidirectional gated recurrent unit (GRU) layers. The probability threshold for positive classification has been set dynamically, depending on the maximum probability in a fixed segment length.

\subsection{Out-of-note Beat Prediction}

The approach to predicting out-of-note beats, which do not align with note onsets, requires distinct approach. Liu et al. (2022) proposed a dynamic programming strategy to solve this problem \cite{Liu2022}.

Let us assume that there are $B^i$ in-note beats $\{b_n^i\}_{n=1}^{B^i}$ in total, and out-of-note beats are at subdivisions of the neighboring in-note beats\footnote{We may select only one note per in-note beat, if there are more.} $b_{n}^i$ and $b_{n+1}^i$.

The goal of the procedure is to find out-of-note beats $b^o$ from a set of candidates: \begin{equation}\label{out_of_note_candidates}
b_{n,K}^o = \left\{b_n^i + \frac{k}{K+1}\left(b_{n+1}^i-b_n^i\right)\right\}_{k=1}^K
\end{equation} where $K\in\{0,1,2,3\}$ is the number of out-of-note beats to insert inside the $\left(b_{n+1}^i, b_n^i\right)$ interval.

The number of candidates is selected in order to minimize the tempo change after adding out-of-note beats. The function may be represented as the sum: \[\mathcal{O}_1 = \sum_{n=1}^{B-2}\left|\ln\left(\frac{b_{n+2} - b_{n+1}}{b_{n+1} - b_n}\right)\right|\] where $\{b_n\}_{n=1}^B$ is the sequence of all $B$ beats (both in-note and out-of-note, sorted chronologically).

To discourage the procedure from adding too many out-of-note beats, which leads to an unnecessarily subdivided output, an additional penalty is associated with the objective function: \begin{equation}\label{out_of_note_objective}
\mathcal{O} = \mathcal{O}_1 + \lambda B^o
\end{equation} where $B^o$ is the number of added out-of-note beats, and $\lambda$ is the penalty coefficient. The coefficient is to be found experimentally, however the default value set by the authors is $1$.

\input{sources/algorithms/out_of_note}

\section{Input Data Encoding}

The entire score needs to be transformed to a suitable data format before passing to any of the model components. Given a note sequence tensor as described earlier, the data is encoded into a variety of features: \begin{itemize}
	\item $128$-dimensional one-hot encoding of MIDI pitches.
	\item One-hot onset time-shift ($o_i - o_{i-1}$) quantised by $10$ ms resolution, with maximum value of $4$ s, $401$ features in total.
	\item Raw duration values in seconds.
	\item Velocities normalized to the unit interval $[0, 1]$.
\end{itemize}

There are $531$ features in total.

The authors tried different encoding schemes, for pitches, onset times and durations, including raw float values in seconds. The full study is available in the paper \cite{Liu2022}.

\section{Score Elements Assignment}

As discussed in the Section \ref{music_score_encoding}, score elements assignment may be viewed as a function from a note sequence $\mathbf{X}$ into a score encoded by a tensor $\mathbf{Y}_n$ representing musical onsets, note values, key signature, time signature, and hand parts, for each note separately.

Besides the quantization model, which relies on the beat tracking module, key signature, time signature and hand parts modules can be treated independently. \textbf{The quantization model is not used for the final MIDI creation, and we won't focus on that part in much detail}.

\begin{figure}[!ht]
\centering
\input{sources/graphs/model_architecture}
\caption[The initial architecture of the model.]{The initial architecture of the model proposed in \cite{Liu2022}. There are five separate modules of the entire model in total: \emph{beat}, \emph{quantization}, \emph{time signature}, \emph{key signature} and \emph{hand parts}.}
\end{figure}

The initial version of time signature model handled a finite set of time signature numerators and denominators, as described in Section \ref{music_score_encoding}, changing over time. Later, after the paper release, the model has been reduced to recognize only two the most common time signatures: $\lilyTimeSignature{4}{4}$ and $\lilyTimeSignature{3}{4}$, and only for the entire song. The architecture has been simplified to only convolutional layers, with the GRUBlock removed. With an additional support of beat quantizer, additional two time signatures $\lilyTimeSignature{2}{4}$ and $\lilyTimeSignature{6}{8}$ could be recognized. For performance analysis, we use the latter approach. Though, the role of the time signature model is rather supplementary: in principle, the time signature could be inferred from the beats and downbeats arrangement, obtained by the beat model.

Instead of direct tempo estimation, for each note an \emph{inter-beat-interval} time is estimated using classification, capturing time intervals between successive beats with 10 ms resolution, up to $4$ seconds.

\section{Score Generation}

The final score generation process synthesizes the source MIDI stream, incorporating pitch sequence, with the model's output. The resultant annotated MIDI encompasses essential information for visual score generation, including key/time signatures, which are not typically present in standard MIDI files.

\section{Training and Evaluation}

The training and evaluation procedures were implemented to replicate those described by the original authors. In the following sections, we provide a detailed overview of the datasets, augmentation techniques, and specific configurations for training and evaluation setups.

\subsection{Datasets}\label{datasets}

The dataset used by the authors of the paper \cite{Liu2022} integrates three sources of MIDI files: \begin{itemize}
	\item The \emph{Classical Piano MIDI} (CPM) database \cite{Krueger1996}.
	\item The \emph{Augmented MIDI Aligned Piano Sounds} (A-MAPS) \cite{Ycart2018}.
	\item The \emph{Aligned Scores and Performances} (ASAP) dataset \cite{Foscarin2020}.
\end{itemize}

The datasets consists of a variety of classical piano pieces by composers including as Bach, Mozart, Beethoven, Schubert, Chopin, Liszt, and others from the Western European classical repertoire.

Certain musical features, such as time and key signatures, are not always encoded in MIDI files. Consequently, the MIDI files have been annotated using different strategies.

\begin{table}[ht!]
\centering
\input{sources/tables/datasets}
\caption[Statistics of the dataset used for training.]{Statistics of the dataset used for training \cite{Liu2022}. Performances of the same piece are counted only once.}
\label{train_valid_test}
\end{table}

The datasets partially overlap; for instance, A-MAPS is derived in part from CPM. Authors avoid using the same pieces in train, validation and test sets by using distinct music piece labels coming from different datasets.

\begin{table}[ht!]
\begin{center}
\input{sources/tables/dataset_annotations}
\caption[Annotation structure for the subdatasets.]{Annotation structure for the subdatasets.}
\end{center}
\end{table}

All dataset are jointly combined into one dataset, \emph{A Dataset of Aligned Classical Piano Audio and Scores} (ACPAS) \cite{Liu2021}.

Below is a brief overview of each data source.

\subsubsection{Classical Piano MIDI Database}

The \emph{Classical Piano MIDI} (CPM) database was created by Bernd Krüger, who produced hundreds of MIDI files containing interpretations of classical piano works. Krüger describes his motivation as follows \cite{Krueger1996}:

\begin{quote}The page serves to describe and make available my interpretations of classical piano works. Although I am a layman in terms of music, I have set myself the goal of painstakingly interpreting difficult works. I would like to make these works accessible to as many musically interested people as possible.\end{quote}

The dataset consists 337 pieces with a cumulative duration of approximately $23$ hours. All MIDI files are score-informed, with separate tracks for the left and right hands.  and time signatures are encoded in the MIDI files as meta messages.

However, despite the fact that the MIDI files are tempo-varied, they were manually crafted, not performed. For instance, note onsets which lie on the same beat, occur simultaneously. In musical performance, even chords are played with certain time variation. Some authors disregard the source as not reliable. For example, Beyer and Dai claim \cite{Beyer2024}: \begin{quote}The underlying data is score-derived and has been manually adjusted to represent aspects of performance and score at the same time. This leads to information leakage as highly regular onset/offset alignment remains in the \emph{performance} data.\end{quote}

\subsubsection{Augmented MIDI Aligned Piano Sounds}

The \emph{Augmented MIDI Aligned Piano Sounds} (A-MAPS) dataset builds on the original \emph{MIDI Aligned Piano Sounds} (MAPS) dataset, introduced by Emiya et al. \cite{Emiya2010}. MAPS contains approximately 65 hours of data, including both MIDI and corresponding audio recordings, and is divided into four main categories:\begin{itemize}
	\item \textbf{ISOL}: Isolated notes and monophonic excerpts.
	\item \textbf{RAND}: Chords with random pitch notes.
	\item \textbf{UCHO}: Usual chords from Western music.
	\item \textbf{MUS}: Piano music pieces, sourced from the \emph{Classical Piano MIDI} database. \end{itemize}

MAPS has been widely used as a benchmark for AMT systems \cite{Ycart2018}. However, the information provided by the MIDI files is limited to basic attributes: pitch, onsets and offsets in seconds, and velocity. The A-MAPS dataset enriches the original with additional annotations, including meter, note values, key signatures and hand separation. Moreover, A-MAPS provides also tempo curves and sustain pedal activations, although this information is not used by the considered model.

The A-MAPS dataset contains 269 files of 159 unique pieces.

\subsubsection{Aligned Scores and Performances}

The \emph{Aligned Scores and Performances} (ASAP) dataset consists of 222 musical score aligned with 1068 performances, of the total duration of 92 hours \cite{Foscarin2020}.

The ground truth is provided twofold: as musical scores in MusicXML format, and quantized MIDI files.

For each performance, including the quantized MIDI file, annotations are provided in tab-separated values (TSV) format. These annotation specify timestamps for: \begin{itemize}
	\item Beats (b) and downbeats (db).
	\item Time signature changes.
	\item Key signature changes.
\end{itemize}

\begin{table}[ht!]
\centering
\input{sources/tables/annotations}
\caption[Example of performance MIDI annotation in TSV format.]{An example of a performance MIDI annotation in TSV format. The first row indicates an initial time signature of $\lilyTimeSignature{6}{8}$. The piece begins in the key of D major, encoded as 2.}
\label{annotations}
\end{table}

The MIDI are actual human performances recorded via digital instruments. The annotations for performance provide ground-truth labels for all considered models. Otherwise, one would need to compare all different interpretations to a single score, risking judging the performance's fidelity to the original instead of transcription quality.

\subsection{Data Augmentation} \label{data_augmentation}

To increase model's versatility, the authors considered four data augmentation techniques, that should not affect the transcription. The note sequence tensors have been transformed using the following methods: \begin{itemize}
	\item \textbf{Pitch Shift}. Transpose all notes by a constant pitch from the range of $\{-12, -11, \ldots, 12\}$.
	\item \textbf{Tempo Change}: Change the tempo by multiplication by a constant from the range $[0.8, 1.2]$.
	\item \textbf{Note Removal}. For each group of $m$ concurrent notes, a random number from $0$ to $m - 1$ of notes is being removed.
	\item \textbf{Note Introduction}. For some notes a concurrent ones are being added, with the same velocity and duration as 
\end{itemize}

Section \ref{beat_tracking} shows ablation studies for data augmentation, for the beat-tracking model.

\subsection{Training and Evaluation}

To reproduce the original results, we followed the authors' training and evaluation setup, adhering to the same training, validation, and test dataset divisions. In the following sections, we provide a detailed examination of the training and evaluation pipeline, including feature preparation and the training and evaluation scheme.

\subsubsection{Feature Preparation}

As the data comes from different sources, they need to be unified before training.

Initially, all MIDI files were gathered from the datasets outlined in Section \ref{datasets}. Following the authors' methodology, all pieces in the \texttt{ENSTDkCl} subset of the MAPS dataset were designated as the test set. The remaining pieces were randomly allocated to the validation set, with $90\%$ reserved for training\footnote{Specifically, pieces with an ID divisible by 10 were assigned to the validation set.}. Refer to Table \ref{train_valid_test} for dataset division details.

The next step involved extracting note sequence tensors and additional features from MIDI files and, where relevant, from external annotations, as in the case of the ASAP dataset. The features encompassed all essential elements for transcription, including beats, downbeats, time signatures, key signatures, onsets, note durations, and hand part assignments.

Once processed, the datasets were ready for model training.

\subsubsection{Training}

Each model was trained with a maximum of 50 epochs. Training examples were augmented according to the procedures discussed in Section \ref{data_augmentation}.

\subsubsection{Evaluation}

During training, validation metrics were calculated after each epoch. The metrics used varied by model: for binary classifiers (such as the time signature and hand part models), standard metrics (accuracy, precision, recall, and $F_1$ score) were employed. For the key signature model, which is a multiclass classifier, both micro and macro metrics were used (see Section \ref{binary_and_multiclass_classifier_metrics}).

Upon completion of training, the model with the highest $F_1$ score on the validation set was selected for further evaluation on the test set. A joint performance score across all models was then calculated using the MV2H metric on the test set.

\section{Model Performance}

We reproduced the results using the training and evaluation setup provided by the authors, described in the previous sections. We achieved a comparable MV2H metric scores on the test set as the paper's authors.

\subsection{Validation Metrics}

Below, we present the validation metrics, including $F_1$ scores, for each submodel.

\begin{table}[ht!]
\centering
\input{sources/tables/f1_hand_part}
\caption[Validation metrics of the hand part model.]{Validation metrics of the hand part model.}
\end{table}

\begin{table}[ht!]
\centering
\input{sources/tables/f1_key_signature}
\caption[Validation metrics of the time signature model.]{Validation metrics of the time signature model.}
\end{table}

\begin{table}[ht!]
\centering
\input{sources/tables/f1_time_signature}
\caption[Validation metrics of the key signature model.]{Validation metrics of the key signature model.}
\end{table}

\begin{table}[ht!]
\centering
\input{sources/tables/f1_beat}
\caption[Validation metrics of the beat quantization model.]{Validation metrics of the beat quantization model.}
\end{table}

\subsection{MV2H Metric}

The MV2H metric results on the test set are presented below, alongside the original authors' evaluation results. It should be noted that minor changes have been introduced since the release of the model, so the comparison is not exact.

We use the following notation for the MV2H components: \begin{itemize}
	\item Multi-pitch detection $F_{\textrm{p}}$
	\item Voice separation $F_{\textrm{vs}}$
	\item Metrical alignment $F_{\textrm{ma}}$
	\item Note value detection $F_{\textrm{nv}}$
	\item Harmonic analysis $F_{\textrm{ha}}$
\end{itemize}

$F$ stands for the final MV2H metric, the average of all submetrics.

\begin{table}[ht!]
\centering
\input{sources/tables/mv2h}
\caption[MV2H metric evaluation on the test set.]{MV2H metric evaluation on the test set, with results compared to the original model evaluation.}
\end{table}

\section{Case Studies}

To evaluate the model’s performance, we conducted an analysis of specific outputs from the validation and test sets. This investigation aimed to assess how effectively the model handled the transcription task. Specifically, we sought to answer the following questions:
\begin{itemize} 
	\item What are the most common types of errors?
	\item How disruptive are these errors to the resulting score?
	\item What level of effort is required to repair a flawed transcription?
\end{itemize}

The model performed quite well on human-crafted MIDI songs. Transcriptions of live performances exhibited a greater prevalence of note metric alignment errors and, in some cases, were rendered unreadable due to an overabundance of inaccuracies.

For key or time signature misalignments, the corrections required were generally straightforward and demanded a single change in the transcription. Errors in key signature assignments were relatively rare, but the time signature model's classifier constraints made it impossible to handle non-standard odd signatures. 

The model occasionally struggled with establishing the correct metric grid, particularly for pieces with unusual or complex time signatures. These instances, while infrequent, significantly impacted the overall transcription quality. Section \ref{meter_misalignment} provides an example of such a case.

Below, we outline three common errors observed in the transcriptions: a metrical shift, hand part misalignments and the most severe of them: meter misalignments.

\subsection{Unrecognized Pickup Measures}

One prevalent error type involves a systematic shift of the entire piece by a fraction of a measure. A special case of this error is the failure to recognize a pickup measure (an incomplete measure at the beginning of a piece).

\begin{figure}[ht!]
\centering
\begin{tabular}{c}a)
\includesvg[width=0.95\textwidth]{images/albeniz_gen.svg} \\ b)
\includesvg[width=0.95\textwidth]{images/albeniz_output_gen.svg}
\end{tabular}
\caption[\emph{Sevilla} by Isaac Albéniz.]{\emph{Sevilla} by Isaac Albéniz: a) the piece with a distinguished pickup measure (first three notes for each hand), b) the model transcription. Since the model assumed the time signature of $\lilyTimeSignature{3}{4}$, the resulting content is shifted by the half of a measure. Other musical elements have been correctly assigned.}
\label{albeniz}
\end{figure}

This type of error usually requires some manual adjustments: shifting the entire piece and, probably, correcting note ties that extend across measure boundaries.

\subsection{Hand Part Misalignments}

Hand part assignment errors, although not so common, do occur. These errors can be particularly problematic when a single note is misattributed to the wrong hand.

Some errors of this type are easily noticeable, while others are subtle and harder to detect. Even a single hand part misalignment can render a piece unplayable, requiring manual verification and correction. In some cases, the piece remains playable despite the error, but the transcription quality is slightly degraded.

\subsection{Meter Misalignment}\label{meter_misalignment}

One illustrative example of meter misalignment is found in the transcription of Claude Debussy’s \emph{Clair de Lune}, a piece from the composer’s \emph{Suite bergamasque}. The original score employs a non-standard $\lilyTimeSignature{9}{8}$ time signature in the key of D$\flat$.

\begin{figure}[ht!]
\centering \begin{tabular}{c}a) \includesvg[width=0.95\textwidth]{images/claire_de_lune_gen.svg} \\ b) \includesvg[width=0.95\textwidth]{images/claire_de_lune_output_gen.svg} \end{tabular} \caption[Claude Debussy's \emph{Clair de Lune}.]{The beginning of Claude Debussy's \emph{Clair de Lune}: a) the original score from the MAPS test dataset, b) the model transcription. The model misaligned the metric structure by mixing quarter notes with triplets, disrupting the inner relationships between notes within a measure. Some right-hand notes (e.g., C in the second measure) are misattributed. The key signature was assigned correctly.}
\label{claire_de_lune}
\end{figure}

In this case, the model incorrectly imposed a $\lilyTimeSignature{4}{4}$ time signature, resulting in a mixture of quarter notes and triplets that do not align with the original meter. This error not only disrupts the metric structure but also breaks the temporal relationships between consecutive notes. Consequently, simply changing the time signature to ${9 \atop 8}$ is insufficient to resolve the issue. Correcting such a transcription requires substantial effort.

Errors of this type are particularly common in pieces with unusual time signatures. While the severity of the error varies depending on the extent of the misalignment, such issues often require significant manual intervention to repair. In many cases, these errors are confined to specific phrases or sections, but they are more prevalent in transcriptions of fast-paced live performances.

\section{Robustness Analysis} \label{robustness_analysis}

The quality of the model can be considered in many dimensions. Evaluating model quality extends beyond the accuracy of outputs relative to ground truth; it also includes assessing how robust the model is to transformations that should theoretically leave certain outputs unchanged.

For this analysis, we make the following assumptions: \begin{itemize}
	\item Note velocity should not influence time signature or hand part assignment.
	\item Note pitch should be irrelevant to time signature prediction.
	\item Note pitch is essential for determining key signature and hand part assignment, while other features should have minimal impact on these aspects.\
	\item Note duration should not affect key signature assignment.
\end{itemize} These are intended as general guidelines rather than strict rules. There may be situations where slight deviations occur. For instance, a chord played be one hand tend to be of uniform intensity but the second hand may play a note with markedly different articulation than the other. These nuances explain why certain features cannot be entirely disregarded across all cases. For a more in-depth analysis of feature importance, refer to Section \ref{ablation_studies}.

A robust model should obey to these rules to some reasonable extent. For example, it is natural  to expect the model to assign the same time signatures to a transposed version of a piece, as a pitch shift should not affect the time signature.

We conducted an analysis of how the model is robust to transformation, for each of three components: \begin{itemize}
	\item The \textbf{hand part} $f_H$.
	\item The \textbf{key signature} model $f_K$.
	\item The \textbf{time signature} model $f_T$.
\end{itemize} In the subsequent sections, we use models pretrained by the authors unless stated otherwise.

\subsection{\emph{Ceteris Paribus} Analysis}

We conducted a \emph{ceteris paribus} (Latin for \emph{all other things being equal}) type of analysis by augmenting original note sequence tensors. This approach allowed us to compare model outputs for transformed note sequences, isolating and measuring the influence of specific features.

A natural way of augmenting data in this context is to randomly distort a single feature (e.g., pitch, duration, or velocity), while keeping all others intact.

\subsubsection{Perturbations} \label{perturbations}

For a sample $M = 50$ musical pieces from the dataset, we introduced perturbations to test the assumptions given in the Section \ref{robustness_analysis}: \begin{itemize}
	\item Changing velocity with standard deviations $\sigma_v$ of 8, 16, 32, and 64, but with clipping to the range $[1, 127]$.
	\item Scaling note lengths by a factor from an interval $\alpha\in(\alpha_l,1)$, where $\alpha_l\in\{0.9, 0.75, 0.5, 0.2\}$ (note extension may lead to overlapping).
	\item Changing pitch with standard deviations $\sigma_p$ of 12 and 24 (whole octave, double octave). However, we ensure, that the entire piece stays in the playable range of pitches $[21, 108]$, corresponding to the range of a typical $88$-key piano.
\end{itemize}

Each transformation was applied independently to each feature, note by note. Importantly, note onsets were not altered, as this would fundamentally change the structure of the music.

For each feature model $f$: \emph{hand part} $f_H$, \emph{key signature} $f_K$, \emph{time signature} $f_T$, we calculated the average error between the output sequence and the output of a perturbed sequence.

For each element ${\bf x}_i$ in the dataset sample $i\in\{1,2,\ldots,50\}$ and for each change, we sampled $m=10$ perturbations ${\bf x}^{(j)}$ of the original item and measured the error: \[\operatorname{error}\left(f\right) = \frac{1}{M}\sum_{i=1}^{M}\frac{1}{m}\sum_{j=1}^{m}\operatorname{error}\left(f\left({\bf x}_i\right), f\left({\bf x}_i^{(j)}\right)\right)\] Here, $\operatorname{error}$ between two sequences ${\bf x} = (x_i)_{i=1}^N$ and ${\bf y} = (y_i)_{i=1}^N$ is defined as the mean number of indices for which these sequences differ: \[\operatorname{error}({\bf x},{\bf y}) = \frac{1}{N}\sum_{i=1}^N \left[x_i \neq y_i\right]\]  Higher error values indicate a greater sensitivity of the model to the applied transformation. Note that this analysis does not assess whether a change improves or worsens the model’s performance; it solely measures the impact of the transformation.

\subsubsection{Results} \label{results}

The results of the perturbation experiments are summarized below.

The key signature model $f_K$ demonstrated high robustness across all perturbations, with an average error of less than $1.5\%$ in all categories.

The time signature model $f_T$ was robust to pitch and velocity manipulations, with average errors below $13\%$. However, it exhibited sensitivity to changes in note durations, consistent with the role of duration in determining meter. See Table \ref{average_errors} for details.

\begin{table}[ht!]
\input{sources/tables/perturbations_errors}
\caption[The average errors of certain perturbations (in percent).]{The average errors of certain perturbations (in percent).}
\label{average_errors}
\end{table}

The hand part model $f_H$ was robust to note duration changes (maximum average error of $3\%$), but it exhibited significant inconsistency when note velocities were modified. This issue was mitigated when velocity perturbations were applied uniformly to notes played simultaneously, reflecting the common practice of maintaining similar velocities for chords played by one hand (see Table \ref{hand_part_perturbations} for detailed results).

\subsubsection{Hand Part Mitigation Strategies}

To address the hand part model’s sensitivity to velocity perturbations, we explored two strategies: \begin{enumerate} \item \textbf{Data Augmentation}: Introducing random noise to the velocity feature during training, with a probability of application set to $50\%$ and a standard deviation $\sigma_v = 12$. \item \textbf{Feature Removal}: Excluding the velocity feature entirely. \end{enumerate}

The first approach effectively resolved the issue without compromising model performance, reducing the error rate from $34.80\%$ to $3.24\%$ under the most extreme perturbation scheme. Figure \ref{ceteris_paribus_h_augmentation} illustrates the results of this approach.

\begin{figure}[ht!]
\centering \includesvg[width=0.9\textwidth]{images/ceteris_paribus_results_h_augmentation.svg} \caption[Impact of data augmentation on hand part model robustness.]{Impact of data augmentation on hand part model robustness. Even under extreme velocity perturbations ($\sigma_v = 64$), the model’s decision was only marginally affected.} \label{ceteris_paribus_h_augmentation} \end{figure}

The second approach, while more radical, produced similar results. Ablation studies (Section \ref{ablation_studies}) indicated that the absence of the velocity feature did not noticeably affect model performance.

\begin{table}[ht!]
\input{sources/tables/hand_part_perturbations}
\caption[The average errors for the hand part model.]{The average errors for the hand part model $f_H$ for 1. standard perturbation, 2. uniform random change for notes played in the same time. The second transformation introduces less inconsistencies.}
\label{hand_part_perturbations}
\end{table} 

\begin{figure}[ht!]
\centering
\includesvg[width=0.9\textwidth]{images/ceteris_paribus_results_h.svg}
\includesvg[width=0.9\textwidth]{images/ceteris_paribus_results_k.svg}
\includesvg[width=0.9\textwidth]{images/ceteris_paribus_results_t.svg}
\caption[Results of the \emph{ceteris paribus} experiment.]{Results of the \emph{ceteris paribus} experiment for the hand part model $f_H$, the key signature model $f_K$ and the time signature model $f_T$. The hand part model is robust to note duration changes while it is susceptible to velocity manipulation. On the other hand, the time signature model is sensitive to note duration changes, which is expected to some extent, and relatively robust to other transformations. The key signature model is robust to all perturbations.}
\label{ceteris_paribus}
\end{figure}

Figure \ref{hand_part_misalignment} shows the result of hand part assignment for velocity perturbation.

\begin{figure}[!ht] \centering \includesvg[width=1.0\textwidth]{images/hand_part1.svg} \includesvg[width=1.0\textwidth]{images/hand_part2.svg} \caption[Hand part assignment errors after velocity perturbations.]{Hand part assignment errors after velocity perturbations. The first figure shows the original sequence, while the second shows the sequence after perturbations. Left-hand notes are diagonally patterned. Many hand part misalignments can be observed in the second figure. As a rule of thumb, left-hand notes should be at the bottom, while right-hand notes should be at the top.} \label{hand_part_misalignment} \end{figure}

\section{Local Feature Importance}

To better understand the models' behavior, we investigated the local influences of individual note features on model's decisions. In particularly, we posed several questions:

\begin{itemize}
	\item Is it possible to determine which note features influenced the model's decisions?
	\item Can we identify the specific notes that contribute most significantly to a model's predictions, such as key signature attribution?
	\item Can we quantify the strength and direction of a particular note feature's influence? For example, can we measure whether a note feature supports or opposes a right-hand assignment?
\end{itemize}

This type of questions, along with previous analysis, may help understanding the decision process of the model, facilitate recognizing weak spots of AMT systems and its limitations, and, finally, guide towards developing better AMT system.

We encountered challenges with common explainable machine learning approaches: 

\begin{itemize}
	\item The input data is a very high-dimensional space, computationally infeasible for certain methods (e.g. Shapley values).
	\item Pitch space is a discrete space that lacks a meaningful measure of distance.
	\item The note sequence tensors consists of interdependent features. In particular, they are not statistically independent, both horizontally and vertically.
\end{itemize}

To overcome these obstacles, we developed a custom solution inspired by the \emph{Local Interpetable Model-agnostic Explanations} (LIME) framework \cite{Ribeiro2016}.

\subsection{LIME-Based Analysis for Hand Part Assignment}

We designed a locally interpretable approach to assess the influence of individual note features on model outputs. The approach involves generating a locally modified version of the input MIDI tensor by selectively randomizing the pitch or velocity of individual notes. This approach aids in explaining the findings from the previous section (see Figure \ref{hand_part_misalignment} for an example).

The procedure follows these steps:
\begin{enumerate}
	\item For each note in the sequence, create $n=50$ perturbations by replacing a specific feature (e.g., pitch or velocity) with a randomized value.
	\item Measure the proximity of the perturbed value to the original value. For velocity, we assign a weight $w=\exp\left(-\tfrac{|d|}{20}\right)$, where $d$ is the absolute difference. For pitch, the weight is binary, with $w=1$ if the perturbed value differs from the original.
	\item Train a linear regression model to predict the difference between the model's output for the original sequence and the perturbed sequence, using the proximity weights.
	\item Record the regression coefficients as feature influence scores. \end{enumerate}

The resulting influence scores form a tensor of size $2 \times N \times N$, where $N$ is the number of notes in the sequence, and the two dimensions correspond to pitch and velocity influences. A single vector contains influences of a particular feature onto a decision on the level of a single note.

\subsubsection{Observations and Limitations}

The proposed approach ignores temporal aspect of the sequence, as not taking onsets and durations into account. Moreover, the approach assumes feature independence, which is clearly not justified. However, it may give a basic insight into two influence of two basic MIDI elements: pitch and velocity.

Despite these limitations, the approach offers a basic understanding of the influence of two key MIDI features (pitch and velocity) on model decisions. For example, Figure \ref{hand_part_misalignment} illustrates how changing the velocity of right-hand notes impacts hand part assignments for neighboring notes.

\begin{figure}[ht!] \centering \includesvg[width=0.95\textwidth]{images/lime_hand_part.svg} \caption[Velocity influence on hand part assignment.]{Velocity influence on hand part assignment. The G2 note (thick outline) is influenced by the velocity of neighboring notes. Negative influence is represented by the diagonal pattern. Increasing the velocity of the low note causes misalignment of G3 and B3 notes to the left hand, while reducing the G2 velocity shifts it to the right hand. This behavior is undesirable.} \label{hand_part_misalignment} \end{figure}

As one-note replacement usually won't lead to key signature change, this practice is useful mostly for the hand-part assignment. For the key signature model we propose another technique.

\subsection{Key Signature Attribution via Note Omission}

For the key signature model, where the pitch space is discrete and lacks a meaningful distance metric, we adopted a different strategy based on note omission. This method estimates the contribution of individual notes to key signature attribution.

The procedure is as follows: \begin{enumerate}
	\item Compute the key signature prediction values (before the last activation function) for the entire sequence.
	\item For each note, remove it from the sequence and calculate the difference between the original prediction and the prediction for the modified sequence.
	\item Aggregate the differences for each note. Only predictions on the current key signature are taken into account.
	\end{enumerate} This approach enables the measurement of each note's contribution to the attribution of a specific scale. Refer to Figure \ref{note_removing} for an illustrative example.
	
Using this technique we were able to estimate the influence of certain note feature to a model's decision.

\begin{figure}[ht!] \centering \includesvg[width=0.95\textwidth]{images/note_removing.svg} \caption[Key signature alignment for G major scale.]{Key signature alignment for G major scale. The influence of each note on the key signature attribution is represented by grayscale shading, with a diagonal pattern indicating negative influence. Notes D$\sharp$ and G$\sharp$, which are outside the scale, have the strongest negative impact. Sporadically, some other notes in the scale, especially B, voted slightly against the current key signature. The strongest negative vote changes the assignment probability by $6$ percentage points.}
\label{note_removing}\end{figure}
